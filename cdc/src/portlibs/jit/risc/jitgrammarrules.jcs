//
// @(#)jitgrammarrules.jcs	1.348 06/10/13
//
// Portions Copyright  2000-2008 Sun Microsystems, Inc. All Rights  
// Reserved.  Use is subject to license terms.  
// DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER  
//   
// This program is free software; you can redistribute it and/or  
// modify it under the terms of the GNU General Public License version  
// 2 only, as published by the Free Software Foundation.  
//   
// This program is distributed in the hope that it will be useful, but  
// WITHOUT ANY WARRANTY; without even the implied warranty of  
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU  
// General Public License version 2 for more details (a copy is  
// included at /legal/license.txt).  
//   
// You should have received a copy of the GNU General Public License  
// version 2 along with this work; if not, write to the Free Software  
// Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  
// 02110-1301 USA  
//   
// Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa  
// Clara, CA 95054 or visit www.sun.com if you need additional  
// information or have any questions.
//

//
// Copyright 2005 Intel Corporation. All rights reserved.
//

//
// converting CVM IR subset to RISC assembler
//

%name CVMJITCompileExpression
%type CVMJITIRNodePtr
%goal root
%opcode CVMJITgetMassagedIROpcode
%left CVMJITirnodeGetLeftSubtree 
%right CVMJITirnodeGetRightSubtree
%setstate IRRecordState
%getstate IRGetState

%leaf LOCAL32 //  Java 1-word local
%leaf LOCAL64 // Java 2-word local
%unary STATIC32 // Java 1-word static
%unary STATIC64 // Java 2-word static
%unary STATIC64VOL // Java 2-word volatile static

//
// this represents the first evaluation of a 1-word CSE
%unary IDENT32
// this represents the first evaluation of a 2-word CSE
%unary IDENT64

//
// the one word pointer that appears (in ARG1) on entry to a catch.
%leaf EXCEPTION_OBJECT
//
// make a new object. Has a class block beneath it.
%unary NEW_OBJECT

// make an array of obj. Has class block of element type, and a dimension.
%binary NEW_ARRAY_REF
%binary MULTI_NEW_ARRAY_REF

// make an array of a basic type. Has only a dimension.
%unary  NEW_ARRAY_BASIC

//
// this roots an expression which is the 
// definition of a value live across a branch.
//
%unary DEFINE_VALUE32
%unary DEFINE_VALUE64
// for loading all DEFINE values that need to be in registers
%leaf LOAD_PHIS
// for releasing the registers used by phi values
%leaf RELEASE_PHIS
// the values as they appear when used in an expression.
%leaf USED32
%leaf USED64

//
// we distinguish among a couple of constants.
// This actually requires some target-dependent re-writing of
// the IROpcode the first time we encounter the target
// independent integer-constant nodes. This is done by
// CVMJITgetMassagedIROpcode.
// These are mutually exclusive, since we have to classify a constant
// without context. Thus the classes for intersections. Also, of course,
// all mode 3 constants are mode 2 constants, (and we may wish to constrain
// mode 3 to be unsigned 8 bit values, in which case they'd also all be
// mode 1's)

%leaf 	ICONST_32	// all the others.
%leaf 	ICONST_64	// big numbers.
%leaf   STRING_ICELL_CONST // StringICell pointer from cp.

%leaf   METHOD_BLOCK	// from context, either a mb or something like it.
%leaf	CLASS_BLOCK

%binary ASSIGN		// assignment

//
// the mechanism of Java method invocation.
//
%binary IINVOKE		// return type is 32-bits
%binary LINVOKE		// return type is 64-bits
%binary VINVOKE		// return type is void
%binary IPARAMETER      // 32-bit parameter
%binary LPARAMETER      // 64-bit parameter
%leaf	NULL_PARAMETER
%unary  GET_VTBL
%unary  GET_ITBL
%binary FETCH_MB_FROM_VTABLE
%binary FETCH_MB_FROM_ITABLE

// #ifdef IAI_VIRTUAL_INLINE_CB_TEST
%unary  FETCH_VCB
%binary FETCH_MB_FROM_VTABLE_OUTOFLINE
%binary MB_TEST_OUTOFLINE
// #endif

//
// memory accesses
//
%unary  FETCH32		// memory fetch from STATIC or INDEX or FIELDREF
%unary	FETCH64
%binary INDEX  		// an array index operation
%unary  ALENGTH		// array length, of type integer
%unary  NULLCHECK	// null checked object
%binary FIELDREFOBJ     // object ref field of object
%binary FIELDREF32	// one-word field of object
%binary FIELDREF64	// two-word field of object
%binary FIELDREF64VOL	// two-word volatile field of object

%binary ISEQUENCE_R
%binary LSEQUENCE_R
%binary VSEQUENCE_R
%binary ISEQUENCE_L
%binary LSEQUENCE_L
%binary VSEQUENCE_L

//
// operations on word-sized values
//
%unary  INEG32		// one-word integer unary -
%unary  NOT32	        // one-word integer !  i.e. (x == 0)?1:0
%unary  INT2BIT32	// one-word integer !! i.e. (x != 0)?1:0
%binary IADD32		// one-word integer +
%binary ISUB32		// one-word integer binary -
%binary IMUL32		// one-word integer *
%binary IDIV32		// one-word integer /
%binary IREM32		// one-word integer %
%binary AND32		// one-word bitwise AND
%binary OR32		// one-word bitwise OR
%binary XOR32		// one-word bitwise XOR
%binary SLL32		// one-word <<
%binary SRL32		// one-word >>>
%binary SRA32		// one-word >>

//
// operations on long integer
//
%unary  INEG64		// integer unary -
%binary IADD64		// integer +
%binary ISUB64		// integer binary -
%binary IMUL64		// integer *
%binary IDIV64		// integer /
%binary IREM64		// integer %
%binary AND64		// bitwise AND
%binary OR64		// bitwise OR
%binary XOR64		// bitwise XOR
%binary SLL64		// <<
%binary SRL64		// >>>
%binary SRA64		// >>

%binary LCMP

//
// operations on floats
//
%unary  FNEG            // one-word float unary -
%binary FADD            // one-word float +
%binary FSUB            // one-word float binary -
%binary FMUL            // one-word float *
%binary FDIV            // one-word float /
%binary FREM            // one-word float %

%binary FCMPL
%binary FCMPG

//
// operations on doubles
//
%unary  DNEG            // two-word double unary -
%binary DADD            // two-word double +
%binary DSUB            // two-word double binary -
%binary DMUL            // two-word double *
%binary DDIV            // two-word double /
%binary DREM            // two-word double %

%binary DCMPL
%binary DCMPG

//
// some conversions
%unary I2B
%unary I2C
%unary I2S
%unary I2L
%unary I2F
%unary I2D
%unary L2I
%unary L2F
%unary L2D
%unary F2D
%unary F2I
%unary F2L
%unary D2F
%unary D2I
%unary D2L

//
// control operations
//
%unary TABLESWITCH
%unary LOOKUPSWITCH
%binary BCOND_INT	// conditional branch
%binary BCOND_LONG	// conditional branch
%binary BCOND_FLOAT	// conditional branch
%binary BCOND_DOUBLE	// conditional branch
%binary BOUNDS_CHECK    // array index out of bounds check
%leaf  GOTO
%leaf  RETURN


%leaf  JSR                 // a lot like goto, but is returned to
%leaf  JSR_RETURN_ADDRESS  // binds lr to the incomming jsr return address
%unary RET

// NOTE: Some of the following return opcodes are NOT NEEDED because they
//       can be handle by equivalents.  FRETURN and ARETURN are mapped into
//       IRETURN, and DRETURN is mapped into LRETURN.  The mapping is done
//       by CVMJITgetMassagedIROpcode() (see the case for
//       CVMJIT_RETURN_VALUE).

%unary IRETURN
//%unary FRETURN    // Mapped into IRETURN by CVMJITgetMassagedIROpcode().
//%unary ARETURN    // Mapped into IRETURN by CVMJITgetMassagedIROpcode().

%unary LRETURN
//%unary DRETURN    // Mapped into LRETURN by CVMJITgetMassagedIROpcode().

%unary ATHROW
%binary CHECKCAST
%binary INSTANCEOF

//
// synchronization
//
%unary MONITOR_ENTER
%unary MONITOR_EXIT

//
// lazy resolution & checkinit
//
%leaf RESOLVE_REF
%binary CHECKINIT

//
// mapping java pc's to compiled pc's
//
%leaf MAP_PC

//
// Inlining info
//
%leaf BEGININLINING
%leaf VENDINLINING
%leaf OUTOFLINEINVOKE

// Intrinsic nodes
%binary VINTRINSIC
%binary INTRINSIC32
%binary INTRINSIC64
%binary IARG
%leaf   NULL_IARG

//
// A root can be an embeddable effect
//
root: effect : 0 : : : : ;

//
// 
// The goal for grammatical purposes will be called "root";
// The only root rules for now are assignment and control ops.
// The universal nonterminal here is reg32, i.e. a single processor register.

// Purpose: LOCAL32 = value32.
root:	ASSIGN LOCAL32 reg32 : 10 : : ASSIGN_INHERITANCE(con, $$) : : {
	CVMRMResource* rhs = popResource(con);
	CVMJITIRNode*  localNode = CVMJITirnodeGetLeftSubtree($$);
	CVMJITIRNode*  rhsNode = CVMJITirnodeGetRightSubtree($$);
	CVMJITLocal*   lhs = CVMJITirnodeGetLocal(localNode);
	CVMBool        isRef = CVMJITirnodeIsReferenceType($$);
	int target;

	if (rhsNode->decorationType == CVMJIT_REGHINT_DECORATION) {
	    target = 1U << rhsNode->decorationData.regHint;
	} else {
	    target = CVMRM_ANY_SET;
	}

	CVMRMpinResource(CVMRM_INT_REGS(con), rhs, target, CVMRM_EMPTY_SET);
	CVMRMstoreJavaLocal(CVMRM_INT_REGS(con), rhs, 1, isRef, lhs->localNo);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    };

// Purpose: LOCAL64 = value32.
root:	ASSIGN LOCAL64 reg64 : 10 : : ASSIGN_INHERITANCE(con, $$) : : {
	CVMRMResource * rhs = popResource(con);
	CVMJITLocal   * lhs = CVMJITirnodeGetLocal(
	    CVMJITirnodeGetLeftSubtree($$));
	CVMRMpinResource(CVMRM_INT_REGS(con), rhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMRMstoreJavaLocal(CVMRM_INT_REGS(con), rhs, 2, CVM_FALSE,
			    lhs->localNo);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    };

%{

/* Purpose: Sets a value to a static field of an object. */
static void setStaticField(CVMJITCompilationContext *con,
			   CVMJITRMContext* rc,
                           CVMInt32 opcode,
                           CVMBool isVolatile)
{
    /* NOTE: For a non-LVM build, the staticFieldSpec is the
       staticFieldAddress.  For an LVM build, the staticFieldSpec is the
       fieldblock of the static field. */

    /* store over static-field-ref over static field address */
    CVMRMResource *src = popResource(con); /* Right Hand Side first. */
    CVMRMResource *staticField = popResource(con); /* lhs next.*/

    /* %comment l024 */

    if (isVolatile) {
        CVMCPUemitMemBarRelease(con);
    }

    CVMRMpinResource(CVMRM_INT_REGS(con), staticField,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(rc, src, rc->anySet, CVMRM_EMPTY_SET);

    CVMJITcsSetPutStaticFieldInstruction(con);

    CVMCPUemitMemoryReferenceImmediate(con, opcode,
        CVMRMgetRegisterNumber(src), CVMRMgetRegisterNumber(staticField), 0);
    if (isVolatile) {
        CVMCPUemitMemBar(con);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), staticField);
    CVMRMrelinquishResource(rc, src);
}

%}

// Purpose: STATIC32(staticFieldSpec) = value32.
root: ASSIGN STATIC32 reg32 reg32 : 20 : : : : {
        CVMBool isVolatile;
        CVMJITprintCodegenComment(("Do putstatic:"));
        CVMJITaddCodegenComment((con,
            "putstatic(staticFieldAddr, value{I|F|O})"));
        isVolatile =
            ((CVMJITirnodeGetUnaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITUNOP_VOLATILE_FIELD) != 0);
        setStaticField(con, CVMRM_INT_REGS(con), CVMCPU_STR32_OPCODE,
                       isVolatile);
    };

// Purpose: STATIC64(staticFieldSpec) = value64.
root: ASSIGN STATIC64 reg32 reg64 : 20 : : : : {
        CVMJITprintCodegenComment(("Do putstatic:"));
        CVMJITaddCodegenComment((con,
            "putstatic(staticFieldAddr, value{L|D})"));
        setStaticField(con, CVMRM_INT_REGS(con), CVMCPU_STR64_OPCODE,
                       CVM_FALSE);
    };

// Purpose: STATIC64VOL(staticFieldSpec) = value64.
root: ASSIGN STATIC64VOL reg32 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG3, ARG1) : : {
	CVMRMResource* rhs = popResource(con);
	CVMRMResource* lhs = popResource(con);

	/* Swap the arguments because the runtime helper function will expect
	   the 64-bit source value to come first followed by the static field
	   address: */
	pushResource(con, rhs);
	pushResource(con, lhs);

        CVMJITprintCodegenComment(("Do volatile putstatic:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimePutstatic64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimePutstatic64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimePutstatic64Volatile);

	/* Call the helper function: */
        longBinaryHelper2(con, (void*)CVMCCMruntimePutstatic64Volatile, $$,
			  CVM_FALSE);
    };

aluRhs: ICONST_32 : 0 : : : : {
        CVMInt32 constant;
        if (CVMJITirnodeIsConstant32Node($$)) {
            constant = CVMJITirnodeGetConstant32($$)->j.i;
        } else {
            constant = CVMJITirnodeGetConstantAddr($$)->vAddr;
        }
        pushALURhsConstant(con, constant);
    };

%{

#ifdef CVM_NEED_DO_INT_SHIFT_HELPER
/* NOTE: Maybe the shift emitter could be consolidated into the
   BinaryALU emitter?

          It seems like the right thing to do.  The only reason it is broken
          out now is because of how the ARM uses it, but it uses override rules
	  now. If it is consolidated in the BINARYALU emitter, then we can,
          consolidate some codegen rules and helpers. */
/* Purpose: Emits code for a shift operation with a const shiftAmount.  Also
            masks off the offset with 0x1f before shifting per VM spec. */
static void doIntShift(CVMJITCompilationContext *con, int shiftOp,
                       CVMJITIRNodePtr thisNode,
                       CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *lhs = popResource(con);
    CVMRMResource* dest;
    int lhsRegNo = CVMRMgetRegisterNumberUnpinned(lhs);
    CVMInt32 shiftOffset =
        CVMJITirnodeGetConstant32(CVMJITirnodeGetRightSubtree(thisNode))->j.i;
    CVMJITRMContext* rc = CVMRM_INT_REGS(con);

    /* If the dest node has a regHint and the register number is the same as
     * the register the lhs is already loaded into, then reuse the lhs
     * register as the dest register. This is common when locals are
     * shifted.
     */
    if (thisNode->decorationType == CVMJIT_REGHINT_DECORATION &&
	lhsRegNo != -1 &&
	(1U << lhsRegNo) == target &&
	CVMRMgetRefCount(CVMRM_INT_REGS(con), lhs) == 1)
    {
	/* relinquish first so dirty resources are not spilled */
	CVMRMrelinquishResource(rc, lhs);
	lhs = NULL;
	dest = CVMRMgetResourceSpecific(rc, lhsRegNo, 1);
	CVMassert(lhsRegNo == CVMRMgetRegisterNumber(dest));
    } else {
	/* avoid dest target when pinning lhs */
	CVMRMpinResource(rc, lhs, ~target, target);
	dest = CVMRMgetResource(rc, target, avoid, 1);
	lhsRegNo = CVMRMgetRegisterNumber(lhs);
    }

    CVMCPUemitShiftByConstant(con, shiftOp, CVMRMgetRegisterNumber(dest),
			      lhsRegNo, shiftOffset & 0x1f);

    if (lhs != NULL) {
	CVMRMrelinquishResource(rc, lhs);
    }
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}
#endif

#ifdef CVM_NEED_DO_REG_SHIFT_HELPER
/* Purpose: Emits code for a shift operation with an unknown shiftAmount.
            The emitter is responsible for ensuring that the shiftAmount is
            masked with 0x1f before shifting per VM spec. */
static void doRegShift(CVMJITCompilationContext *con, int shiftOp,
                       CVMJITIRNodePtr thisNode,
                       CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *rhs = popResource(con);
    CVMRMResource *lhs = popResource(con);
    CVMRMResource *dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					   target, avoid, 1);

    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    CVMCPUemitShiftByRegister(con, shiftOp, CVMRMgetRegisterNumber(dest),
        CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs));

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}
#endif

%}

// Purpose: value32 = value32 << (const32 & 0x1f).
reg32: SLL32 reg32 ICONST_32 : 20 : : : CVM_NEED_DO_INT_SHIFT_HELPER :
        doIntShift(con, CVMCPU_SLL_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value32 = value32 << (value32 & 0x1f).
reg32: SLL32 reg32 reg32 : 20 : : : CVM_NEED_DO_REG_SHIFT_HELPER :
        doRegShift(con, CVMCPU_SLL_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value32 = value32 >>> (const32 & 0x1f).
reg32: SRL32 reg32 ICONST_32 : 20 : : : CVM_NEED_DO_INT_SHIFT_HELPER :
        doIntShift(con, CVMCPU_SRL_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value32 = value32 >>> (value32 & 0x1f).
reg32: SRL32 reg32 reg32 : 20 : : : CVM_NEED_DO_REG_SHIFT_HELPER :
        doRegShift(con, CVMCPU_SRL_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value32 = value32 >> (const32 & 0x1f).
reg32: SRA32 reg32 ICONST_32 : 20 : : : CVM_NEED_DO_INT_SHIFT_HELPER :
        doIntShift(con, CVMCPU_SRA_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value32 = value32 >> (value32 & 0x1f).
reg32: SRA32 reg32 reg32 : 20 : : : CVM_NEED_DO_REG_SHIFT_HELPER :
        doRegShift(con, CVMCPU_SRA_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: Converts a value32 into an aluRhs.
aluRhs: reg32 : 0 : : : : {
	/*
	 * a simple matter of bookkeeping.
	 * may be able to (may need to!) delete this rule.
	 * it probably leads to ambiguity.
	 */
	CVMRMResource* operand = popResource(con);
        pushALURhsResource(con, operand);
    };

memSpec: ICONST_32 : 0 : : : : {
        pushMemSpecImmediate(con, CVMJITirnodeGetConstant32($$)->j.i);
    };

// Purpose: Converts a value32 into a memSpec.
memSpec: reg32 : 0 : : : : {
        /* If a numeric constant is too large, it will be converted into an
           ICONST_32 which can be mapped into a reg32.  This rule will provide
           a means to use that reg32 as an offset. */
        CVMRMResource *operand = popResource(con);
        pushMemSpecRegister(con, CVM_TRUE, operand);
    };

%{
static void
wordUnaryOp(
    CVMJITCompilationContext* con,
    int opcode,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* src = popResource(con);
    CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					   target, avoid, 1);

    CVMRMpinResource(CVMRM_INT_REGS(con), src,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitUnaryALU(con, opcode,
	CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(src),
	CVMJIT_NOSETCC);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

static void
wordBinaryOp(
    CVMJITCompilationContext* con,
    int opcode,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMCPUALURhs* rhs = popALURhs(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    int lhsRegNo = CVMRMgetRegisterNumberUnpinned(lhs);
    CVMJITRMContext* rc = CVMRM_INT_REGS(con);

#ifdef IAI_IMPROVED_CONSTANT_ENCODING
    /*
     * If rhs is a constant that is not encodable, but the negative of the
     * constant is, then reverse the sign if doing an Add or Sub.
     */
    if (CVMCPUalurhsIsConstant(rhs) &&
	(opcode == CVMCPU_ADD_OPCODE || opcode == CVMCPU_SUB_OPCODE))
    {
	int op1;
	int op2;
	if (opcode == CVMCPU_ADD_OPCODE) {
	    op1 = CVMCPU_ADD_OPCODE;
	    op2 = CVMCPU_SUB_OPCODE;
	} else {
	    op1 = CVMCPU_SUB_OPCODE;
	    op2 = CVMCPU_ADD_OPCODE;
	}
	if (!CVMCPUalurhsIsEncodableAsImmediate(op1, rhs->constValue) &&
	    CVMCPUalurhsIsEncodableAsImmediate(op2, -rhs->constValue))
	{
	    opcode = op2;
	    rhs->constValue = -rhs->constValue;
	}
    }
#endif

    /* avoid dest target when pinning rhs */
    CVMCPUalurhsPinResource(rc, opcode, rhs, ~target, target);

    /* If the dest node has a regHint and the register number is the same as
     * the register the lhs is already loaded into, then reuse the lhs
     * register as the dest register. This is common when locals are
     * incremented.
     */
    if (thisNode->decorationType == CVMJIT_REGHINT_DECORATION &&
	lhsRegNo != -1 &&
	(1U << lhsRegNo) == target &&
	CVMRMgetRefCount(rc, lhs) == 1)
    {
	/* relinquish first so dirty resources are not spilled */
	CVMRMrelinquishResource(rc, lhs);
	lhs = NULL;
	dest = CVMRMgetResourceSpecific(rc, lhsRegNo, 1);
	CVMassert(lhsRegNo == CVMRMgetRegisterNumber(dest));
    } else {
	/* avoid dest target when pinning lhs */
	CVMRMpinResource(rc, lhs, ~target, target);
	dest = CVMRMgetResource(rc, target, avoid, 1);
	lhsRegNo = CVMRMgetRegisterNumber(lhs);
    }

    CVMCPUemitBinaryALU(con, opcode, CVMRMgetRegisterNumber(dest),
			lhsRegNo, CVMCPUalurhsGetToken(con, rhs),
			CVMJIT_NOSETCC);
    if (lhs != NULL) {
	CVMRMrelinquishResource(rc, lhs);
    }
    CVMCPUalurhsRelinquishResource(rc, rhs);
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

#ifdef CVM_NEED_WORD_BINARY_OP_WITH_REG32_RHS
static void
wordBinaryOpWithReg32Rhs(
    CVMJITCompilationContext* con,
    int opcode,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* operand = popResource(con);
    pushALURhsResource(con, operand);
    wordBinaryOp(con, opcode, thisNode, target, avoid);
}
#endif

/* Purpose: Emits a call to a Unary CCM helper. */
static void
unaryHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMRMregset outgoingSpillRegSet,
    int resultSize)
{
    CVMRMResource *src = popResource(con);
    CVMRMResource *dest;

    /* Pin the input to CVMCPU_ARG1_REG because the helper expects it there: */
    src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src, CVMCPU_ARG1_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, outgoingSpillRegSet);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        int argSize = CVMRMgetSize(src);
        if (argSize == 2) {
            /* argSize = 2 means the argument is doubleword */
            CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                          CVMCPU_ARG1_REG);
        }
    }
#endif

    /* Emit the call to the helper to compute the result: */
    CVMCPUemitAbsoluteCall(con, helperAddress, 
			   CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
    CVMJITcsBeginBlock(con);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (resultSize == 2) {
            /* resultSize = 2 means the result is doubleword. */
            CVMCPUemitMoveFrom64BitRegister(con, CVMCPU_RESULT1_REG,
                                            CVMCPU_RESULT1_REG);
        }
    }
#endif

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_RESULT1_REG,
				    resultSize);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

/* Purpose: Emits a call to a Binary CCM helper. */
static void
binaryHelper(
    CVMJITCompilationContext *con,
    void* helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero,
    int argSize, /* total size in words of all arguments */
    int resultSize)
{
    const char *symbolName;
    CVMCodegenComment *comment;
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    int lhsReg, rhsReg;
    CVMRMregset outSet;

    CVMJITpopSymbolName(con, symbolName);
    CVMJITpopCodegenComment(con, comment);
    lhsReg = CVMCPU_ARG1_REG;
    if (argSize == 2) {
	rhsReg = CVMCPU_ARG2_REG;
	outSet = ARG1|ARG2;
    } else if (argSize == 3){
	rhsReg = CVMCPU_ARG3_REG;
	outSet = ARG1|ARG2|ARG3;
    } else {
	CVMassert(argSize == 4);
	rhsReg = CVMCPU_ARG3_REG;
	outSet = ARG1|ARG2|ARG3|ARG4;
    }
    /* Pin the input to the first two arguments because the helper expects it
       there. Note that if the rhs is already in a register, then pin it first
       so it is setup using a move instruction. Otherwise you run the risk of
       it getting clobbered when pinning the lhs, and then having to reload
       if from memory (after possibly also having spilled it). */
    if (CVMRMgetRegisterNumberUnpinned(rhs) != -1) {
	rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, rhsReg);
	lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, lhsReg);
    } else {
	rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, rhsReg);
	lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, lhsReg);
    }

    if (checkZero) {
	if (CVMRMdirtyJavaLocalsCount(con) == 0) {
	    if (resultSize == 2) {
		CVMRMResource *scratch =
		    CVMRMgetResource(CVMRM_INT_REGS(con),
				     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
		int destReg = CVMRMgetRegisterNumber(scratch);
                CVMCPUemitBinaryALURegister(con, CVMCPU_OR_OPCODE,
					    destReg, rhsReg, rhsReg+1,
					    CVMJIT_SETCC);
#ifndef CVMCPU_HAS_ALU_SETCC
		CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
				  destReg, CVMCPUALURhsTokenConstZero);
					  
#endif
		CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	    } else {
                CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
				  rhsReg, CVMCPUALURhsTokenConstZero);
	    }
	    CVMJITaddCodegenComment((
                con, trapCheckComments[CVMJITIR_DIVIDE_BY_ZERO]));
            CVMCPUemitAbsoluteCallConditional(con, 
		(void*)CVMCCMruntimeThrowDivideByZeroGlue,
                CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_EQ);
	    CVMJITcsBeginBlock(con);
	} else {
	    /* Dirty locals are not supported yet */
	    CVMassert(CVM_FALSE);
	}
    }

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, outSet);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (argSize >= 3) {
            /* argSize >= 3 means the lhs argument is doubleword */
            CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                          CVMCPU_ARG1_REG);
            if (argSize == 4) {
                /* argSize = 4 means both lhs and rhs arguments 
                   are doubleword */
                CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG2_REG,
                                              CVMCPU_ARG3_REG);
            } else {
                /* argSize is 3, and rhs is a 32-bit type. We need to move
                 * CVMCPU_ARG3_REG to CVMCPU_ARG2_REG to set up the second
                 * argument.
                 */
                CVMassert(argSize == 3);
                CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
                                       CVMCPU_ARG2_REG, CVMCPU_ARG3_REG,
                                       CVMJIT_NOSETCC);
            }
        }
    }
#endif

    /* Emit the call to the helper to compute the result: */
    CVMJITpushCodegenComment(con, comment);
    CVMJITpushSymbolName(con, symbolName);
    CVMCPUemitAbsoluteCall(con, helperAddress,
			   CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
    CVMJITcsBeginBlock(con);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (resultSize == 2) {
            /* resultSize = 2 means the result is a doubleword type */
            CVMCPUemitMoveFrom64BitRegister(con, CVMCPU_RESULT1_REG,
                                            CVMCPU_RESULT1_REG);
        }
    }
#endif

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_RESULT1_REG,
				    resultSize);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

/* Purpose: Emits a call to a Binary CCM helper which return a word result. */
static void
wordBinaryHelper(
    CVMJITCompilationContext *con,
    void* helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 2, 1);
}

/* Purpose: Emits a call to a Binary CCM helper which return a dword result. */
static void
longBinaryHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 4, 2);
}

/* Purpose: Emits a call to a Binary CCM helper which return a dword resul,
 * but whose 2nd argument is 32-bit, not 64-bit. */
static void
longBinaryHelper2(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 3, 2);
}

/* Purpose: Emits a call to a Binary CCM helper which return a dword result. */
static void
longBinary2WordHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero)
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 4, 1);
}

/*
 * For nodes of the form:
 * NODE .... reg<width>
 * 
 * Pass the reg<width> value onto NODE
 */
static void
passLastEvaluated(CVMJITCompilationContext* con, CVMJITRMContext* rc, 
		  CVMJITIRNodePtr thisNode)
{
    /* Associate the last evaluated sub-node on to its parent */
    CVMRMResource* arg = popResource(con);
    CVMRMoccupyAndUnpinResource(rc, arg, thisNode);
    pushResource(con, arg);
}

/*
 * Inlining start
 */
static void
beginInlining(CVMJITCompilationContext* con, CVMJITIRNodePtr thisNode)
{
    CVMJITUnaryOp* unop = CVMJITirnodeGetUnaryOp(thisNode);
    CVMJITConstantAddr* constAddr;
    CVMJITMethodContext* mc;
    CVMUint16 pcStart = CVMJITcbufGetLogicalPC(con);
    CVMJITInliningInfoStackEntry* newEntry;
    
    CVMassert(CVMJITirnodeIsConstMC(unop->operand));
    constAddr = CVMJITirnodeGetConstantAddr(unop->operand);
    mc = constAddr->mc;
    /* Start a new inlining entry */
    CVMassert(con->inliningDepth < con->maxInliningDepth);
    newEntry = &con->inliningStack[con->inliningDepth++];
    /* Record the starting point and mb of this inlining record */
    /* The end point will be recorded by the corresponding END_INLINING node */
    newEntry->pcOffset1 = pcStart;
    newEntry->mc = mc;
    CVMJITprintCodegenComment(("Begin inlining of %C.%M (start pc=%d):",
			       CVMmbClassBlock(mc->mb), mc->mb, pcStart));
}

/*
 * We have finished inlined code. Record this PC.
 */
static void
endInlining(CVMJITCompilationContext* con, CVMJITIRNodePtr thisNode)
{
    CVMJITInliningInfoStackEntry* topEntry;
    CVMCompiledInliningInfoEntry* recordedEntry;
    CVMUint16 pcEnd = CVMJITcbufGetLogicalPC(con);
    CVMJITMethodContext *mc;
    CVMUint16 count = CVMJITirnodeGetNull(thisNode)->data;

    CVMassert(count > 0);
    while (count-- > 0) {
	CVMassert(con->inliningDepth > 0);
	topEntry = &con->inliningStack[--con->inliningDepth];
	mc = topEntry->mc;
	CVMassert(topEntry->pcOffset1 > 0);
	CVMassert(topEntry->mc != NULL);

	CVMassert(con->inliningInfoIdx < con->numInliningInfoEntries);
	/* Now commit another "permanent" entry */
	recordedEntry = &con->inliningInfo->entries[con->inliningInfoIdx++];
	recordedEntry->pcOffset1 = topEntry->pcOffset1;
	recordedEntry->mb = mc->mb;
	recordedEntry->pcOffset2 = pcEnd;
	recordedEntry->invokePC = mc->invokePC;
	recordedEntry->flags = 0;

	{
	    recordedEntry->firstLocal = mc->firstLocal;
	    if (CVMmbIs(mc->mb, SYNCHRONIZED)) {
		CVMassert(!CVMmbIs(mc->mb, STATIC));
		recordedEntry->syncObject = mc->syncObject;
	    } else {
		recordedEntry->syncObject = 0;
	    }
	}

	CVMJITprintCodegenComment(("End inlining of %C.%M (end pc=%d):",
				   CVMmbClassBlock(recordedEntry->mb),
				   recordedEntry->mb,
				   pcEnd));
    }
}

#ifdef CVM_DEBUG
static void 
printInliningInfo(CVMJITIRNode* thisNode, const char* what)
{
    CVMJITUnaryOp* unop = CVMJITirnodeGetUnaryOp(thisNode);
    CVMJITConstantAddr* constAddr;
    CVMMethodBlock* mb;
    
    if (CVMJITirnodeIsConstMB(unop->operand)) {
	constAddr = CVMJITirnodeGetConstantAddr(unop->operand);       
	mb = constAddr->mb;
	CVMJITprintCodegenComment(("%s of %C.%M:", what,
				   CVMmbClassBlock(mb), mb));
    } else {
	CVMJITprintCodegenComment(("%s (node id %d):", what,
				   CVMJITirnodeGetID(unop->operand)));
    }
}
#else
#define printInliningInfo(thisNode, what)
#endif
%}

//
// "R" Sequences:
//
//         SEQUENCE_R
//         /      \
//      effect   expr
//
// evaluates to the value of 'expr'. 'effect' does not produce a value.
//

%{
#define SEQUENCE_R_INHERITANCE(thisNode, rc) { \
    SET_ATTRIBUTE_TYPE(0, CVMJIT_EXPRESSION_ATTRIBUTE_TARGET_AVOID); \
    goal_top[0].attributes[0].u.rs.target = CVMRM_GET_ANY_SET(rc);   \
    goal_top[0].attributes[0].u.rs.avoid  = CVMCPU_AVOID_NONE;       \
    goal_top[0].attributes[1] = goal_top[-1].curr_attribute[0];      \
}
%}

reg32:  ISEQUENCE_R effect reg32 : 0 : :
        SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

reg64:  LSEQUENCE_R effect reg64 : 0 : :
	SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

// "type-less" sequence. 
reg32:  VSEQUENCE_R effect reg32 : 0 : : 
	SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : ;

//
// "L" Sequences:
//
//         SEQUENCE_L
//         /      \
//      expr    effect
//
// evaluates to the value of 'expr'. 'effect' does not produce a value.
//

%{
#define SEQUENCE_L_INHERITANCE(thisNode, rc) { \
    SET_ATTRIBUTE_TYPE(1, CVMJIT_EXPRESSION_ATTRIBUTE_TARGET_AVOID); \
    goal_top[0].attributes[0] = goal_top[-1].curr_attribute[0];      \
    goal_top[0].attributes[1].u.rs.target = CVMRM_GET_ANY_SET(rc);   \
    goal_top[0].attributes[1].u.rs.avoid  = CVMCPU_AVOID_NONE;       \
}
%}

reg32:  ISEQUENCE_L reg32 effect : 0 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

reg64:  LSEQUENCE_L reg64 effect : 0 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

// "type-less" sequence. 
reg32:  VSEQUENCE_L reg32 effect : 0 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : ;

// this is always a root node
root: BEGININLINING : 0 : : : : {
    beginInlining(con, $$);
};

//
// The following node is to mark the end of the inlining of a method.
//
effect:  VENDINLINING : 0 : : : : {
    endInlining(con, $$);
};

reg32:	LOCAL32 : 10 : : : : {
	CVMJITLocal*   l = CVMJITirnodeGetLocal( $$ );
	CVMBool isRef = CVMJITirnodeIsReferenceType($$);
	CVMRMResource* dest = 
	    CVMRMbindResourceForLocal(CVMRM_INT_REGS(con), 1,
				      isRef, l->localNo);
	CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
					    dest, GET_REGISTER_GOALS);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

reg64:	LOCAL64 : 10 : : : : {
	CVMJITLocal*   l = CVMJITirnodeGetLocal( $$ );
	CVMRMResource* dest =
	    CVMRMbindResourceForLocal(CVMRM_INT_REGS(con), 2,
				      CVM_FALSE, l->localNo);
	CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
					    dest, GET_REGISTER_GOALS);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

%{

/* Purpose: Gets a value from a static field of an object. */
static void getStaticField(CVMJITCompilationContext *con,
			   CVMJITRMContext* rc,
                           CVMJITIRNodePtr thisNode,
                           CVMRMregset target, CVMRMregset avoid,
                           CVMInt32 opcode, int fieldSize,
                           CVMBool isVolatile)
{
    /* NOTE: For a non-LVM build, the staticFieldSpec is the
       staticFieldAddress.  For an LVM build, the staticFieldSpec is the
       fieldblock of the static field. */

    /* fetch over static-field-ref over fb constant */
    CVMRMResource *staticField = popResource(con);
    CVMRMResource *dest = CVMRMgetResource(rc, target, avoid, fieldSize);

    /* %comment l026 */ 

    CVMRMpinResource(CVMRM_INT_REGS(con), staticField,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMJITcsSetGetStaticFieldInstruction(con);
    CVMCPUemitMemoryReferenceImmediate(con, opcode,
        CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(staticField), 0);

    if (isVolatile) {
        CVMCPUemitMemBarAcquire(con);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), staticField);
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

%}

// Purpose: value32 = FETCH32(STATIC32(staticFieldSpec))
reg32: FETCH32 STATIC32 reg32 : 20 : : : : {
        CVMBool isVolatile;
        CVMJITprintCodegenComment(("Do getstatic:"));
        CVMJITaddCodegenComment((con,
            "value{I|F|O} = getstatic(staticFieldAddr);"));
        isVolatile =
            ((CVMJITirnodeGetUnaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITUNOP_VOLATILE_FIELD) != 0);
        getStaticField(con, CVMRM_INT_REGS(con),
		       $$, GET_REGISTER_GOALS, CVMCPU_LDR32_OPCODE, 1,
                       isVolatile);
    };

// Purpose: value64 = FETCH64(STATIC64(staticFieldSpec))
reg64: FETCH64 STATIC64 reg32 : 20 : : : : {
        CVMJITprintCodegenComment(("Do getstatic:"));
        CVMJITaddCodegenComment((con,
            "value{L|D} = getstatic(staticFieldAddr);"));
        getStaticField(con, CVMRM_INT_REGS(con),
		       $$, GET_REGISTER_GOALS, CVMCPU_LDR64_OPCODE, 2,
                       CVM_FALSE);
    };

// Purpose: value64 = FETCH64(STATIC64VOL(staticFieldSpec))
reg64: FETCH64 STATIC64VOL reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITprintCodegenComment(("Do volatile getstatic:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeGetstatic64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeGetstatic64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeGetstatic64Volatile);
        unaryHelper(con, (void*)CVMCCMruntimeGetstatic64Volatile, $$, ARG1, 2);
    };

reg32: EXCEPTION_OBJECT : 0 : : : : {
	/* It appears in ARG1, so we initially bind it to ARG1. Note that
	 * the front end guarantees that the first thing done is an
	 * ASSIGN of the exception object, so we don't need to worry
	 * about it being trashed before we get here.
	 */
	CVMRMResource* dest =
	    CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_ARG1_REG, 1);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueRef = NEW_OBJECT(classBlock)
reg32: NEW_OBJECT reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG2); : : {
        CVMRMResource* cbRes = popResource(con);
	CVMRMResource* dest;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        CVMJITprintCodegenComment(("Do new:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG2_REG);
        CVMRMmajorSpill(con, ARG2, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
#ifdef CVM_TRACE_JIT
        if (CVMRMisConstant(cbRes)) {
	    CVMJITprintCodegenComment(("cb: %C", cbRes->constant));
	}
#endif

#ifdef IAI_NEW_GLUE_CALLING_CONVENTION_IMPROVEMENT
	/* Load the class instanceSize and accessFlags into ARG3 and ARG4.
	 * If the class is resolved and linked, then these are known at
	 * compile time and can be treated as constants. Otherwise they
	 * are loaded from the cb.
	 */
        if (CVMRMisConstant(cbRes) &&
            CVMcbCheckRuntimeFlag((CVMClassBlock*)(cbRes->constant), LINKED))
	{
	    CVMClassBlock* cb = (CVMClassBlock*)(cbRes->constant);
	    /* get class instanceSize into ARG3 */
	    CVMJITaddCodegenComment((con, "CVMcbInstanceSize(cb)"));
	    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG,
				   CVMcbInstanceSize(cb));
	    /* get class accessFlags into ARG4 */
	    CVMJITaddCodegenComment((con, "CVMcbAccessFlags(cb)"));
	    CVMCPUemitLoadConstant(con, CVMCPU_ARG4_REG,
				   CVMcbAccessFlags(cb));
        } else {
	    /* load class instanceSize from cb into ARG3 */
	    CVMJITaddCodegenComment((con, "CVMcbInstanceSize(cb)"));
            CVMCPUemitMemoryReferenceImmediate(con,
                CVMCPU_LDR16U_OPCODE,
                CVMCPU_ARG3_REG, CVMCPU_ARG2_REG,
		offsetof(CVMClassBlock, instanceSizeX));
	    /* load class accessFlags from cb into ARG4 */
	    CVMJITaddCodegenComment((con, "CVMcbAccessFlags(cb)"));
	    CVMCPUemitMemoryReferenceImmediate(con,
                CVMCPU_LDR8_OPCODE,
                CVMCPU_ARG4_REG, CVMCPU_ARG2_REG,
		offsetof(CVMClassBlock, accessFlagsX));
        }
#endif

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeNewGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeNewGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeNew);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeNewGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);
	/*
	 * Return value is RESULT1
	 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
	pushResource(con, dest);
    };

// Purpose: valueRef = NEW_ARRAY_REF(elementClassBlock, dimension)
reg32: NEW_ARRAY_REF reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG3, ARG2); : : {
        CVMRMResource *dest;
        CVMRMResource *dimension = popResource(con);
        CVMRMResource *cbRes = popResource(con);

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        CVMJITprintCodegenComment(("Do anewarray:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG3_REG);
        dimension = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), dimension,
					     CVMCPU_ARG2_REG);

        CVMRMmajorSpill(con, ARG2|ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeANewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeANewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeANewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeANewArrayGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);
	
	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimension);
	pushResource(con, dest);
    };

// Purpose: valueRef = MULTI_NEW_ARRAY_REF(basicElementClassBlock, dimensions)
reg32: MULTI_NEW_ARRAY_REF reg32 parameters : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2_0($$, ARG3); : : {
	CVMRMResource* dest;
	CVMJITIRNode* paramnode;
	int nDimensions;
        CVMRMResource *cbRes = popResource(con);
        CVMRMResource *dimensionsDepth;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        CVMJITprintCodegenComment(("Do multianewarray:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG3_REG);

        CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

	/*
	 * number of dimensions is simply the number of PARAMETER nodes
	 * stacked under this one.
	 */
	paramnode = CVMJITirnodeGetRightSubtree($$);
        nDimensions = CVMJITirnodeGetBinaryOp($$)->data;

        /* Load the number of dimensions into ARG2: */
        CVMJITaddCodegenComment((con, "number of dimensions"));
        dimensionsDepth = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
						   CVMCPU_ARG2_REG, 1);
        CVMCPUemitLoadConstant(con, CVMCPU_ARG2_REG, nDimensions);

        /* Address of the first of them is JSP-4*nDimensions: */
        CVMJITaddCodegenComment((con, "&dimensions"));
        /* arg4 = jsp - (dimensionsDepth << 2): */
        CVMCPUemitComputeAddressOfArrayElement(con, CVMCPU_SUB_OPCODE,
            CVMCPU_ARG4_REG, CVMCPU_JSP_REG,
            CVMRMgetRegisterNumber(dimensionsDepth), CVMCPU_SLL_OPCODE, 2);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMultiANewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMultiANewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMultiANewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMultiANewArrayGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);

	/* pop dimensions from operand stack */
        CVMJITaddCodegenComment((con, "pop dimensions off the stack"));
        CVMCPUemitBinaryALUConstant(con, CVMCPU_SUB_OPCODE,
            CVMCPU_JSP_REG, CVMCPU_JSP_REG, 4*nDimensions, CVMJIT_NOSETCC);
        /* Tell stackman to pop the dimensions: */
        CVMSMpopParameters(con, nDimensions);

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimensionsDepth);
	pushResource(con, dest);
    };

// Purpose: valueRef = NEW_ARRAY_BASIC(dimension)
reg32: NEW_ARRAY_BASIC reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG2); : : {
	CVMRMResource* dest;
	CVMRMResource* dimension = popResource(con);
        CVMBasicType typeCode;
	CVMClassBlock* arrCB;
	CVMUint32 elementSize;
	
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        /* Map the IR node opcode to the type code: */
        typeCode = (CVMJITgetOpcode($$) >> CVMJIT_SHIFT_OPCODE)
                    - CVMJIT_NEW_ARRAY_BOOLEAN + CVM_T_BOOLEAN;

	/* This is known at compile time for arrays of basic types */
	arrCB = (CVMClassBlock*)CVMbasicTypeArrayClassblocks[typeCode];
	elementSize = CVMbasicTypeSizes[typeCode];

        CVMJITprintCodegenComment(("Do newarray:"));
        dimension = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), dimension,
					     CVMCPU_ARG2_REG);
	CVMRMmajorSpill(con, ARG2, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

        CVMJITaddCodegenComment((con, "%C", arrCB));
        CVMJITsetSymbolName((con, "cb %C", arrCB));
        CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, (int)arrCB);
        CVMJITaddCodegenComment((con, "array element size"));
        CVMCPUemitLoadConstant(con, CVMCPU_ARG1_REG, elementSize);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeNewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeNewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeNewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeNewArrayGlue, 
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimension);
	pushResource(con, dest);
    };

%{

/* Purpose: Converts a constant into reg32 value. */
static void
const2Reg32(CVMJITCompilationContext *con, CVMJITRMContext *rc,
	    CVMJITIRNodePtr thisNode, int target, int avoid)
{
    CVMInt32 constant;
    CVMRMResource *dest;
    if (CVMJITirnodeIsConstant32Node(thisNode)) {
        constant = CVMJITirnodeGetConstant32(thisNode)->j.i;
    } else {
        constant = CVMJITirnodeGetConstantAddr(thisNode)->vAddr;
    }
    dest = CVMRMbindResourceForConstant32(rc, constant);
    /* only pin eagerly if constant cannot be treated as an immediate */
    if (!CVMCPUalurhsIsEncodableAsImmediate(CVMCPU_ADD_OPCODE, constant) &&
	!CVMCPUalurhsIsEncodableAsImmediate(CVMCPU_SUB_OPCODE, constant))
    {
	CVMRMpinResourceEagerlyIfDesireable(rc, dest, target, avoid);
    }
    /* Need this in case this constant is a CSE */
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

%}

reg32: ICONST_32 : 20 : : : :
        const2Reg32(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);

reg32: STRING_ICELL_CONST : 20 : : : : {
	CVMRMResource* stringICellResource;
	CVMUint32      stringICellReg;
	CVMRMResource* stringObjectResource =
	    CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
	CVMUint32      stringObjectReg =
	    CVMRMgetRegisterNumber(stringObjectResource);

	CVMStringICell* stringICell =
	    CVMJITirnodeGetConstantAddr($$)->stringICell;
        CVMJITsetSymbolName((con, "StringICell"));
	stringICellResource = 
	    CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
					  CVMRM_ANY_SET, CVMRM_EMPTY_SET,
                                          (CVMUint32)stringICell);
	stringICellReg = CVMRMgetRegisterNumber(stringICellResource);
        CVMJITaddCodegenComment((con, "StringObject from StringICell"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            stringObjectReg, stringICellReg, 0);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con),
					stringObjectResource, $$);
	pushResource(con, stringObjectResource);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), stringICellResource);
    };

reg32: CLASS_BLOCK : 20 : : : : {
        CVMRMResource *dest;
        CVMClassBlock *cb = CVMJITirnodeGetConstantAddr($$)->cb;
        CVMJITsetSymbolName((con, "cb %C", cb));
        dest =
	    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), (CVMInt32)cb);
	CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
					    dest, GET_REGISTER_GOALS);
        /* Need this in case this constant is a CSE */
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };


%{
#define IDENT_SYNTHESIS(con, thisNode)			\
{							\
    if (!CVMJIT_JCS_DID_PHASE(IRGetState(thisNode),	\
	CVMJIT_JCS_STATE_SYNTHED))			\
    {							\
	/* same as DEFAULT_SYNTHESIS_CHAIN  */		\
	DEFAULT_SYNTHESIS1(con, thisNode);		\
    } else {						\
	/* Leaf node */					\
    }							\
}

#define IDENT_INHERITANCE(con, thisNode)		\
{							\
    CVMassert(!CVMJIT_DID_SEMANTIC_ACTION(thisNode));	\
    DEFAULT_INHERITANCE_CHAIN(con, thisNode);		\
}

%}

%dag reg32: IDENT32 reg32 : 0 :
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	CVMRMResource* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popResource(con);
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, $$);
	    /* CVMconsolePrintf("Initial evaluation of "); */
	} else {
	    src = CVMRMfindResource(CVMRM_INT_REGS(con), $$);
	    /* CVMconsolePrintf("Reiteration of "); */
	    CVMassert(src != NULL);
	}
	/*
	CVMconsolePrintf("Fixed IDENT32 ID %d, resource 0x%x\n",
	    $$->nodeID, src);
	*/
	pushResource(con, src);
    };

// This handles IDENT32 over a BOUNDS_CHECK with a constant index
%dag iconst32Index: IDENT32 iconst32Index : 0 :
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	CVMInt32 idx;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    idx = popIConst32(con);
	    CVMJITirnodeGetIdentOp($$)->backendData = idx;
	} else {
	    idx = CVMJITirnodeGetIdentOp($$)->backendData;
	}
	pushIConst32(con, idx);
    };

%dag arrayIndex: IDENT32 arrayIndex : 0 :
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	ScaledIndexInfo* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popScaledIndexInfo(con);
	    CVMJITidentitySetDecoration(con, (CVMJITIdentityDecoration*)src,
					$$);
	} else {
	    src = (ScaledIndexInfo*)CVMJITidentityGetDecoration(con, $$);
	    CVMassert((src == NULL) ||
		      CVMJITidentityDecorationIs(con, $$, SCALEDINDEX));
	    /* CVMconsolePrintf("Reiteration of "); */
	    CVMassert(src != NULL);
	}
	/*
	CVMconsolePrintf("IDENT32 ID %d, resource 0x%x\n",
	    $$->nodeID, src);
	*/
	
#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
        src->isIDENTITYOutofBoundsCheck = CVM_TRUE;
#endif
	pushScaledIndexInfo(con, src);
    };

%dag reg64: IDENT64 reg64 : 0 :
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	CVMRMResource* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popResource(con);
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, $$);
	} else {
	    src = CVMRMfindResource(CVMRM_INT_REGS(con), $$);
	    CVMassert(src != NULL);
	}
	pushResource(con, src);
    };

reg32: INEG32 reg32 : 10 : : : :
	wordUnaryOp(con, CVMCPU_NEG_OPCODE, $$, GET_REGISTER_GOALS);

reg32: NOT32 reg32 : 10 : : : :
	wordUnaryOp(con, CVMCPU_NOT_OPCODE, $$, GET_REGISTER_GOALS);

reg32: INT2BIT32 reg32 : 10 : : : :
	wordUnaryOp(con, CVMCPU_INT2BIT_OPCODE, $$, GET_REGISTER_GOALS);

%{
#ifdef CVMJIT_INTRINSICS

/* Get absolute value of srcReg and set condition codes:
     adds    rDest, rSrc, #0
     neglt   rDest, rSrc
*/
static void emitAbsolute(CVMJITCompilationContext* con,
			 int destReg, int srcReg)
{
    CVMCPUemitBinaryALU(con, CVMCPU_ADD_OPCODE,
        destReg, srcReg, CVMCPUALURhsTokenConstZero, CVMJIT_SETCC);
#ifndef CVMCPU_HAS_ALU_SETCC
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LT,
        srcReg, CVMCPUALURhsTokenConstZero);
#endif
    CVMCPUemitUnaryALUConditional(con, CVMCPU_NEG_OPCODE,
        destReg, srcReg, CVMJIT_NOSETCC, CVMCPU_COND_LT);
}

CVMJITRegsRequiredType
CVMJITRISCintrinsicDefaultGetRequired(CVMJITCompilationContext *con,
                                      CVMJITIRNode *intrinsicNode,
                                      CVMJITRegsRequiredType argsRequiredSet)
{
    return argsRequiredSet;
}

CVMRMregset
CVMJITRISCintrinsicDefaultGetArgTarget(CVMJITCompilationContext *con,
                                       int typeTag, CVMUint16 argNumber,
                                       CVMUint16 argWordIndex)
{
    return CVMRM_ANY_SET;
}

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
CVMJITRegsRequiredType
CVMJITRISCintrinsicSimpleLockReleaseGetRequired(
    CVMJITCompilationContext *con,
    CVMJITIRNode *intrinsicNode,
    CVMJITRegsRequiredType argsRequiredSet)
{
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS
    /* During the release, we may call CVMCCMruntimeSimpleSyncUnlock,
     * which requires ARG1 and ARG2
     */
    return argsRequiredSet | ARG1 | ARG2 | CVMCPU_AVOID_C_CALL;
#else
    return argsRequiredSet;
#endif
}


CVMRMregset
CVMJITRISCintrinsicSimpleLockReleaseGetArgTarget(
    CVMJITCompilationContext *con,
    int typeTag, CVMUint16 argNumber,
    CVMUint16 argWordIndex)
{
    CVMassert(argNumber == 0);
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS
    /* During the release, we may call CVMCCMruntimeSimpleSyncUnlock,
     * which requires "this" to be in ARG2
     */
    return ARG2;
#else
    return CVMRM_ANY_SET;
#endif
}
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */

/* intrinsic emitter for Thread.currentThread(). */
static void
java_lang_Thread_currentThread_EmitOperator(CVMJITCompilationContext *con,
                                            CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    CVMRMResource* dest =
	CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    int destReg = CVMRMgetRegisterNumber(dest);
    int eeReg;

    /*
        ldr eeReg, [sp, #OFFSET_CVMCCExecEnv_ee]           @ Get ee.
        ldr rDest, [eeReg, #OFFSET_CVMExecEnv_threadICell] @ Get threadICell.
        ldr rDest, [rDest]                                 @ Get thread obj.
    */

#ifdef CVMCPU_EE_REG
    eeReg = CVMCPU_EE_REG;
#else
    eeReg = destReg;
    /* Get the ee from the ccee: */
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        eeReg, offsetof(CVMCCExecEnv, eeX));
#endif
    /* Get the thread icell from the ee: */
    CVMJITaddCodegenComment((con, "destReg = ee->threadICell"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        destReg, eeReg, offsetof(CVMExecEnv, threadICell));

    /* Get the thread object from the thread icell: */
    CVMJITaddCodegenComment((con, "destReg = *ee->threadICell"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        destReg, destReg, 0);

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

static void
iabsEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    CVMRMResource* src = popResource(con);
    CVMRMResource* dest =
	CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), src,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    emitAbsolute(con, CVMRMgetRegisterNumber(dest),
                 CVMRMgetRegisterNumber(src));
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_MICROLOCK && \
    CVM_MICROLOCK_TYPE == CVM_MICROLOCK_SWAP_SPINLOCK

/*
 * Intrinsic emitter for spinlock microlock version of CVM.simpleLockGrab().
 *
 * Grabs CVMglobals.objGlobalMicroLock using atomic swap. If it fails,
 * returns FALSE. If successful, checks if the object is already locked.
 * If locked, releases CVMglobals.objGlobalMicroLock and returns FALSE.
 * Otherwise returns TRUE.
 */

static void
simpleLockGrabEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* dest;
    CVMRMResource* microLock;
    int objRegID, objHdrRegID, destRegID, microLockRegID;
    int fixupPC1, fixupPC2; /* To patch the conditional branches */

    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);

    dest = CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    objHdr = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), obj, CVMRM_ANY_SET, CVMRM_SAFE_SET);

    objRegID    = CVMRMgetRegisterNumber(obj);
    objHdrRegID = CVMRMgetRegisterNumber(objHdr);
    destRegID   = CVMRMgetRegisterNumber(dest);

    /* load microlock address into microLockRegID */
    CVMJITsetSymbolName((con, "&CVMglobals.objGlobalMicroLock"));
    microLock = CVMRMgetResourceForConstant32(
        CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	(CVMUint32)&CVMglobals.objGlobalMicroLock);
    microLockRegID = CVMRMgetRegisterNumber(microLock);
    /* preload the address to help caching */
    CVMJITaddCodegenComment((con, "tmp = CVMglobals.objGlobalMicroLock"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       destRegID, microLockRegID, 0);
    /* Get microlock LOCKED flag */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_LOCKED"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_MICROLOCK_LOCKED);
    /* atomic swap LOCKED into the microlock */
    CVMJITaddCodegenComment((con,
	"swp(CVMglobals.objGlobalMicroLock, CVM_MICROLOCK_LOCKED)"));
    CVMCPUemitAtomicSwap(con, destRegID, microLockRegID);
    CVMCPUemitMemBarAcquire(con);

    /* check if microlock is already locked */
    CVMJITaddCodegenComment((con, "check if microlock is locked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      destRegID, CVM_MICROLOCK_LOCKED);

    /* branch if microlock already locked */
    CVMJITaddCodegenComment((con, "br failed if microlock is locked"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* assume not locked and set intrinsic result */
    CVMJITaddCodegenComment((con, "assume not locked: result = true"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_TRUE);
    /* get sync bits from object header */
    CVMJITaddCodegenComment((con, "get obj sync bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
				objHdrRegID, objHdrRegID,
				CVM_SYNC_MASK, CVMJIT_NOSETCC);
    /* check if object is unlocked */
    CVMJITaddCodegenComment((con, "check if obj unlocked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      objHdrRegID, CVM_LOCKSTATE_UNLOCKED);
    /* branch if object not locked */
    CVMJITaddCodegenComment((con, "br done if object is not locked"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC2 = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC2 = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC2 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* Object is locked. Release microlock */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_MICROLOCK_UNLOCKED);
    CVMJITaddCodegenComment((con,
	"CVMglobals.objGlobalMicroLock = CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
				       destRegID, microLockRegID, 0);
    /* Failure target. Make instrinsic return false. */
    CVMtraceJITCodegen(("\t\tfailed:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    CVMCPUemitLoadConstant(con, destRegID, CVM_FALSE);
    /* "done" target. No change is made instrinc result. */
    CVMtraceJITCodegen(("\t\tdone:\n"));
    CVMJITfixupAddress(con, fixupPC2, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

#ifdef CVM_DEBUG
    /* For Debug builds, we do the following:
     *
     * 1. Set the ee's microlock depth to 0 or 1 based on success.
     * 2. Set CVMglobals.jit.currentSimpleSyncMB to the Simple Sync
     *    mb we are currently generating code for.
     *
     * (1) is done so C code will assert if the microlock gets out
     * of balance. Note we don't assert in here in the generated code
     * because it is too ugly.
     *
     * (2) is done in case there is ever a problem, we can find out
     * the last Simple Sync method called by looking in CVMglobals.
     * It is disabled with #if 0 by default.
     */
    {
	/* 1. Set the ee's microlock depth to 0 or 1 based on success. */
	int eeReg;
#ifndef CVMCPU_EE_REG
	CVMRMResource *eeRes =
	    CVMRMgetResource(CVMRM_INT_REGS(con),
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	eeReg = CVMRMgetRegisterNumber(eeRes);
	/* Get the ee: */
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeReg,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeReg = CVMCPU_EE_REG;
#endif
	/* Set the ee's microlock depth. We just set it to the result
	 * of this intrinsic, which will be 0 or 1. */
	CVMJITaddCodegenComment((con, "ee->microLock = <result>"));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
            destRegID, eeReg, offsetof(CVMExecEnv, microLock));
#ifndef CVMCPU_EE_REG
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    }
    /* 
     * The following debugging code is disabled for now, but can be enabled
     * if Simple Sync methods are suspected of causing problems, like a
     * deadlock or assert.
     */
#if 0
    {
	/* Store the mb of the currently executing Simple Sync method into
	 * CVMglobals.jit.currentSimpleSyncMB. */
	CVMRMResource* currentSimpleSyncMBRes;
	CVMRMResource* simpleSyncMBRes;
	CVMJITMethodContext* mc = con->inliningStack[con->inliningDepth-1].mc;
	/* load CVMglobals.jit.currentSimpleSyncMB address into a register */
	CVMJITsetSymbolName((con, "&CVMglobals.jit.currentSimpleSyncMB"));
	currentSimpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)&CVMglobals.jit.currentSimpleSyncMB);
	/* load the Simple Sync mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M", mc->cb, mc->mb));
	simpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)mc->mb);
	/* Store the Simple Sync mb into CVMglobals.jit.currentSimpleSyncMB. */
	CVMJITaddCodegenComment((con,
				 "CVMglobals.jit.currentSimpleSyncMB = %C.%M",
				 mc->cb, mc->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(simpleSyncMBRes),
	    CVMRMgetRegisterNumber(currentSimpleSyncMBRes), 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), currentSimpleSyncMBRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), simpleSyncMBRes);
    }
    {
	/* Store the mb of the currently executing method into
	 * CVMglobals.jit.currentMB. */
	CVMRMResource* currentMBRes;
	CVMRMResource* mbRes;
	/* load CVMglobals.jit.currentMB address into a register */
	CVMJITsetSymbolName((con, "&CVMglobals.jit.currentMB"));
	currentMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)&CVMglobals.jit.currentMB);
	/* load the current mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M",
			     CVMmbClassBlock(con->mb), con->mb));
	mbRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)con->mb);
	/* Store the Simple Sync mb into CVMglobals.jit.currentMB. */
	CVMJITaddCodegenComment((con,
				 "CVMglobals.jit.currentMB = %C.%M",
				 CVMmbClassBlock(con->mb), con->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(mbRes),
	    CVMRMgetRegisterNumber(currentMBRes), 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), currentMBRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbRes);
    }
#endif /* 0 */
#endif /* CVM_DEBUG */

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), microLock);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

/*
 * Intrinsic emitter for microlock version of CVM.simpleLockRelease().
 *
 * Stores CVM_MICROLOCK_UNLOCKED into CVMglobals.objGlobalMicroLock.
 */
static void
simpleLockReleaseEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* tmp;
    CVMRMResource* microLock;
    int tmpRegID, microLockRegID;

    popResource(con); /* pop the "this" argument, which we don't use */
    tmp = CVMRMgetResource(CVMRM_INT_REGS(con),
			   CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);

    tmpRegID = CVMRMgetRegisterNumber(tmp);

    /* get CVM_MICROLOCK_UNLOCKED value */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitLoadConstant(con, tmpRegID, CVM_MICROLOCK_UNLOCKED);

#ifdef CVM_DEBUG
    /* Set the ee's microlock depth to 0, which is the same as
     * CVM_MICROLOCK_UNLOCKED. */
    {
	int eeReg;
#ifndef CVMCPU_EE_REG
	CVMRMResource *eeRes =
	    CVMRMgetResource(CVMRM_INT_REGS(con),
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	eeReg = CVMRMgetRegisterNumber(eeRes);
	/* Get the ee: */
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeReg,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeReg = CVMCPU_EE_REG;
#endif
	CVMassert(CVM_MICROLOCK_UNLOCKED == 0);
	CVMJITaddCodegenComment((con, "ee->microLock = 0"));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
					   tmpRegID, eeReg,
					   offsetof(CVMExecEnv, microLock));
#ifndef CVMCPU_EE_REG
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    }
#endif /* CVM_DEBUG */

    /* load microlock address into microLockRegID */
    CVMJITsetSymbolName((con, "&CVMglobals.objGlobalMicroLock"));
    microLock = CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
				  CVMRM_ANY_SET, CVMRM_SAFE_SET,
				  (CVMUint32)&CVMglobals.objGlobalMicroLock);
    microLockRegID = CVMRMgetRegisterNumber(microLock);
    CVMCPUemitMemBarRelease(con);
    /* release the microlock */
    CVMJITaddCodegenComment((con,
	"CVMglobals.objGlobalMicroLock = CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
				       tmpRegID, microLockRegID, 0);
    CVMCPUemitMemBar(con);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), tmp);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), microLock);
}

#elif CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS

/*
 * Intrinsic emitter for fastlock CAS version of CVM.simpleLockGrab().
 *
 * Attempts to lock the object using the reserved 
 * ee->simpleSyncReservedOwnedMonitor. If the object is already locked,
 * returns FALSE. Otherwise returns TRUE.
 */

static void
simpleLockGrabEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* lockRec;
    CVMRMResource* dest;
    int objRegID, objHdrRegID, lockRecRegID, destRegID, eeRegID;
    int fixupPC1, fixupPC2; /* To patch the conditional branches */
#ifndef CVMCPU_EE_REG
    CVMRMResource *eeRes;
#endif

    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);

    if (CVMRMisConstant(obj) && CVMRMgetConstant(obj) == 0) {
	/* We know the object is a null, so an NPE would be thrown before
	 * code generated here is executed. Therefore we don't need to
	 * generate anything. We'll stuff the obj resource into the
	 * intrinisic node just to keep JCS happy.
	 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), obj, intrinsicNode);
	pushResource(con, obj);
	return;
    }

#ifndef CVMCPU_EE_REG
    eeRes = CVMRMgetResource(
        CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
#endif

    /* get all our resources ready */
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), obj, CVMRM_ANY_SET, CVMRM_SAFE_SET);
    objHdr = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);
    lockRec = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);

    /* ... and get all the register numbers */
    objRegID    = CVMRMgetRegisterNumber(obj);
    objHdrRegID = CVMRMgetRegisterNumber(objHdr);
    lockRecRegID = CVMRMgetRegisterNumber(lockRec);
    destRegID   = CVMRMgetRegisterNumber(dest);

#ifdef CVM_DEBUG
    CVMJITprintCodegenComment(("DEBUG-ONLY CODE"));
    {
	/* Store the mb of the currently executing Simple Sync method into
	 * ee->currentSimpleSyncMB. */
	CVMRMResource* simpleSyncMBRes;
	CVMJITMethodContext* mc = con->inliningStack[con->inliningDepth-1].mc;
	/* load the Simple Sync mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M", mc->cb, mc->mb));
	simpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)mc->mb);
	/* Get the ee: */
#ifndef CVMCPU_EE_REG
	eeRegID = CVMRMgetRegisterNumber(eeRes);
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeRegID = CVMCPU_EE_REG;
#endif
	/* Store the Simple Sync mb into ee->currentSimpleSyncMB. */
	CVMJITaddCodegenComment((con,
				 "ee->currentSimpleSyncMB = %C.%M",
				 mc->cb, mc->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(simpleSyncMBRes), eeRegID,
	     CVMoffsetof(CVMExecEnv,currentSimpleSyncMB));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), simpleSyncMBRes);
    }
    {
	/* Store the mb of the currently executing method into
	 * ee->currentMB. */
	CVMRMResource* mbRes;
	/* load the current mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M",
			     CVMmbClassBlock(con->mb), con->mb));
	mbRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)con->mb);
	/* Store the Simple Sync mb into ee->currentMB. */
	CVMJITaddCodegenComment((con,
				 "ee->currentMB = %C.%M",
				 CVMmbClassBlock(con->mb), con->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(mbRes), 
            eeRegID, CVMoffsetof(CVMExecEnv,currentMB));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbRes);
    }
    CVMJITprintCodegenComment(("END OF DEBUG-ONLY CODE"));
#endif /* CVM_DEBUG */

    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* assume object locked and set intrinsic result */
    CVMJITaddCodegenComment((con, "assume locked: result = false"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_FALSE);
    /* get sync bits from object header, borrow lockRecRegID for now */
    CVMJITaddCodegenComment((con, "get obj sync bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
				lockRecRegID, objHdrRegID,
				CVM_SYNC_MASK, CVMJIT_NOSETCC);
    /* check if object is unlocked */
    CVMJITaddCodegenComment((con, "check if obj unlocked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      lockRecRegID, CVM_LOCKSTATE_UNLOCKED);
    /* branch if object is locked */
    CVMJITaddCodegenComment((con,"br simpleLockGrabDone if object is locked"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_NE);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* Get the ee: */
#ifndef CVMCPU_EE_REG
    eeRegID = CVMRMgetRegisterNumber(eeRes);
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
				     CVMoffsetof(CVMCCExecEnv, eeX));
#else
    eeRegID = CVMCPU_EE_REG;
#endif
    /* Store the object pointer into the lock record */
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.object = obj"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
        objRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.object));
    /* Store the object header word into the lock record */
    CVMJITaddCodegenComment((con,
       "ee->simpleSyncReservedOwnedMonitor.u.fast.bits = obj->hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
        objHdrRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.u.fast.bits));
    /* compute address of lock record */
    CVMJITaddCodegenComment((con, "compute address of lock record fast.bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
	lockRecRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor),
	CVMJIT_NOSETCC);
    /* CAS the lock record pointer into the object header. The PC of the
     * failure branch is returned by CVMCPUemitAtomicCompareAndSwap. */
    CVMJITprintCodegenComment((
	"CAS the lock record pointer into the object header"));
    CVMJITsetSymbolName((con, "simpleLockGrabDone"));/* name of branch label */
    fixupPC2 = CVMCPUemitAtomicCompareAndSwap(con,
	objRegID, CVMoffsetof(CVMObjectHeader,various32),
        objHdrRegID, lockRecRegID);
    /* Success if we fall through to here. Return true. */
    CVMJITaddCodegenComment((con, "success: result = true"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_TRUE);
    /* "done" target for failure branches. */
    CVMtraceJITCodegen(("\t\tsimpleLockGrabDone:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    CVMJITfixupAddress(con, fixupPC2, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

#ifndef CVMCPU_EE_REG
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lockRec);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

/*
 * Intrinsic emitter for fastlock CAS version of  CVM.simpleLockRelease().
 *
 * Attempts to unlock the object by swapping in the old bits, expecting
 * the reserved ee->simpleSyncReservedOwnedMonitor pointer to be swapped
 * out. If the atomic CAS of these values failed, then in if defers
 * to the CVMCCMruntimeSimpleSyncUnlock() helper.
 */
static void
simpleLockReleaseEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* lockRec;
    CVMRMResource* eeRes;
    int objRegID, objHdrRegID, lockRecRegID, eeRegID;
    int fixupPC1, fixupPC2; /* To patch the conditional branches */

    if (CVMRMisConstant(obj) && CVMRMgetConstant(obj) == 0) {
	/* We know the object is a null, so an NPE would be thrown before
	 * code generated here is executed. Therefore we don't need to
	 * generate anything.
	 */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
	return;
    }

    /* Make sure obj is in ARG2 in case we call C helper */
    if (CVMRMgetRegisterNumberUnpinned(obj) != CVMCPU_ARG2_REG) {
	CVMJITprintCodegenComment(("ARG2 = obj"));
    }
    CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), obj, CVMCPU_ARG2_REG);
    objRegID = CVMRMgetRegisterNumber(obj);
    CVMassert(objRegID == CVMCPU_ARG2_REG);

    /* Be sure to spill ARG1. We will load it with the ee later if needed.
     * ARG2 since it already contains obj. */
    CVMRMmajorSpill(con, ARG2, CVMRM_SAFE_SET);

    eeRes = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_ARG1_REG, 1);
    objHdr = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);
    lockRec = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);

    objHdrRegID = CVMRMgetRegisterNumber(objHdr);
    lockRecRegID = CVMRMgetRegisterNumber(lockRec);

#ifndef CVMCPU_EE_REG
    /* Get the ee: */
    eeRegID = CVMRMgetRegisterNumber(eeRes);
    CVMassert(eeRegID == CVMCPU_ARG1_REG);
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
				     CVMoffsetof(CVMCCExecEnv, eeX));
#else
    eeRegID = CVMCPU_EE_REG;
#endif

    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* compute address of lock record */
    CVMJITaddCodegenComment((con, "compute address of lock record fast.bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
	lockRecRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor),
	CVMJIT_NOSETCC);
    /* Note the lock state bits for "locked" must be 0 in order for the
     * CAS below to work without first checking the lock state bits. */
    CVMassert(CVM_LOCKSTATE_LOCKED == 0);
    /* get old object header word from lock record. Note, it will have been
     * overwritten if inflated. */
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.u.fast.bits"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        objHdrRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.u.fast.bits));
    /* CAS the old header word back into the object header, requiring
       that the swapped out word be the unmodified lock record pointer */
    CVMJITprintCodegenComment((
	"CAS the old header word back into the object header, requiring"));
    CVMJITprintCodegenComment((
	"that the swapped out word be the unmodified lock record pointer"));
    CVMJITsetSymbolName((con, "simpleLockReleaseFailed")); /* branch label */
    fixupPC2 = CVMCPUemitAtomicCompareAndSwap(con,
	objRegID, CVMoffsetof(CVMObjectHeader,various32),
	lockRecRegID, objHdrRegID);
    /* If we get here, then success. br simpleLockReleaseDone. */
    CVMJITaddCodegenComment((con, "br simpleLockReleaseDone"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_AL);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC1 = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* failed: target for failed CAS */
    CVMtraceJITCodegen(("\t\tsimpleLockReleaseFailed:\n"));
    CVMJITfixupAddress(con, fixupPC2, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

    /* Setup arguments for C helper that will do unlock */
#ifdef CVMCPU_EE_REG
    CVMJITaddCodegenComment((con, "ARG1 = ee"));
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
			   CVMCPU_ARG1_REG, CVMCPU_EE_REG, CVMJIT_NOSETCC);
#else
    CVMJITprintCodegenComment(("ee already in ARG1"));
#endif
    CVMJITprintCodegenComment(("obj already in ARG2"));

    /* Call C helper to do unlock */
    CVMJITaddCodegenComment((con, "call CVMCCMruntimeSimpleSyncUnlockGlue"));
    CVMJITsetSymbolName((con, "CVMCCMruntimeSimpleSyncUnlockGlue"));
    CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeSimpleSyncUnlock);
    CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeSimpleSyncUnlockGlue,
                           CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);
    CVMJITcsBeginBlock(con);
    CVMJITcaptureStackmap(con, 0);

    /* "simpleLockReleaseDone" target for branches. */
    CVMtraceJITCodegen(("\t\tsimpleLockReleaseDone:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
#ifdef CVM_DEBUG
    /* NULL out object pointer in the lock record */
    CVMJITprintCodegenComment(("DEBUG-ONLY CODE"));
#ifndef CVMCPU_EE_REG
    /* Get the ee. It could have been clobbered by the call to
       CVMCCMruntimeSimpleSyncUnlockGlue */
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
				     CVMoffsetof(CVMCCExecEnv, eeX));
#endif
    CVMCPUemitLoadConstant(con, objHdrRegID, 0);
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.object = NULL"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
        objHdrRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.object));
    CVMJITprintCodegenComment(("END OF DEBUG-ONLY CODE"));
#endif

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lockRec); 
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
}

#else
#error Unsupported locking type for CVMJIT_SIMPLE_SYNC_METHODS
#endif
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */


static void
doIntMinMax(
    CVMJITCompilationContext * con,
    CVMBool	min,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    int cclhs, ccrhs;
    int destRegID, lhsRegID, rhsRegID;
    if (min){
        cclhs = CVMCPU_COND_LE;
        ccrhs = CVMCPU_COND_GT;
    } else {
        cclhs = CVMCPU_COND_GE;
        ccrhs = CVMCPU_COND_LT;
    }
    /* TODO: Reduce instruction count by looking at refcounts. If either
     * the lhs or rhs have a refcount of 1 (and are not a local that we
     * want to keep in a register), then we can save a move by resuing
     * it as the dest register. For example:
     *    cmp    lhs, rhs
     *    movle  lhs, rhs
     * And then lhs is repurposed as the "dest" resource. Note, if we
     * reuse rhs, then we need to reverse the compare and reverse the mov.
     */
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    destRegID = CVMRMgetRegisterNumber(dest);
    lhsRegID = CVMRMgetRegisterNumber(lhs);
    rhsRegID = CVMRMgetRegisterNumber(rhs);
#ifdef CVMCPU_HAS_CONDITIONAL_ALU_INSTRUCTIONS
    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, cclhs,
			      lhsRegID, rhsRegID);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      lhsRegID, CVMJIT_NOSETCC, cclhs);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      rhsRegID, CVMJIT_NOSETCC, ccrhs),
#else
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, destRegID,
			   lhsRegID, CVMJIT_NOSETCC);
    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, ccrhs,
			      lhsRegID, rhsRegID);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      rhsRegID, CVMJIT_NOSETCC, ccrhs),
#endif
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

static void
imaxEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    doIntMinMax(con, CVM_FALSE, intrinsicNode, GET_REGISTER_GOALS);
}

static void
iminEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    doIntMinMax(con, CVM_TRUE, intrinsicNode, GET_REGISTER_GOALS);
}

const CVMJITIntrinsicEmitterVtbl
   CVMJITRISCintrinsicThreadCurrentThreadEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    java_lang_Thread_currentThread_EmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIAbsEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    iabsEmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIMaxEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    imaxEmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIMinEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    iminEmitOperator,
};

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicSimpleLockGrabEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    simpleLockGrabEmitter,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicSimpleLockReleaseEmitter =
{
    CVMJITRISCintrinsicSimpleLockReleaseGetRequired,
    CVMJITRISCintrinsicSimpleLockReleaseGetArgTarget,
    simpleLockReleaseEmitter,
};
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */

#endif /* CVMJIT_INTRINSICS */

%}

reg32: IADD32 reg32 aluRhs : 10 : : : :
        wordBinaryOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS);
reg32: ISUB32 reg32 aluRhs : 10 : : : :
        wordBinaryOp(con, CVMCPU_SUB_OPCODE, $$, GET_REGISTER_GOALS);
reg32: AND32  reg32 aluRhs : 10 : : : :
        wordBinaryOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS);
reg32: OR32   reg32 aluRhs : 10 : : : :
        wordBinaryOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS);
reg32: XOR32  reg32 aluRhs : 10 : : : :
        wordBinaryOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS);
// reg32: AND32 reg32 NOT32 aluRhs : 1 : : : 
//       wordBinaryOp(CVMCPU_BIC_OPCODE, $$, GET_REGISTER_GOALS, con);

%{
#define IS_POWER_OF_2(x) (((x) & ((x) - 1)) == 0)

/* Purpose: Attempts to apply strength reduction on an IMul by a constant. */
static void
doIMulByIConst32(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
                 CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *lhs = popResource(con);
    CVMJITIRNode *constNode;
    CVMRMResource *dest;
    CVMInt32 value;
    CVMBool constantIsNegative = CVM_FALSE;

    constNode = CVMJITirnodeGetRightSubtree(thisNode);
    value = CVMJITirnodeGetConstant32(constNode)->j.i;

    if (value < 0) {
        value = -value;
        constantIsNegative = CVM_TRUE;
    }

    if (value == 0) {
        dest = CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), 0);

    } else if (value == 1) {
        if (constantIsNegative) {
            /* Reduce multiply into negate: */
            dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
            CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

            CVMJITaddCodegenComment((con, "imul by -1"));
            /*  neg  rDest, rLhs */
            CVMCPUemitUnaryALU(con, CVMCPU_NEG_OPCODE,
                CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(lhs),
                CVMJIT_NOSETCC);
        } else {
            dest = lhs;

            /* Reduce multiply into nothing: */
            CVMJITprintCodegenComment(("imul by 1: Do nothing."));
            CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
            pushResource(con, dest);
            return;
        }

    } else if (IS_POWER_OF_2(value) || IS_POWER_OF_2(value - 1)) {
        int lhsReg, destReg;
        CVMInt32 i;
        CVMBool isPowerOf2 = IS_POWER_OF_2(value);

        CVMJITaddCodegenComment((con, "imul by %d",
                                 CVMJITirnodeGetConstant32(constNode)->j.i));

        /* Reduce multiply into shifts and adds: */
        if (!isPowerOf2) {
            value--;
        }
        for (i = 1; i < 32; i++) {
            if ((1 << i) == value) {
                break;
            }
        }
        CVMassert((i < 32 && constantIsNegative) ||
                  (i < 31 && !constantIsNegative));

        dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
        CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        lhsReg = CVMRMgetRegisterNumber(lhs);
        destReg = CVMRMgetRegisterNumber(dest);
        if (constantIsNegative) {
            /* neg  rDest, rLhs */
            CVMCPUemitUnaryALU(con, CVMCPU_NEG_OPCODE, destReg, lhsReg,
			       CVMJIT_NOSETCC);

            if (isPowerOf2) {
                /* rDest = rDest << #i; */
                CVMCPUemitShiftByConstant(con, CVMCPU_SLL_OPCODE,
                                          destReg, destReg, i);
            } else {
                /* rDest = (rDest << #i) + rDest; */
                CVMCPUemitShiftAndAdd(con, CVMCPU_SLL_OPCODE,
				      destReg, destReg, destReg, i);
            }
        } else {
            if (isPowerOf2) {
                /* rDest = rLhs << #i; */
                CVMCPUemitShiftByConstant(con, CVMCPU_SLL_OPCODE,
                                          destReg, lhsReg, i);
            } else {
                /* rDest = (rLhs << #i) + rLhs; */
                CVMCPUemitShiftAndAdd(con, CVMCPU_SLL_OPCODE,
				      destReg, lhsReg, lhsReg, i);
            }
        }

    } else {
        /* Do multiply: */
	int destRegID;
	int lhsRegID;

        dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
	destRegID = CVMRMgetRegisterNumber(dest);
	CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	lhsRegID = CVMRMgetRegisterNumber(lhs);

        if (constantIsNegative) {
            value = -value;
        }
#ifdef CVMCPU_HAS_IMUL_IMMEDIATE
	if (CVMCPUalurhsIsEncodableAsImmediate(CVMCPU_MULL_OPCODE, value)) {
	    CVMCPUemitMulConstant(con, destRegID, lhsRegID, value);
	} else
#endif
	{
	    CVMRMResource *constRes =
		CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), value);
	    CVMRMpinResource(CVMRM_INT_REGS(con), constRes,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	    CVMCPUemitMul(con, CVMCPU_MULL_OPCODE,
			  destRegID, lhsRegID,
			  CVMRMgetRegisterNumber(constRes),
			  CVMCPU_INVALID_REG);
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), constRes);
	}
    }

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    pushResource(con, dest);
}

/* Purpose: Attempts to apply strength reduction on an IDiv/IRem by a
            constant. */
static void
doIDivOrIRemByIConst32(CVMJITCompilationContext *con,
                       CVMJITIRNodePtr thisNode,
                       CVMRMregset target, CVMRMregset avoid,
                       CVMBool isIDiv)
{
    CVMJITIRNode *rhsNode = CVMJITirnodeGetRightSubtree(thisNode);
    CVMInt32 value = CVMJITirnodeGetConstant32(rhsNode)->j.i;
    CVMBool divisorIsNegative = CVM_FALSE;

    if (value < 0) {
        value = -value;
        divisorIsNegative = CVM_TRUE;
    }

    if (value == 0) {
        CVMRMResource *lhs = popResource(con);
        CVMJITprintCodegenComment(("idiv by 0"));
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeThrowDivideByZeroGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
        CVMJITcsBeginBlock(con);
        /* We push the incoming operand as the result because the rest of
           the code to be compiled still expects a result.  They won't know
           that we're going to throw a DivideByZeroException. */
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), lhs, thisNode);
        pushResource(con, lhs);
        return;

    } else if (value == 1) {
        CVMRMResource *lhs = popResource(con);
        CVMRMResource *dest;
        if (isIDiv) {
            if (divisorIsNegative) {
                /* Reduce divide into negate: */
                dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
                CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
				 CVMRM_ANY_SET, CVMRM_EMPTY_SET);

                CVMJITaddCodegenComment((con, "idiv by -1"));
                /*  neg  rDest, rLhs */
                CVMCPUemitUnaryALU(con, CVMCPU_NEG_OPCODE,
                    CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(lhs),
                    CVMJIT_NOSETCC);
                CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
                CVMRMunpinResource(CVMRM_INT_REGS(con), dest);
            } else {
                CVMJITprintCodegenComment(("idiv by 1: Do nothing"));
                dest = lhs;
            }
        } else {
            CVMJITprintCodegenComment(("irem by %d: Do nothing",
                CVMJITirnodeGetConstant32(rhsNode)->j.i));
            dest = CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), 0);
            CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
        }
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
        pushResource(con, dest);
        return;

    } else if (IS_POWER_OF_2((CVMUint32)value)) {
	/*
	 * For IDIV:
	 *    sra   rDest, rSrc, #31
	 *    srl   rDest, rDest, #32-log2(value)
	 *    add   rDest, rSrc, rDest
	 *    sra   rDest, rDest, #log2(value)
	 *    neg   rDest, rDest   <--- Only if divisor is negative
	 *
	 * For IREM:
	 *    sra   rDest, rSrc, #31
	 *    srl   rDest, rDest, #32-log2(value)
	 *    add   rDest, rSrc, rDest
	 *    bic   rDest, rDest, #(1<<log2(value))-1
	 *    sub   rDest, rSrc, rDest
	 *
	 * This is a fairly generic solution that will work reasonably
	 * well on most platforms. It appears to be optimal for ARM and
	 * PowerPC. Sparc and MIPS could do a bit better if they had
	 * their own emitter for this, so maybe we should consider adding
	 * an optional CVMCPUemitDivOrRemByPowerOf2() emitter.
	 */
        CVMRMResource *lhs = popResource(con);
        CVMRMResource *dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       target, avoid, 1);
        int lhsReg, destReg;
        CVMInt32 i;

        for (i = 1; i < 32; i++) {
            if ((1 << i) == value) {
                break;
            }
        }
        CVMassert((divisorIsNegative && i < 32) ||
                  (!divisorIsNegative && i < 31));

        CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        lhsReg = CVMRMgetRegisterNumber(lhs);
        destReg = CVMRMgetRegisterNumber(dest);

        CVMJITprintCodegenComment(("do %s by %d:", (isIDiv ? "idiv" : "irem"),
                                   CVMJITirnodeGetConstant32(rhsNode)->j.i));

	/* sra   rDest, rSrc, #31 */
	CVMCPUemitShiftByConstant(con, CVMCPU_SRA_OPCODE, destReg, lhsReg, 31);
	/* IAI-02 */
	/* rDest = (rDest << #32-log2(value)) + rSrc */
	CVMCPUemitShiftAndAdd(con, CVMCPU_SRL_OPCODE,
			      destReg, destReg, lhsReg, 32 - i);
        if (isIDiv) {
	    /* sra   rDest, rDest, #log2(value) */
	    CVMCPUemitShiftByConstant(con, CVMCPU_SRA_OPCODE,
				      destReg, destReg, i);
	    /* neg   rDest, rDest */
	    if (divisorIsNegative) {
                CVMCPUemitUnaryALU(con, CVMCPU_NEG_OPCODE,
				   destReg, destReg, CVMJIT_NOSETCC);
	    }
        } else {
	    /* bic   rDest, rDest, #log2(value) */
	    CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
					destReg, destReg, (1<<i)-1,
					CVMJIT_NOSETCC);
	    /* sub   rDest, rSrc, rDest */
	    CVMCPUemitBinaryALURegister(con, CVMCPU_SUB_OPCODE,
					destReg, lhsReg, destReg,
					CVMJIT_NOSETCC);
        }

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
        pushResource(con, dest);
        return;

    } else {
        CVMRMResource *lhs;
        CVMRMResource *dest;
        CVMRMResource *temp;
        CVMRMResource *inverse;
        CVMInt32 i;
        CVMJavaLong magic;
        CVMJavaLong long1 = CVMlongConstOne();
        CVMInt32 inverseValue;
        int lhsReg, destReg, tmpReg, invReg;

        /* NOTE: i can never be less than 2 because we know that value > 2: */
        for (i = 2; i < 32; i++) {
            if ((CVMUint32)(1 << i) > (CVMUint32)value) {
                break;
            }
        }
        i--;

        CVMJITprintCodegenComment(("do %s by %d:", (isIDiv ? "idiv" : "irem"),
                                   CVMJITirnodeGetConstant32(rhsNode)->j.i));

        /* ulong magic = ( (ulong)1 << (32 + i)) - 1; */
        magic = CVMlongSub(CVMlongShl(long1, 32+i), long1);
        /* uint inverse = (uint)(magic / divisor) + 1; */
        inverseValue = CVMlong2Int(CVMlongDiv(magic, CVMint2Long(value))) + 1;

	/*
	 * If the inverseValue is even, then we can cheat and shift it
	 * right by one and shift right by one less later on. This allows
	 * us to avoid the add of the dividend later on.
	 */
	if ((inverseValue & 1) == 0) {
	    inverseValue >>= 1;
	    inverseValue &= 0x7fffffff;
	    i--;
	}

        lhs = popResource(con);
        dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
        temp = CVMRMgetResource(CVMRM_INT_REGS(con),
				CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
        inverse = CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con),
						 inverseValue);
        CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), inverse,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), temp,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        lhsReg = CVMRMgetRegisterNumber(lhs);
        destReg = CVMRMgetRegisterNumber(dest);
        tmpReg = CVMRMgetRegisterNumber(temp);
        invReg = CVMRMgetRegisterNumber(inverse);

	/* sra   destReg,lhsReg,#31 */
        CVMCPUemitShiftByConstant(con, CVMCPU_SRA_OPCODE, destReg, lhsReg, 31);
	/* mulh  tmpReg,lhsReg,invReg */
        CVMCPUemitMul(con, CVMCPU_MULH_OPCODE,
		      tmpReg, lhsReg, invReg, CVMCPU_INVALID_REG);
	/* We only need to add if the inverseValue is still negative */
	if ((inverseValue & 0x80000000) != 0) {
	    /* add   tmpReg,tmpReg,lhsReg */
	    CVMCPUemitBinaryALURegister(con, CVMCPU_ADD_OPCODE,
					tmpReg, tmpReg, lhsReg,
					CVMJIT_NOSETCC);
	}
	/* sra   tmpReg,tmpReg,#<i> */
        CVMCPUemitShiftByConstant(con, CVMCPU_SRA_OPCODE, tmpReg, tmpReg, i);
	/* reverse subtract for neg divisor so we don't need to emit a neg */
	if (isIDiv && divisorIsNegative) {
	    /* sub   destReg,destReg,tmpReg */
	    CVMCPUemitBinaryALURegister(con, CVMCPU_SUB_OPCODE,
					destReg, destReg, tmpReg,
					CVMJIT_NOSETCC);
	} else {
	    /* sub   destReg,tmpReg,destReg */
	    CVMCPUemitBinaryALURegister(con, CVMCPU_SUB_OPCODE,
					destReg, tmpReg, destReg,
					CVMJIT_NOSETCC);
	}

	/* compute remainder if necessary */
        if (!isIDiv) {
            /*
	     * mul tmpReg,destReg,<value>
	     * sub destReg,lhsReg,tmpReg
	     *
	     * TOOD: try to make use of doIMulByIConst32().
             */
#ifdef CVMCPU_HAS_IMUL_IMMEDIATE
	    if (CVMCPUalurhsIsEncodableAsImmediate(CVMCPU_MULL_OPCODE, value)){
		CVMCPUemitMulConstant(con, tmpReg, destReg, value);
	    } else
#endif
	    {
		CVMRMResource *divisor =
		    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), value);
		CVMRMpinResource(CVMRM_INT_REGS(con), divisor,
				 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
		CVMCPUemitMul(con, CVMCPU_MULL_OPCODE,
			      tmpReg, destReg,
			      CVMRMgetRegisterNumber(divisor),
			      CVMCPU_INVALID_REG);
		CVMRMrelinquishResource(CVMRM_INT_REGS(con), divisor);
	    }
	    CVMCPUemitBinaryALURegister(con, CVMCPU_SUB_OPCODE,
					destReg, lhsReg, tmpReg,
					CVMJIT_NOSETCC);
        }

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), temp);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), inverse);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
        pushResource(con, dest);
        return;
    }
}

#undef IS_POWER_OF_2

%}

reg32: IMUL32  reg32 reg32 : 40 : : : : {
	CVMRMResource* rhs = popResource(con);
	CVMRMResource* lhs = popResource(con);
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 1);
	CVMRMpinResource(CVMRM_INT_REGS(con), lhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMRMpinResource(CVMRM_INT_REGS(con), rhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMCPUemitMul(con, CVMCPU_MULL_OPCODE, CVMRMgetRegisterNumber(dest),
	    CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs),
	    CVMCPU_INVALID_REG);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

reg32: IMUL32 reg32 ICONST_32 : 40 : : : : {
        doIMulByIConst32(con, $$, GET_REGISTER_GOALS);
    };

reg32: IDIV32  reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeIDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeIDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeIDiv);
	/* only do a "checkZero" if using the default C helper */
#ifdef CVMCCM_HAVE_PLATFORM_SPECIFIC_IDIV
        wordBinaryHelper(con, (void*)CVMCCMruntimeIDiv, $$, CVM_FALSE);
#else
        wordBinaryHelper(con, (void*)CVMCCMruntimeIDiv, $$, CVM_TRUE);
#endif
    };

reg32: IDIV32 reg32 ICONST_32 : 40 : : : : {
        doIDivOrIRemByIConst32(con, $$, GET_REGISTER_GOALS, CVM_TRUE);
    };

reg32: IREM32  reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeIRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeIRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeIRem);
	/* only do a "checkZero" if using the default C helper */
#ifdef CVMCCM_HAVE_PLATFORM_SPECIFIC_IREM
        wordBinaryHelper(con, (void*)CVMCCMruntimeIRem, $$, CVM_FALSE);
#else
        wordBinaryHelper(con, (void*)CVMCCMruntimeIRem, $$, CVM_TRUE);
#endif
    };

reg32: IREM32 reg32 ICONST_32 : 40 : : : : {
        doIDivOrIRemByIConst32(con, $$, GET_REGISTER_GOALS, CVM_FALSE);
    };

// Purpose: valueFloat = -valueFloat.
reg32: FNEG reg32 : 90 : SET_AVOID_C_CALL($$); : SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFNeg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFNeg"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFNeg);
        unaryHelper(con, (void*)CVMCCMruntimeFNeg, $$, ARG1, 1);
    };

// Purpose: valueFloat = valueFloat + valueFloat.
reg32: FADD reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFAdd"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFAdd"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFAdd);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFAdd, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat - valueFloat.
reg32: FSUB reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFSub"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFSub"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFSub);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFSub, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat * valueFloat.
reg32: FMUL reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFMul"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFMul"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFMul);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFMul, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat / valueFloat.
reg32: FDIV reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFDiv);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFDiv, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat % valueFloat.
reg32: FREM reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFRem);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFRem, $$, CVM_FALSE);
    };

%{
static void
shortenInt(
    CVMJITCompilationContext* con,
    int rightshiftop,
    int shiftwidth,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* src = popResource(con);
    CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					   target, avoid, 1);
    int destregno = CVMRMgetRegisterNumber(dest);
    CVMRMpinResource(CVMRM_INT_REGS(con), src, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitShiftByConstant(con, CVMCPU_SLL_OPCODE, destregno,
                              CVMRMgetRegisterNumber(src), shiftwidth);
    CVMCPUemitShiftByConstant(con, rightshiftop, destregno,
                              destregno, shiftwidth);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    pushResource(con, dest);
}

%}

// Purpose: value32 = I2C(value32)
reg32: I2C reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2c:"));
        shortenInt(con, CVMCPU_SRL_OPCODE, 16, $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = I2S(value32)
reg32: I2S reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2s:"));
        shortenInt(con, CVMCPU_SRA_OPCODE, 16, $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = I2B(value32)
reg32: I2B reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2b:"));
        shortenInt(con, CVMCPU_SRA_OPCODE, 24, $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = ALENGTH(arrayObj)
reg32: ALENGTH reg32 : 10 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 1);
	CVMRMResource* src = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMJITaddCodegenComment((con, "arraylength"));
        CVMJITcsSetExceptionInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(src),
            ARRAY_LENGTH_OFFSET);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	pushResource(con, dest);
    };

// Purpose: valueDouble = (double) valueLong.
reg64: L2D reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeL2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeL2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeL2D);
        unaryHelper(con, (void*)CVMCCMruntimeL2D, $$, ARG1|ARG2, 2);
    };

// Purpose: valueFloat = (float) valueLong.
reg32: L2F reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeL2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeL2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeL2F);
        unaryHelper(con, (void*)CVMCCMruntimeL2F, $$, ARG1|ARG2, 1);
    };

// Purpose: valueInt = (int) valueLong.
reg32: L2I reg64 : 10 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 1);
	CVMRMResource* src = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMCPUemitLong2Int(con, CVMRMgetRegisterNumber(dest),
                           CVMRMgetRegisterNumber(src));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueDouble = (double) valueInt.
reg64: I2D reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeI2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeI2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeI2D);
        unaryHelper(con, (void*)CVMCCMruntimeI2D, $$, ARG1, 2);
    };

// Purpose: valueFloat = (float) valueInt.
reg32: I2F reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeI2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeI2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeI2F);
        unaryHelper(con, (void*)CVMCCMruntimeI2F, $$, ARG1, 1);
    };

// Purpose: valueLong = (long) valueInt.
reg64: I2L reg32 : 20 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 2);
	CVMRMResource* src = popResource(con);
	int destreg, srcreg;

	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	destreg = CVMRMgetRegisterNumber(dest);
	srcreg  = CVMRMgetRegisterNumber(src);
        CVMCPUemitInt2Long(con, destreg, srcreg);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueDouble = (double) valueFloat.
reg64: F2D reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2D);
        unaryHelper(con, (void*)CVMCCMruntimeF2D, $$, ARG1, 2);
    };

// Purpose: valueInt = (int) valueFloat.
reg32: F2I reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2I"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2I"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2I);
        unaryHelper(con, (void*)CVMCCMruntimeF2I, $$, ARG1, 1);
    };

// Purpose: valueLong = (long) valueFloat.
reg64: F2L reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2L"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2L"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2L);
        unaryHelper(con, (void*)CVMCCMruntimeF2L, $$, ARG1, 2);
    };

// Purpose: valueFloat = (float) valueDouble.
reg32: D2F reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2F);
        unaryHelper(con, (void*)CVMCCMruntimeD2F, $$, ARG1|ARG2, 1);
    };

// Purpose: valueInt = (int) valueDouble.
reg32: D2I reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2I"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2I"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2I);
        unaryHelper(con, (void*)CVMCCMruntimeD2I, $$, ARG1|ARG2, 1);
    };

// Purpose: valueLong = (long) valueDouble.
reg64: D2L reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2L"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2L"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2L);
        unaryHelper(con, (void*)CVMCCMruntimeD2L, $$, ARG1|ARG2, 2);
    };

%{

static void
longBinaryOp(
    CVMJITCompilationContext* con,
    int opcode,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid )
{
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 2);

    CVMCPUemitBinaryALU64(con, opcode, CVMRMgetRegisterNumber(dest),
        CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs));

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

%}

// Purpose: value64 = -value64.
reg64: INEG64 reg64 : 20 : : : : {
        CVMRMResource *src = popResource(con);
        CVMRMResource *dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 2);
        CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMCPUemitUnaryALU64(con, CVMCPU_NEG64_OPCODE,
            CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(src));
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

reg64: IADD64 reg64 reg64 : 20 : : : :
        longBinaryOp(con, CVMCPU_ADD64_OPCODE, $$, GET_REGISTER_GOALS);
reg64: ISUB64 reg64 reg64 : 20 : : : :
        longBinaryOp(con, CVMCPU_SUB64_OPCODE, $$, GET_REGISTER_GOALS);
reg64: AND64  reg64 reg64 : 20 : : : :
        longBinaryOp(con, CVMCPU_AND64_OPCODE, $$, GET_REGISTER_GOALS);
reg64: OR64   reg64 reg64 : 20 : : : :
        longBinaryOp(con, CVMCPU_OR64_OPCODE, $$, GET_REGISTER_GOALS);
reg64: XOR64  reg64 reg64 : 20 : : : :
        longBinaryOp(con, CVMCPU_XOR64_OPCODE, $$, GET_REGISTER_GOALS);
reg64: IMUL64 reg64 reg64 : 30 : : : :
        longBinaryOp(con, CVMCPU_MUL64_OPCODE, $$, GET_REGISTER_GOALS);

// Purpose: value64 = value64 / value64.
reg64: IDIV64  reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLDiv);
        longBinaryHelper(con, (void*)CVMCCMruntimeLDiv, $$, CVM_TRUE);
    };

// Purpose: value64 = value64 % value64.
reg64: IREM64  reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLRem);
        longBinaryHelper(con, (void*)CVMCCMruntimeLRem, $$, CVM_TRUE);
    };

%{
/* Purpose: Emits code for a shift operation with a const shiftAmount.  Also
            masks off the offset with 0x3f before shifting per VM spec.

   NOTE: It turns out that emitting these constant shifts inline does not
   speed things up when running kBench, Spec, or CM3.0 because the are
   used so infrequently. We could get rid if the ICONST_32 rhs rules and
   just let the reg32 rhs rules pin the constant to a register and call the
   shift helpers. However, it does speed up a loop that does nothing but
   shift of a 64-bit values by about 3X, so this implementation is left in
   just in case an application is run that will make heavy use of it.
*/
static void doInt64Shift(CVMJITCompilationContext *con,
			 int shiftOp1, int shiftOp2, int shiftOp3,
			 CVMJITIRNodePtr thisNode,
			 CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *src;
    CVMRMResource *dest;
    CVMInt32 shiftOffset =
        CVMJITirnodeGetConstant32(CVMJITirnodeGetRightSubtree(thisNode))->j.i;
    int destHI;
    int destLO;
    int srcHI;
    int srcLO;

    shiftOffset &= 0x3f;  /* mask higher bits per vm spec */

    if (shiftOffset == 0) {
	passLastEvaluated(con, CVMRM_INT_REGS(con), thisNode);
	return;  /* no code to emit in this case */
    }

    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 2);
    src = popResource(con);
    CVMRMpinResource(CVMRM_INT_REGS(con), src,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

#if CVM_ENDIANNESS == CVM_LITTLE_ENDIAN
    destLO = CVMRMgetRegisterNumber(dest);
    destHI = destLO + 1;
    srcLO = CVMRMgetRegisterNumber(src);
    srcHI = srcLO + 1;
#else
    destHI = CVMRMgetRegisterNumber(dest);
    destLO = destHI + 1;
    srcHI = CVMRMgetRegisterNumber(src);
    srcLO = srcHI + 1;
#endif

    /* We can do shifts of >=32 in fewer instructions */
    if (shiftOffset >= 32) {
	shiftOffset -= 32;
	if (shiftOp3 == CVMCPU_SRA_OPCODE) {        /* SRA */
	    CVMCPUemitShiftByConstant(con, shiftOp3, destHI, srcHI, 31);
	    CVMCPUemitShiftByConstant(con, shiftOp3, destLO, srcHI,
				      shiftOffset);
	} else if (shiftOp3 == CVMCPU_SRL_OPCODE) { /* SRL */
	    CVMCPUemitLoadConstant(con, destHI, 0);
	    CVMCPUemitShiftByConstant(con, shiftOp2, destLO, srcHI,
				      shiftOffset);
	} else {                                    /* SLL */
	    CVMCPUemitShiftByConstant(con, shiftOp3, destHI, srcLO,
				      shiftOffset);
	    CVMCPUemitLoadConstant(con, destLO, 0);
	}
    } else {
	CVMRMResource *scratchRes =
	    CVMRMgetResource(CVMRM_INT_REGS(con),
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	int scratch;
	int reverseShiftSrc;
	int reverseShiftDest;
	CVMRMpinResource(CVMRM_INT_REGS(con), scratchRes,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	scratch = CVMRMgetRegisterNumber(scratchRes);
	/* shiftOp1 shifts the reverse direction of shiftOp2 and shiftOp3,
	 * which are the true direction of the shift */
	if (shiftOp1 == CVMCPU_SRL_OPCODE) {    /* SLL */
	    reverseShiftSrc = srcLO;
	    reverseShiftDest = destHI;
	} else {  				/* SRL and SRA */
	    reverseShiftSrc = srcHI;
	    reverseShiftDest = destLO;
	}
	CVMCPUemitShiftByConstant(con, shiftOp1,
				  scratch, reverseShiftSrc, 32-shiftOffset);
	CVMCPUemitShiftByConstant(con, shiftOp2, destLO, srcLO, shiftOffset);
	CVMCPUemitShiftByConstant(con, shiftOp3, destHI, srcHI, shiftOffset);
	CVMCPUemitBinaryALURegister(con, CVMCPU_OR_OPCODE,
				    reverseShiftDest, reverseShiftDest,
				    scratch, CVMJIT_NOSETCC);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratchRes);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}
%}

// Purpose: value64 = value64 << (const32 & 0x3f).
reg64: SLL64 reg64 ICONST_32 : 90 : : : : {
    /* 
     * Implmented as follows if (const32 & 0x3f < 32) :
     *	  srl scratch,srcLO,  #32-<imm>
     *	  sll destLO, srcLO,  #<imm>
     *	  sll destHI, srcHI,  #<imm>
     *	  or  destHI, destHI, scratch
     *
     * Implmented as follows if (const32 & 0x3f >= 32) :
     *	  sll destHI, srcLO,  #<imm>-32
     *    mov destLO, #0
     */
    doInt64Shift(con, CVMCPU_SRL_OPCODE, CVMCPU_SLL_OPCODE, CVMCPU_SLL_OPCODE,
		 $$, GET_REGISTER_GOALS);
};
 
// Purpose: value64 = value64 >>> (const32 & 0x3f).
reg64: SRL64 reg64 ICONST_32 : 90 : : : : {
    /* 
     * Implmented as follows if (const32 & 0x3f < 32) :
     *	  sll scratch,srcHI,  #32-<imm>
     *	  srl destLO, srcLO,  #<imm>
     *	  srl destHI, srcHI,  #<imm>
     *	  or  destLO, destLO, scratch
     *
     * Implmented as follows if (const32 & 0x3f >= 32) :
     *	  srl destLO, srcHI,  #<imm>-32
     *    mov destHI, #0
     */
    doInt64Shift(con, CVMCPU_SLL_OPCODE, CVMCPU_SRL_OPCODE, CVMCPU_SRL_OPCODE,
		 $$, GET_REGISTER_GOALS);
};
 
// Purpose: value64 = value64 >> (const32 & 0x1f).
reg64: SRA64 reg64 ICONST_32 : 90 : : : : {
    /* 
     * Implmented as follows if (const32 & 0x3f < 32) :
     *	  sll scratch,srcHI,  #32-<imm>
     *	  srl destLO, srcLO,  #<imm>
     *	  sra destHI, srcHI,  #<imm>
     *	  or  destLO, destLO, scratch
     *
     * Implmented as follows if (const32 & 0x3f >= 32) :
     *	  sra destLO, srcHI,  #<imm>-32
     *    sra destHI, srcHI,  31
     */
    doInt64Shift(con, CVMCPU_SLL_OPCODE, CVMCPU_SRL_OPCODE, CVMCPU_SRA_OPCODE,
		 $$, GET_REGISTER_GOALS);
};

// Purpose: value64 = value64 << value32.
reg64: SLL64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLShl"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLShl"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLShl);
        longBinaryHelper2(con, (void*)CVMCCMruntimeLShl, $$, CVM_FALSE);
    };

// Purpose: value64 = value64 >> value32.
reg64: SRA64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLShr"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLShr"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLShr);
        longBinaryHelper2(con, (void*)CVMCCMruntimeLShr, $$, CVM_FALSE);
    };

// Purpose: value64 = value64 >>> value32.
reg64: SRL64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLUshr"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLUshr"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLUshr);
        longBinaryHelper2(con, (void*)CVMCCMruntimeLUshr, $$, CVM_FALSE);
    };

reg64: ICONST_64 : 40 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 2);
	int destregno = CVMRMgetRegisterNumber(dest);
	CVMJavaVal64 v64;
	CVMmemCopy64(v64.v, CVMJITirnodeGetConstant64($$)->j.v);
        CVMCPUemitLoadLongConstant(con, destregno, &v64);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueDouble = -valueDouble.
reg64: DNEG reg64 : 90 : SET_AVOID_C_CALL($$); : SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDNeg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDNeg"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDNeg);
        unaryHelper(con, (void*)CVMCCMruntimeDNeg, $$, ARG1|ARG2, 2);
    };

// Purpose: valueDouble = valueDouble + valueDouble.
reg64: DADD reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDAdd"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDAdd"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDAdd);
        longBinaryHelper(con, (void*)CVMCCMruntimeDAdd, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble - valueDouble.
reg64: DSUB reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDSub"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDSub"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDSub);
        longBinaryHelper(con, (void*)CVMCCMruntimeDSub, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble * valueDouble.
reg64: DMUL reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDMul"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDMul"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDMul);
        longBinaryHelper(con, (void*)CVMCCMruntimeDMul, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble / valueDouble.
reg64: DDIV reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDDiv);
        longBinaryHelper(con, (void*)CVMCCMruntimeDDiv, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble % valueDouble.
reg64: DREM reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDRem);
        longBinaryHelper(con, (void*)CVMCCMruntimeDRem, $$, CVM_FALSE);
    };

%{

/*
 * The descriptive table for each kind of array access.
 * This table is indexed by typeid. So the order of CVM_TYPEID constants
 * matters.
 *
 * For each entry, we have an index shift amount, whether this is a
 * ref entry, the appropriate load opcode, and the appropriate store
 * opcode for an array element of this type.
 */
typedef struct ArrayElemInfo ArrayElemInfo;
struct ArrayElemInfo {
    int      shiftAmount;  /* 2<<shiftAmount == elemSize */
    int      size;         /* Resultant size in words */
    CVMBool  isRef;
    int      loadOpcode;
    int      storeOpcode;
#ifdef CVM_JIT_USE_FP_HARDWARE
    int      floatLoadOpcode;
    int      floatStoreOpcode;
#endif
};

#define CVM_ILLEGAL_OPCODE -1

#ifdef CVM_JIT_USE_FP_HARDWARE
#define CVM_NONE	, CVM_ILLEGAL_OPCODE, CVM_ILLEGAL_OPCODE 
#define CVM_FLDST_NONE	, CVM_ILLEGAL_OPCODE, CVM_ILLEGAL_OPCODE
#define CVM_FLDST32	, CVMCPU_FLDR32_OPCODE, CVMCPU_FSTR32_OPCODE
#define CVM_FLDST64	, CVMCPU_FLDR64_OPCODE, CVMCPU_FSTR64_OPCODE
#else
#define CVM_NONE
#define CVM_FLDST_NONE
#define CVM_FLDST32
#define CVM_FLDST64
#endif
 
typedef struct ScaledIndexInfo ScaledIndexInfo;
struct ScaledIndexInfo {
    /*
     * The data for the "super-class"
     */
    CVMJITIdentityDecoration  dec;

    CVMBool hasConstIndex;
    CVMInt32 index;
    CVMRMResource* indexReg;
    CVMRMResource* arrayBaseReg;
    int shiftAmount;
    const ArrayElemInfo* elemInfo;

    int baseRegID;
    CVMRMResource *slotAddrReg;
    int slotAddrOffset;
    
#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    CVMBool isIDENTITYOutofBoundsCheck;
#endif
};

const ArrayElemInfo typeidToArrayElemInfo[] = {
    /* CVM_TYPEID_NONE */      
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_ENDFUNC */   
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_VOID */      
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_INT */       
    {2, 1, CVM_FALSE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_SHORT */     
    {1, 1, CVM_FALSE, CVMCPU_LDR16_OPCODE, CVMCPU_STR16_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_CHAR */      
    {1, 1, CVM_FALSE, CVMCPU_LDR16U_OPCODE,CVMCPU_STR16_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_LONG */      
    {3, 2, CVM_FALSE, CVMCPU_LDR64_OPCODE, CVMCPU_STR64_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_BYTE */      
    {0, 1, CVM_FALSE, CVMCPU_LDR8_OPCODE, CVMCPU_STR8_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_FLOAT */     
    {2, 1, CVM_FALSE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST32},
    /* CVM_TYPEID_DOUBLE */    
    {3, 2, CVM_FALSE, CVMCPU_LDR64_OPCODE, CVMCPU_STR64_OPCODE CVM_FLDST64},
    /* CVM_TYPEID_BOOLEAN  This will look like a byte array */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},
    /* CVM_TYPEID_OBJ */       
    {2, 1, CVM_TRUE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST_NONE},
    /* CVMJIT_TYPEID_32BITS */      
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_64BITS */   
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_ADDRESS */      
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_UBYTE */       
    {0, 1, CVM_FALSE, CVMCPU_LDR8U_OPCODE, CVMCPU_STR8_OPCODE CVM_FLDST_NONE},
};


/* Purpose: Instantiates a ScaledIndexInfo data structure. */
static ScaledIndexInfo*
newScaledIndexInfo(CVMJITCompilationContext *con, CVMRMResource *indexReg,
                   CVMInt32 index, CVMBool isConstIndex)
{
    ScaledIndexInfo* sinfo = CVMJITmemNew(con, JIT_ALLOC_CGEN_OTHER,
                                          sizeof(ScaledIndexInfo));
    CVMJITidentityInitDecoration(con, &sinfo->dec,
				 CVMJIT_IDENTITY_DECORATION_SCALEDINDEX);
    sinfo->hasConstIndex = isConstIndex;
    sinfo->index = index;
    sinfo->indexReg = indexReg;
    if (indexReg != NULL) {
	/* incorporated into sinfo. Increment ref count */
	CVMRMincRefCount(con, indexReg); 
    }
    sinfo->shiftAmount = -1;
#ifdef CVM_DEBUG
    sinfo->baseRegID = CVMCPU_INVALID_REG;
    sinfo->slotAddrReg = NULL;
    sinfo->slotAddrOffset = 0;
#endif

#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    sinfo->isIDENTITYOutofBoundsCheck = CVM_FALSE;
#endif
    
    return sinfo;
}

/* Purpose: Pushes a register index type ScaledIndexInfo on to the codegen
            semantic stack. */
static void
pushScaledIndexInfoReg(CVMJITCompilationContext *con, CVMRMResource *indexReg)
{
    ScaledIndexInfo* sinfo = newScaledIndexInfo(con, indexReg, 0, CVM_FALSE);
    pushIConst32(con, (CVMInt32)sinfo);
}

/* Purpose: Pushes an immediate index type ScaledIndexInfo on to the codegen
            semantic stack. */
static void
pushScaledIndexInfoImmediate(CVMJITCompilationContext *con, CVMInt32 index)
{
    ScaledIndexInfo* sinfo = newScaledIndexInfo(con, NULL, index, CVM_TRUE);
    pushIConst32(con, (CVMInt32)sinfo);
}

/* Purpose: Pops a ScaledIndexInfo off of the codegen semantic stack. */
CVM_INLINE static ScaledIndexInfo*
popScaledIndexInfo(CVMJITCompilationContext *con)
{
    return (ScaledIndexInfo*)popIConst32(con);
}

/* Purpose: Pops a ScaledIndexInfo off of the codegen semantic stack. */
CVM_INLINE static void
pushScaledIndexInfo(CVMJITCompilationContext *con, ScaledIndexInfo* sinfo)
{
    pushIConst32(con, (CVMInt32)sinfo);
}

/* Purpose: Does setup for doing a scaled index operation.  This may entail:
            1. Allocating and pinning any scratch resources needed.
            2. Emitting some setup code to produce intermediate values to be
               used in a memory reference later to do the actual Java array
               element access. 

   computeSlotAddr tells setupScaledIndex() that the array slot address
   has to be computed unconditionally as a register-immediate pair.

   isRef tells setupScaledIndex() that a card table routine is going to
   be called on the array access, so the slot address has to be computed
   precisely.

*/
static void
setupScaledIndex(CVMJITCompilationContext *con, int opcode,
                 CVMRMResource *array, ScaledIndexInfo *sinfo,
		 CVMBool isRefStore)
{
    int arrayRegID = CVMRMgetRegisterNumber(array);

    sinfo->slotAddrReg = NULL;

    /* Need to compute effective address into a register: */
    if (sinfo->hasConstIndex) {
        CVMUint32 offset;

        /* Fold all constants into a single offset: */
        offset = (sinfo->index << sinfo->shiftAmount) + ARRAY_DATA_OFFSET;

        if (isRefStore) {
            CVMRMResource *scratch;
            int scratchRegID;
            /* If we get here, we're dealing with a obj ref store.  Hence,
               we will need to fully evaluate the effective address into a
               register and use a register offset type of memspec.  The fully
               evaluated effective address will be used later by GC card
               table marking code.
            */
            scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				       CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
            sinfo->slotAddrReg = scratch;
	    
            scratchRegID = CVMRMgetRegisterNumber(scratch);
            CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE, scratchRegID,
					arrayRegID, offset, CVMJIT_NOSETCC);
            sinfo->baseRegID = scratchRegID;
            sinfo->slotAddrOffset = 0;
        } else {
            if (CVMCPUmemspecIsEncodableAsOpcodeSpecificImmediate(opcode,
                                                                  offset)) {
                /* If we get here, then we can fold the indexing and array
                   header offset all into a single immediate offset: */
                sinfo->slotAddrOffset = offset;
            } else {
                /* If we get here, then the indexing and array header offset
                   is too big to fit into a single immediate offset.  Instead,
                   we bind the constant index to a register and go emit the
                   code for handling a non-const index.

                   NOTE: Chances are that the index was already loaded
                   previously to do a bounds check.  Hence, we use
                   CVMRMbindResourceForConstant32() to take advantage of this
                   instead of attempting to load it explicitly.
                */

                sinfo->hasConstIndex = CVM_FALSE;
                sinfo->indexReg =
                    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con),
						   sinfo->index);
                goto doNonConstIndex;
            }
	    sinfo->baseRegID = arrayRegID;
	    sinfo->slotAddrReg = array;
            /* incorporated into sinfo. Increment ref count */
	    CVMRMincRefCount(con, array);
        }

    } else {
	/* The index is not a constant but is in a register */
        CVMRMResource *scratch;
        int indexRegID;
        int scratchRegID;
doNonConstIndex:

        /* If we get here, then the index is not constant.  We will first
           compute an arrayIndex:
              subscript = base + (index << shiftAmount)

           Then we use an immediate offset type memspec to add the array
           header offset to the subscript to compute the actual effective
           address we want.
        */
        scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				   CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
        sinfo->slotAddrReg = scratch;
	
        scratchRegID = CVMRMgetRegisterNumber(scratch);
        CVMRMpinResource(CVMRM_INT_REGS(con), sinfo->indexReg,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        indexRegID = CVMRMgetRegisterNumber(sinfo->indexReg);

        /* scratchReg = baseReg + (indexReg << #shiftAmount): */
        CVMCPUemitComputeAddressOfArrayElement(con, CVMCPU_ADD_OPCODE,
            scratchRegID, arrayRegID, indexRegID, CVMCPU_SLL_OPCODE,
            sinfo->shiftAmount);

        if (isRefStore) {
            /* If we get here, we're dealing with a obj ref store.  Hence,
               we will need to fully evaluate the effective address into a
               register and use a register offset type of memspec.  The fully
               evaluated effective address will be used later by GC card
               table marking code.
            */
            CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
                scratchRegID, scratchRegID, ARRAY_DATA_OFFSET, CVMJIT_NOSETCC);
            sinfo->slotAddrOffset = 0;
        } else {
            CVMassert(CVMCPUmemspecIsEncodableAsOpcodeSpecificImmediate(
                        opcode, ARRAY_DATA_OFFSET));
	    sinfo->slotAddrOffset = ARRAY_DATA_OFFSET;
        }
        sinfo->baseRegID = scratchRegID;
    }
}

/* Purpose: Gets the baseRegID that was setup by setupScaledIndex(). */
#define getScaledIndexBaseRegID(sinfo)      ((sinfo)->baseRegID)

/* Purpose: unpin resource components of a ScaledIndexInfo */
static void
unpinScaledIndex(CVMJITCompilationContext *con, ScaledIndexInfo *sinfo)
{
    /* We are done with this ScaledIndexInfo. Get rid of it and
       its associated componenets */
    if (!sinfo->hasConstIndex) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->indexReg);
    }
    if (sinfo->slotAddrReg != NULL) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg);
    }
    if (sinfo->arrayBaseReg != NULL) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg);
    }
}

/* Purpose: unpin resource components of a ScaledIndexInfo */
static void
persistAndUnpinScaledIndex(CVMJITCompilationContext *con, 
			   ScaledIndexInfo *sinfo)
{
    /* We are done with this ScaledIndexInfo. Get rid of it and
       its associated componenets */
    if (!sinfo->hasConstIndex) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->indexReg,
				    NULL);
    }
    if (sinfo->slotAddrReg != NULL) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg,
				    NULL);
    }
    if (sinfo->arrayBaseReg != NULL) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg,
				    NULL);
    }
}

/* Purpose: Releases any scratch that may have been pinned by
            setupScaledIndex(). */
static void
relinquishScaledIndex(CVMJITCompilationContext *con, ScaledIndexInfo *sinfo)
{
    /* We should never decrement below 0. */
    CVMassert(CVMJITidentityGetDecorationRefCount(con, &sinfo->dec) > 0); 
    CVMJITidentityDecrementDecorationRefCount(con, &sinfo->dec);
    if (CVMJITidentityGetDecorationRefCount(con, &sinfo->dec) <= 0) {
	/* We are done with this ScaledIndexInfo. Get rid of it and
	   its associated componenets */
	if (!sinfo->hasConstIndex) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->indexReg);
	}
	if (sinfo->slotAddrReg != NULL) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg);
	}
	if (sinfo->arrayBaseReg != NULL) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg);
	}
    } else {
	/* Merely unpin everything */
	unpinScaledIndex(con, sinfo);
    }
}

/* Purpose: Emits code to do a load of a Java array element. */
static void
indexedLoad(
    CVMJITCompilationContext* con,
    CVMJITRMContext* rc,
    CVMJITIRNodePtr fetchNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource *dest;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arrayObj = popResource(con);
    CVMUint16 typeId;
    CVMJITIRNode* indexNode;
    const ArrayElemInfo* elemInfo;
    int size;
    int shiftAmount;
    int opcode;
#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    CVMJITIRNode* outOfBoundsCheckNode = NULL;
#endif

    CVMassert(CVMJITirnodeIsFetch(fetchNode));
    indexNode = CVMJITirnodeGetLeftSubtree(fetchNode);
    
    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));
    
    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];
    
    size = elemInfo->size;
    shiftAmount = elemInfo->shiftAmount;
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->loadOpcode
				    : elemInfo->floatLoadOpcode;
#else
    opcode = elemInfo->loadOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment((
        "Do load(arrayObj, index) (elem type=%d,%c):", typeId,
	typeId == CVMJIT_TYPEID_UBYTE ?
	   'b' : CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));

    dest = CVMRMgetResource(rc, target, avoid, size);
    CVMRMpinResource(CVMRM_INT_REGS(con), arrayObj,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    setupScaledIndex(con, opcode, arrayObj, sinfo, CVM_FALSE);
    CVMJITcsSetGetArrayInstruction(con);

/* IAI - 12 */
#if defined(IAI_CS_EXCEPTION_ENHANCEMENT) && !defined(IAI_CS_EXCEPTION_ENHANCEMENT2) 
    CVMJITcsSetExceptionInstruction(con);
#endif
/* IAI - 12 */

#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    indexNode = CVMJITirnodeValueOf(indexNode);
    if(CVMJITirnodeIsBinaryNode(indexNode)) {
        outOfBoundsCheckNode = CVMJITirnodeValueOf(
              CVMJITirnodeGetRightSubtree(indexNode));
    }
    
    /*
     * The following code deals with the case which the bounds-check sharing 
     * between different node.
     * From the code scheduling point of view, the more formal way is to emit all the
     * array fetch/store instruction in conditional style.
     * There are two possible solution,
     * 1) Share the resource directly
     * 2) Setup the dependency rule between array fetch/store and bounds check instructions
     */
    if(outOfBoundsCheckNode && CVMJITirnodeIsBoundsCheckNode(outOfBoundsCheckNode)
               && !sinfo->isIDENTITYOutofBoundsCheck) {
         CVMJITprintCodegenComment(("Schedule conditinal load"));
         CVMCPUemitMemoryReferenceImmediateConditional(con, opcode, 
         CVMRMgetRegisterNumber(dest),
         getScaledIndexBaseRegID(sinfo), 
         sinfo->slotAddrOffset, CVMCPU_COND_HI);
    } else {
         CVMCPUemitMemoryReferenceImmediate(con, opcode, 
         CVMRMgetRegisterNumber(dest),
         getScaledIndexBaseRegID(sinfo), 
         sinfo->slotAddrOffset);
    }
#else
    CVMCPUemitMemoryReferenceImmediate(con, opcode, 
                                       CVMRMgetRegisterNumber(dest),
                                       getScaledIndexBaseRegID(sinfo), 
                                       sinfo->slotAddrOffset);
#endif

    
    /* We should not have set arrayObj in this sinfo instance. arrayObj
       is relinquished separately below */
    CVMassert(sinfo->arrayBaseReg == NULL);
    
    relinquishScaledIndex(con, sinfo);

    CVMRMoccupyAndUnpinResource(rc, dest, fetchNode);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrayObj);
    pushResource(con, dest);
}

/* Purpose: Emits code to do a load of a Java array element. */
static void
indexedAddr(
    CVMJITCompilationContext* con,
    CVMJITIRNodePtr indexNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arrayObj = popResource(con);
    CVMUint16 typeId;
    const ArrayElemInfo* elemInfo;
    int shiftAmount;
    int opcode;
    CVMBool canBeUsedInStore;

    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));
    
    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];
    
    shiftAmount = elemInfo->shiftAmount;
    opcode = elemInfo->loadOpcode;

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment((
        "Compute slot &arr[index] (elem type=%c):",
	CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));
    CVMRMpinResource(CVMRM_INT_REGS(con), arrayObj,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    canBeUsedInStore = CVMJITirnodeBinaryNodeIs(indexNode, WRITE);
    
    /*
     * When we are setting up the scaled index, we don't know if it's
     * for a load or a store. So we conservatively set isRef, even though
     * we might not need the extra computation for a precise slot address
     * in case of a reference read (and not a write)
     */
    setupScaledIndex(con, opcode, arrayObj, sinfo,
		     canBeUsedInStore && elemInfo->isRef /* isRefStore */);

    /*
     * Remember some more in the sinfo
     */
    sinfo->arrayBaseReg = arrayObj;
    /* incorporated into sinfo. Increment ref count */
    CVMRMincRefCount(con, arrayObj);
    
    sinfo->elemInfo = elemInfo;

    /*
     * Make sure the sinfo resource components persist across rules
     */
    persistAndUnpinScaledIndex(con, sinfo);
    
    /*
     * And propagate the scaledIndex part further. sinfo->slotAddrReg
     * contains the slot address.
     */
    pushScaledIndexInfo(con, sinfo);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrayObj);
}

#define ARRAY_LOAD_SYNTHESIS(con, p)   L_BINARY_SYNTHESIS((con), (p))
#define ARRAY_LOAD_INHERITANCE(con, p) L_BINARY_INHERITANCE((con), (p))

/* Purpose: reads a slot from an array */
static void 
fetchArraySlot(CVMJITCompilationContext *con,
	       CVMJITRMContext* rc,
	       CVMJITIRNodePtr fetchNode,
	       CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *dest;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arraySlot = sinfo->slotAddrReg;
    const ArrayElemInfo* elemInfo = sinfo->elemInfo;
    int opcode;
#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    CVMJITIRNode* indexNode;
    CVMJITIRNode* outOfBoundsCheckNode = NULL;
#endif

    CVMassert(CVMJITirnodeIsFetch(fetchNode));
    
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->loadOpcode
				    : elemInfo->floatLoadOpcode;
#else
    opcode = elemInfo->loadOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    dest = CVMRMgetResource(rc, target, avoid, elemInfo->size);
    CVMRMpinResource(CVMRM_INT_REGS(con), arraySlot,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    CVMJITcsSetGetArrayInstruction(con);

/* IAI - 12 */
#if defined(IAI_CS_EXCEPTION_ENHANCEMENT) && !defined(IAI_CS_EXCEPTION_ENHANCEMENT2) 
    CVMJITcsSetExceptionInstruction(con);
#endif
/* IAI - 12 */

#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    indexNode = CVMJITirnodeValueOf(CVMJITirnodeGetLeftSubtree(
                    CVMJITirnodeValueOf(fetchNode)));

    if(CVMJITirnodeIsBinaryNode(indexNode)) {
        outOfBoundsCheckNode = CVMJITirnodeValueOf(
                    CVMJITirnodeGetRightSubtree(indexNode));
    }
    
    if(outOfBoundsCheckNode && outOfBoundsCheckNode->tag == BOUNDS_CHECK 
               && !sinfo->isIDENTITYOutofBoundsCheck) {
        CVMJITprintCodegenComment(("Schedule conditinal load"));
        CVMCPUemitMemoryReferenceImmediateConditional(con, opcode,
  				       CVMRMgetRegisterNumber(dest), 
  				       CVMRMgetRegisterNumber(arraySlot),
  				       sinfo->slotAddrOffset, CVMCPU_COND_HI);
    } else {
        CVMCPUemitMemoryReferenceImmediate(con, opcode,
   				       CVMRMgetRegisterNumber(dest), 
   				       CVMRMgetRegisterNumber(arraySlot),
   				       sinfo->slotAddrOffset);
    }
#else
    CVMCPUemitMemoryReferenceImmediate(con, opcode,
				       CVMRMgetRegisterNumber(dest), 
				       CVMRMgetRegisterNumber(arraySlot),
				       sinfo->slotAddrOffset);
#endif				       

    CVMRMoccupyAndUnpinResource(rc, dest, fetchNode);
    relinquishScaledIndex(con, sinfo);
    pushResource(con, dest);
}

%}

// Purpose: remember variable arraySubscript
arraySubscript: reg32 : 0 : : : :{
	CVMRMResource* operand = popResource(con);
        pushScaledIndexInfoReg(con, operand);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

// Purpose: remember constant arraySubscript
arraySubscript: iconst32Index : 0 : : : :{
        CVMInt32 index = popIConst32(con);
        pushScaledIndexInfoImmediate(con, index);
    };

// Purpose: value32 = FETCH(INDEX(arrayObject, arraySubscript))
arrayIndex: INDEX reg32 arraySubscript : 20 : : : : {
        indexedAddr(con, $$, GET_REGISTER_GOALS);
    };

// Purpose: value64 = FETCH64(INDEX(arrayObject, arraySubscript))
reg64: FETCH64 INDEX reg32 arraySubscript : 20 :
    ARRAY_LOAD_SYNTHESIS(con, $$); : ARRAY_LOAD_INHERITANCE(con, $$); : : {
        indexedLoad(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = FETCH32(INDEX(arrayObject, arraySubscript))
reg32: FETCH32 INDEX reg32 arraySubscript : 20 :
    ARRAY_LOAD_SYNTHESIS(con, $$); : ARRAY_LOAD_INHERITANCE(con, $$); : : {
        indexedLoad(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value64 = FETCH64(INDEX(arrayObject, arraySubscript))
reg64: FETCH64 arrayIndex : 20 : :  : : {
        CVMJITprintCodegenComment(("Do *slotAddr64:"));
        fetchArraySlot(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = FETCH32(INDEX(arrayObject, arraySubscript))
reg32: FETCH32 arrayIndex : 20 : :  : : {
        CVMJITprintCodegenComment(("Do *slotAddr32:"));
        fetchArraySlot(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

%{

/* Purpose: Fetches an instance field from an object. */
static void fetchField(CVMJITCompilationContext *con,
		       CVMJITRMContext* rc,
                       CVMJITIRNodePtr thisNode,
                       CVMRMregset target, CVMRMregset avoid,
                       CVMInt32 opcode, int fieldSize,
                       CVMBool isVolatile)
{
    CVMCPUMemSpec *fieldOffset = popMemSpec(con);
    CVMRMResource *objPtr = popResource(con);
    CVMRMResource *dest;
    dest = CVMRMgetResource(rc, target, avoid, fieldSize);
    CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMJITaddCodegenComment((con, "= getfield(obj, fieldIdx);"));
    CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMJITcsSetGetFieldInstruction(con);
    CVMJITcsSetExceptionInstruction(con);

    CVMCPUemitMemoryReference(con, opcode,
        CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(objPtr),
        CVMCPUmemspecGetToken(con, fieldOffset));
    if (isVolatile) {
        CVMCPUemitMemBarAcquire(con);
    }

    CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

#define GETFIELD_SYNTHESIS(con, p)   L_BINARY_SYNTHESIS((con), (p))
#define GETFIELD_INHERITANCE(con, p) L_BINARY_INHERITANCE((con), (p))

%}

// Purpose: value = FETCH32(FIELDREFOBJ(obj,fieldOffset))
reg32: FETCH32 FIELDREFOBJ reg32 memSpec : 10 :
    GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
        CVMBool isVolatile;
        CVMJITprintCodegenComment(("Do getfield:"));
        CVMJITaddCodegenComment((con, "valueObj"));
        isVolatile =
            ((CVMJITirnodeGetBinaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITBINOP_VOLATILE_FIELD) != 0);
        fetchField(con, CVMRM_INT_REGS(con), $$,
		   GET_REGISTER_GOALS, CVMCPU_LDR32_OPCODE, 1,
                   isVolatile);
    };

// Purpose: value = FETCH32(FIELDREF32(obj,fieldOffset))
reg32: FETCH32 FIELDREF32 reg32 memSpec : 10 :
    GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
        CVMBool isVolatile;
        CVMJITprintCodegenComment(("Do getfield:"));
        CVMJITaddCodegenComment((con, "value{I|F}"));
        isVolatile =
            ((CVMJITirnodeGetBinaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITBINOP_VOLATILE_FIELD) != 0);
        fetchField(con, CVMRM_INT_REGS(con), $$,
		   GET_REGISTER_GOALS, CVMCPU_LDR32_OPCODE, 1,
                   isVolatile);
    };

// Purpose: value = FETCH64(FIELDREF64(obj,fieldOffset))
reg64: FETCH64 FIELDREF64 reg32 memSpec : 10 :
    GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do getfield:"));
        CVMJITaddCodegenComment((con, "value{L|D}"));
        fetchField(con, CVMRM_INT_REGS(con), $$,
		   GET_REGISTER_GOALS, CVMCPU_LDR64_OPCODE, 2,
                   CVM_FALSE);
    };

// Purpose: value = FETCH64(FIELDREF64VOL(obj,fieldOffset))
reg64: FETCH64 FIELDREF64VOL reg32 reg32 : 90 :
    SET_AVOID_C_CALL($$); : SET_TARGET2($$, ARG1, ARG2); : : {
	CVMRMResource *fieldOffset;

	/* Note: we need to do the NULL check for this object reference because
	   the CCM helper can't do it: */
	fieldOffset = popResource(con); /* Leave the objRes on the stack. */
	doNullCheck(con, NULL, ARG1, ARG2);
	pushResource(con, fieldOffset); /* Put back the fieldOffset. */

	/* Now do the get field: */
        CVMJITprintCodegenComment(("Do volatile getfield:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeGetfield64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeGetfield64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeGetfield64Volatile);
        binaryHelper(con, (void*)CVMCCMruntimeGetfield64Volatile, $$,
		     CVM_FALSE, 2, 2);
    };

%{

#if (CVM_GCCHOICE == CVM_GC_GENERATIONAL)
/*
 * Emit barrier for write of srcReg into objReg+dataOffset=destAddr
 */
static void
emitMarkCardTable(CVMJITCompilationContext *con, 
		  int objAddrReg, int destAddrReg)
{
    /* Need three registers here. One to hold zero, the other
     * to hold the card table virtual base, and a third
     * to hold the slot address being written into:
     */

    CVMCodegenComment *comment;
    CVMRMResource* cardtableReg;
    CVMRMResource* markReg;
    int            markRegID;
#ifdef CVM_SEGMENTED_HEAP
    /* t0 and t1 will eventually become cardTable and zero registers */
    CVMRMResource* t0;
    CVMRMResource* t1;
    int fixupPC; /* To patch the conditional barrier branch */
    CVMUint32 lowerBound  = (CVMUint32)CVMglobals.youngGenLowerBound;
    CVMUint32 higherBound = (CVMUint32)CVMglobals.youngGenUpperBound;
#endif
#ifdef CVM_JAVASE_CLASS_HAS_REF_FIELD
    int fixupPC1;
#endif

    CVMJITpopCodegenComment(con, comment);

    /* In SE 1.5 and later version, the java.lang.Class has non-static
     * reference fields. If the Class instance is ROMized, we should 
     * not mark the card table.
     */
#ifdef CVM_JAVASE_CLASS_HAS_REF_FIELD
    {
        CVMRMResource *tmpRes = CVMRMgetResource(CVMRM_INT_REGS(con),
                          CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
        int tmpReg = CVMRMgetRegisterNumber(tmpRes);
        /* load object hdr.clas */
        CVMJITaddCodegenComment((con, "get obj.hdr.clas"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
                                       tmpReg, objAddrReg,
                                       CVMoffsetof(CVMObjectHeader, clas));
        /* check if the object is in ROM */
        CVMJITaddCodegenComment((con, "check if object is in ROM"));
        CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
                                    tmpReg, tmpReg,
                                    CVM_OBJECT_IN_ROM_MASK,
                                    CVMJIT_NOSETCC);
        CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_NE,
                                  tmpReg, 0);
        /* skip mark cardtable if the object is in ROM */
        CVMJITaddCodegenComment((con, "skip mark cardtable if it's in ROM"));
        CVMCPUemitBranch(con, 0, CVMCPU_COND_NE);
#ifdef CVMCPU_HAS_DELAY_SLOT
        fixupPC1 = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
        fixupPC1 = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), tmpRes);
    }
#endif

#ifdef CVM_SEGMENTED_HEAP
    CVMJITprintCodegenComment(("Check if barrier required:"));

    t0 = CVMRMgetResource(CVMRM_INT_REGS(con), CVMRM_ANY_SET, 
			  CVMRM_EMPTY_SET, 1);
    /* t0 = #lowerBound */
    CVMJITaddCodegenComment((con, "CVMglobals.youngGenLowerBound"));
    CVMJITsetSymbolName((con, "CVMglobals.youngGenLowerBound"));
    CVMCPUemitLoadConstant(con, CVMRMgetRegisterNumber(t0), lowerBound);
    t1 = CVMRMgetResource(CVMRM_INT_REGS(con), CVMRM_ANY_SET, 
			  CVMRM_EMPTY_SET, 1);
    /* t1 = objRef - t0 */
    CVMCPUemitBinaryALURegister(con, CVMCPU_SUB_OPCODE, 
				CVMRMgetRegisterNumber(t1),
				objAddrReg, 
				CVMRMgetRegisterNumber(t0),
				CVMJIT_NOSETCC);
    /* Compare(t1, #higherBound - #lowerBound) */
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LO,
			      CVMRMgetRegisterNumber(t1),
			      higherBound - lowerBound);

    CVMJITaddCodegenComment((con, "br skipBarrier if less than (hi-lo)"));
    /* Branch to skipBarrier if unsigned less than */
    CVMCPUemitBranch(con, 0, CVMCPU_COND_LO);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC = CVMJITcbufGetLogicalInstructionPC(con);
#endif /* IAI_CODE_SCHEDULER_SCORE_BOARD */
    
    CVMJITcsSetEmitInPlace(con);
    con->inConditionalCode = CVM_TRUE;
    CVMJITprintCodegenComment(("Will do barrier:"));
    CVMJITprintCodegenComment(("Compute segment addr = BIC(obj, 0xffff)"));

    /* t1 = BIC(objAddr, 0xffff) to compute the segment address */
    /*
     * WARNING: don't let CVMCPUemitBinaryALUConstant() load a
     * large constant into a register. This will change the
     * regman state, which is a no-no in conditionally executed
     * code. Instead, manually load the large constant here
     * using CVMCPUemitLoadConstant(), which won't affect the
     * regman state.
     */
    CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE, 
				CVMRMgetRegisterNumber(t1),
				objAddrReg,
				SEGMENT_ALIGNMENT - 1, CVMJIT_NOSETCC);
	
    CVMJITaddCodegenComment((con, "seg->cardTableVirtualBase"));
    /* And from the segment address, the barrier address: */
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
       CVMRMgetRegisterNumber(t1),
       CVMRMgetRegisterNumber(t1),
       GC_SEGMENT_CTV_OFFSET);
    cardtableReg = t1;

    /* Now we can re-use t0 to store the CARD_DIRTY_BYTE mark value: */
    markReg = t0;
    markRegID = CVMRMgetRegisterNumber(markReg);
    CVMCPUemitLoadConstant(con, markRegID, (CVMInt32)CARD_DIRTY_BYTE);
    CVMJITcsClearEmitInPlace(con);
#else /* !CVM_SEGMENTED_HEAP case below */

    /* Easy to compute for the non-segmented case */
#if defined(CVM_AOT)
    if (CVMglobals.jit.isPrecompiling) {
        /*
         * We can't use the cardTableVirtualBase
         * as a constant since the cardtable is dynamically
         * allocated. Emit code to access the cardTableVirtualBase
         * indirectly.
         */
        cardtableReg = CVMRMgetResource(CVMRM_INT_REGS(con),
                                    CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
        CVMCPUemitLoadConstant(con, CVMRMgetRegisterNumber(cardtableReg),
            (CVMInt32)(&CVMglobals.gc.cardTableVirtualBase));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(cardtableReg),
            CVMRMgetRegisterNumber(cardtableReg), 0);
    } else
#endif
   {
       CVMJITsetSymbolName((con, "cardTableVirtualBase"));
        cardtableReg =
            CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
	        CVMRM_ANY_SET, CVMRM_EMPTY_SET,
                (CVMInt32)CVMglobals.gc.cardTableVirtualBase);
        /* Make sure we get these constants right: */
        CVMassert(CVMRMgetRegisterNumber(cardtableReg) != -1);
    }

    CVMJITsetSymbolName((con, "CARD_DIRTY_BYTE"));
    markReg = CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
	          CVMRM_ANY_SET, CVMRM_EMPTY_SET, (CVMInt32)CARD_DIRTY_BYTE);
    markRegID = CVMRMgetRegisterNumber(markReg);

#endif /* !CVM_SEGMENTED_HEAP */

    CVMassert(markRegID != -1);

    /* Now we are ready to write into card table, as well as
       perform the actual write: */

    /* Do card table write: */
    CVMJITaddCodegenComment((con, "mark card table"));
    CVMCPUemitArrayElementReference(con, CVMCPU_STR8_OPCODE,
        markRegID, CVMRMgetRegisterNumber(cardtableReg),
        destAddrReg, CVMCPU_SRL_OPCODE, CVM_GENGC_CARD_SHIFT);

#ifdef CVM_SEGMENTED_HEAP
    CVMtraceJITCodegen(("\t\tskipBarrier:\n"));
    CVMJITfixupAddress(con, fixupPC, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    con->inConditionalCode = CVM_FALSE;
#endif

#ifdef CVM_JAVASE_CLASS_HAS_REF_FIELD
    CVMJITfixupAddress(con, fixupPC1,  CVMJITcbufGetLogicalPC(con),
                       CVMJIT_COND_BRANCH_ADDRESS_MODE);
#endif

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), markReg);
    
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), cardtableReg);

    CVMJITpushCodegenComment(con, comment);
}

/*
 * Given an evaluated FIELDREF's components on the stack, return the
 * object address portion
 */
static CVMRMResource*
extractObjectAddrFromFieldRef(CVMJITCompilationContext* con)
{
    CVMCPUALURhs* rhs = popALURhs(con);
    CVMRMResource* lhs = popResource(con);
    pushResource(con, lhs);
    pushALURhs(con, rhs);
    return lhs;
}
 
#else
#define emitMarkCardTable(con, destAddrReg)

/* Currently, the JIT code assumes that our GC choice is generational,
   since we have to emit a card table write barrier. So enforce this
   using an cpp error if the GC choice is not as expected. */
#error The dynamic compiler only works with generational GC
#endif

#define PUTFIELD_SYNTHESIS(con, thisNode) \
    L_BINARY_R_UNARY_SYNTHESIS((con), (thisNode))
#define PUTFIELD_INHERITANCE(con, thisNode) \
    L_BINARY_R_UNARY_INHERITANCE((con), (thisNode))

%}

// Purpose: ASSIGN(FIELDREFOBJ(obj,fieldOffset), valueRef)
root: ASSIGN FIELDREFOBJ reg32 aluRhs reg32 : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMRMResource *src = popResource(con);
        CVMRMResource *fieldAddr;
	CVMRMResource *objAddr;
        int srcRegID, fieldAddrRegID, objAddrRegID;

        CVMJITprintCodegenComment(("Do putfield:"));

        /* Compute the address of the field: */
        CVMJITaddCodegenComment((con, "fieldAddr = obj + fieldOffset;"));
	objAddr = extractObjectAddrFromFieldRef(con);
	CVMRMincRefCount(con, objAddr); /* One more use for the barrier */
        wordBinaryOp(con, CVMCPU_ADD_OPCODE, CVMJITirnodeGetLeftSubtree($$),
                     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

        /* Emit write along with barrier for reference writes: */
        fieldAddr = popResource(con);
        CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), fieldAddr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), objAddr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        srcRegID = CVMRMgetRegisterNumber(src);
        fieldAddrRegID = CVMRMgetRegisterNumber(fieldAddr);
        objAddrRegID = CVMRMgetRegisterNumber(objAddr);

        CVMJITaddCodegenComment((con, "putfield(fieldAddr, valueObj);"));
        CVMJITcsSetPutFieldInstruction(con);
        CVMJITcsSetExceptionInstruction(con);
        if ((CVMJITirnodeGetBinaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITBINOP_VOLATILE_FIELD) != 0) {
            CVMCPUemitMemBarRelease(con);
        }
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE, srcRegID,
                                           fieldAddrRegID, 0);
        emitMarkCardTable(con, objAddrRegID, fieldAddrRegID);
        if ((CVMJITirnodeGetBinaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITBINOP_VOLATILE_FIELD) != 0) {
            CVMCPUemitMemBar(con);
        }

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), objAddr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), fieldAddr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

%{

/* Purpose: Sets a value to an instance field of an object. */
static void setField(CVMJITCompilationContext *con,
		     CVMJITRMContext* rc,
                     CVMInt32 opcode,
                     CVMBool isVolatile)
{
    CVMRMResource *src = popResource(con);
    CVMCPUMemSpec *fieldOffset = popMemSpec(con);
    CVMRMResource *objPtr = popResource(con);

    if (isVolatile) {
        CVMCPUemitMemBarRelease(con);
    }

    CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(rc, src,    rc->anySet, CVMRM_EMPTY_SET);

    CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMJITcsSetPutFieldInstruction(con);
    CVMJITcsSetExceptionInstruction(con);

    CVMCPUemitMemoryReference(con, opcode,
        CVMRMgetRegisterNumber(src), CVMRMgetRegisterNumber(objPtr),
        CVMCPUmemspecGetToken(con, fieldOffset));
    if (isVolatile) {
        CVMCPUemitMemBar(con);
    }

    CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
    CVMRMrelinquishResource(rc, src);
}

%}

// Purpose: ASSIGN(FIELDREF32(obj,fieldOffset), value32)
root: ASSIGN FIELDREF32 reg32 memSpec reg32 : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMBool isVolatile;
        CVMJITprintCodegenComment(("Do putfield:"));
        CVMJITaddCodegenComment((con,
            "putfield(obj, fieldOffset, value{I|F});"));
        isVolatile =
            ((CVMJITirnodeGetBinaryNodeFlag(CVMJITirnodeGetLeftSubtree($$)) &
             CVMJITBINOP_VOLATILE_FIELD) != 0);
        setField(con, CVMRM_INT_REGS(con), CVMCPU_STR32_OPCODE,
                 isVolatile);
    };

// Purpose: ASSIGN(FIELDREF64(obj,fieldOffset), value64)
root: ASSIGN FIELDREF64 reg32 memSpec reg64 : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do putfield:"));
        CVMJITaddCodegenComment((con,
            "putfield(obj, fieldOffset, value{L|D});"));
        setField(con, CVMRM_INT_REGS(con), CVMCPU_STR64_OPCODE,
                 CVM_FALSE);
    };

// Purpose: ASSIGN(FIELDREF64VOL(obj,fieldOffset), value64)
root: ASSIGN FIELDREF64VOL reg32 reg32 reg64 : 90 :
    SET_AVOID_C_CALL($$); : PUTFIELD_INHERITANCE(con, $$);
    SET_TARGET3_WO_INHERITANCE($$, ARG3, ARG4, ARG1); : : {

	CVMRMResource* value = popResource(con);
	CVMRMResource* fieldOffset = popResource(con);
	CVMRMResource* obj; /* Leave the obj on the stack for the null check. */

	/* Note: we need to do the NULL check for this object reference because
	   the CCM helper can't do it: */
	doNullCheck(con, NULL, ARG3, ARG1|ARG2|ARG4);

	/* Now do the put field: */
	obj = popResource(con); /* OK, to pop the obj now. */

	value = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					 value, CVMCPU_ARG1_REG);
	obj = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
				       obj, CVMCPU_ARG3_REG);
	fieldOffset = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					       fieldOffset, CVMCPU_ARG4_REG);

	/* Spill the outgoing registers if necessary: */
	CVMRMminorSpill(con, ARG1|ARG2|ARG3|ARG4);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
	/* Shuffle the 64-bit arg first: */
	CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, CVMCPU_ARG1_REG);
	/* Shuffle the remaining 32-bit args: */
	CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
			       CVMCPU_ARG2_REG, CVMCPU_ARG3_REG,
			       CVMJIT_NOSETCC);
	CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
			       CVMCPU_ARG3_REG, CVMCPU_ARG4_REG,
			       CVMJIT_NOSETCC);
#endif

        CVMJITprintCodegenComment(("Do volatile putfield:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimePutfield64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimePutfield64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimePutfield64Volatile);

	CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimePutfield64Volatile,
			       CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
	
	/* Release resources and publish the result: */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), value);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), fieldOffset);
    };

%{

/* Purpose: Emits code to do */
static void
emitArrayAssignabilityCheck(CVMJITCompilationContext* con,
                            CVMRMResource *aref, CVMRMResource *src)
{
    CVMRMResource* arrClass; /* Scratch register */
    CVMRMResource* rhsClass; /* Scratch register */
    int srcRegID;
    CVMCPUCondCode condCode;
#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    int fixupPC;
#endif

    /* This is an 'aastore'. Do the assignability check */
    CVMJITaddCodegenComment((con, "aastore assignability check"));
    arrClass = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
				        CVMCPU_ARG3_REG, 1);
    rhsClass = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_ARG4_REG, 1);

    /* Get the rhs of the assignment and the array reference
       into registers. Do not clobber ARG1-ARG4 */
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), src,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);
    srcRegID = CVMRMgetRegisterNumber(src);
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), aref,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);

    /*
     * We might eventually do a call. Assume we will
     */
    CVMRMminorSpill(con, ARG3|ARG4);

    /* OK, first off, check for the rhs being NULL. NULL is assignable
       to any array of references. */
    /* Compare to NULL, and skip assignability check if NULL */
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
		      srcRegID, CVMCPUALURhsTokenConstZero);

    /*
     * Call the helper after loading the cb of the array class and
     * rhs class into ARG3 and ARG4 (with the lower bits still set).
     * This is all done conditionally based on whether or not
     * the object is NULL.
     */

#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    condCode = CVMCPU_COND_AL;
    CVMJITaddCodegenComment((con, "br .skip"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);
#ifdef CVMCPU_HAS_DELAY_SLOT
    fixupPC = CVMJITcbufGetLogicalPC(con) - 2 * CVMCPU_INSTRUCTION_SIZE;
#else
    fixupPC = CVMJITcbufGetLogicalPC(con) - 1 * CVMCPU_INSTRUCTION_SIZE;
#endif

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC = CVMJITcbufGetLogicalInstructionPC(con);
#endif /* IAI_CODE_SCHEDULER_SCORE_BOARD */
    con->inConditionalCode = CVM_TRUE;
    /*
     * The instructions which will be emitted by the following code will
     * include a compare and a branch instruction on platforms that don't
     * support both a conditional load and a conditional call.  For code
     * scheduling, it is quite hard to know the dependency of
     * the compare instruction and the instructions which follow that
     * compare instruction. For example:
     * Block 7:
     * ...
     * cmp a0, 0
     * bne l1:
     * mov a1, 0
     * l1:
     * str a1, local0
     * ...
     * All of this code is in same block, and a1 is not a register which
     * flows between blocks. It is hard to understand the dependency
     * among cmp, bne and mov instructions. The solution is to turn off code
     * scheduling under such case.
     */
    CVMJITcsSetEmitInPlace(con);
#else
    condCode = CVMCPU_COND_NE;
#endif

    /* LDRNE ARG3, [robj] */
    CVMJITaddCodegenComment((con, "Get the array class"));
    CVMCPUemitMemoryReferenceImmediateConditional(con, CVMCPU_LDR32_OPCODE,
        CVMCPU_ARG3_REG, CVMRMgetRegisterNumber(aref), OBJECT_CB_OFFSET,
        condCode);

    /* LDRNE ARG4, [src] */
    CVMJITaddCodegenComment((con, "Get the rhs class"));
    CVMCPUemitMemoryReferenceImmediateConditional(con, CVMCPU_LDR32_OPCODE,
        CVMCPU_ARG4_REG, srcRegID, OBJECT_CB_OFFSET, condCode);

    /* do helper call */
    CVMJITaddCodegenComment((con, "call %s", 
                             "CVMCCMruntimeCheckArrayAssignableGlue"));
    CVMJITsetSymbolName((con,"CVMCCMruntimeCheckArrayAssignableGlue"));
    CVMJITstatsRecordInc(con,
        CVMJIT_STATS_CVMCCMruntimeCheckArrayAssignable);
    CVMCPUemitAbsoluteCallConditional(con,
        (void*)CVMCCMruntimeCheckArrayAssignableGlue,
        CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, condCode);
    CVMJITcsBeginBlock(con);
    CVMJITcaptureStackmap(con, 0);

#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    CVMtraceJITCodegen((".skip"));
    CVMJITfixupAddress(con, fixupPC, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    con->inConditionalCode = CVM_FALSE;
#endif

/* IAI-07 */
#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    CVMJITcsClearEmitInPlace(con);
#endif

    /* And we are at done */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrClass);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhsClass);

    CVMJITprintCodegenComment(("aastore assignment check done"));
    CVMRMunpinResource(CVMRM_INT_REGS(con), aref);
    CVMRMunpinResource(CVMRM_INT_REGS(con), src);
}

/* Purpose: Emits code to do a store of a Java array element. */
static void
indexedStore(
    CVMJITCompilationContext* con,
    CVMJITRMContext* rc,
    CVMJITIRNodePtr thisNode)
{
    CVMBool isRefStore;
    CVMRMResource *src  = popResource(con);
    int srcRegID, arrayRegID;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *aref = popResource(con);

    CVMUint16 typeId;
    CVMJITIRNode* indexNode;
    const ArrayElemInfo* elemInfo;
    int size;
    int shiftAmount;
    int opcode;

    indexNode = CVMJITirnodeGetLeftSubtree(thisNode);
    
    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));
    
    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];
    
    size = elemInfo->size;
    shiftAmount = elemInfo->shiftAmount;
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->storeOpcode
				    : elemInfo->floatStoreOpcode;
#else
    opcode = elemInfo->storeOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment((
        "Do store(arrayObj, index) (elem type=%c):",
	CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));

    CVMassert(elemInfo->isRef == CVMJITirnodeIsReferenceType(thisNode));
    isRefStore = CVMJITirnodeIsReferenceType(thisNode);
    if (isRefStore) {
        emitArrayAssignabilityCheck(con, aref, src);
    }

    CVMRMpinResource(rc, src,  rc->anySet, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), aref, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    srcRegID = CVMRMgetRegisterNumber(src);
    arrayRegID = CVMRMgetRegisterNumber(aref);

    {
        int baseRegID;
        setupScaledIndex(con, opcode, aref, sinfo, isRefStore);
        baseRegID = getScaledIndexBaseRegID(sinfo);
        CVMJITcsSetPutArrayInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, opcode, srcRegID, baseRegID,
					   sinfo->slotAddrOffset);
        if (isRefStore) {
	    CVMassert(sinfo->slotAddrOffset == 0);
            emitMarkCardTable(con, arrayRegID, baseRegID);
        }
	/* We should not have set arrayObj in this sinfo instance. arrayObj
	   is relinquished separately below */
	CVMassert(sinfo->arrayBaseReg == NULL);
    
        relinquishScaledIndex(con, sinfo);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), aref);
    CVMRMrelinquishResource(rc, src);
}

/* Purpose: reads a slot from an array */
static void 
storeArraySlot(CVMJITCompilationContext *con,
	       CVMJITRMContext* rc,
	       CVMJITIRNodePtr thisNode)
{
    CVMRMResource* rhs = popResource(con);
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arraySlot = sinfo->slotAddrReg;
    CVMRMResource *arrayRef;
    int opcode;
    const ArrayElemInfo* elemInfo = sinfo->elemInfo;
    
    arrayRef = sinfo->arrayBaseReg;
    if (elemInfo->isRef) {
	emitArrayAssignabilityCheck(con, arrayRef, rhs);
    }
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->storeOpcode
				    : elemInfo->floatStoreOpcode;
#else
    opcode = elemInfo->storeOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMRMpinResource(CVMRM_INT_REGS(con), arraySlot,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(rc, rhs, rc->anySet, CVMRM_EMPTY_SET);
    CVMJITcsSetPutArrayInstruction(con);
    CVMCPUemitMemoryReferenceImmediate(con, opcode,
				       CVMRMgetRegisterNumber(rhs),
				       CVMRMgetRegisterNumber(arraySlot),
				       sinfo->slotAddrOffset);
    if (elemInfo->isRef) {
	CVMassert(sinfo->slotAddrOffset == 0);
	CVMRMpinResource(CVMRM_INT_REGS(con), arrayRef,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	emitMarkCardTable(con, 
			  CVMRMgetRegisterNumber(arrayRef),
			  CVMRMgetRegisterNumber(arraySlot));
	CVMRMunpinResource(CVMRM_INT_REGS(con), arrayRef);
    }
    
    relinquishScaledIndex(con, sinfo);
    CVMRMrelinquishResource(rc, rhs);
}

#define ARRAY_STORE_SYNTHESIS(con, thisNode) \
    L_BINARY_R_UNARY_SYNTHESIS((con), (thisNode))
#define ARRAY_STORE_INHERITANCE(con, thisNode) \
    L_BINARY_R_UNARY_INHERITANCE((con), (thisNode))

%}

//
// The rhs of a 32-bit array store
//
arrayAssignmentRhs32: reg32 : 0 : : : : ;
//
// No narrowing conversions on the rhs of an array store needed, since
// they are going to be expanded straight back anyway.
//
// Make them all nops
//
arrayAssignmentRhs32: I2S reg32 : 0 : : : : ;
arrayAssignmentRhs32: I2B reg32 : 0 : : : : ;
arrayAssignmentRhs32: I2C reg32 : 0 : : : : ;

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value64)
root: ASSIGN arrayIndex reg64 : 20 : :  : : {
        CVMJITprintCodegenComment(("*slotAddr64 = reg:"));
        storeArraySlot(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value32)
root: ASSIGN arrayIndex arrayAssignmentRhs32 : 20 : : : : {
        CVMJITprintCodegenComment(("*slotAddr32 = reg:"));
        storeArraySlot(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value64)
root: ASSIGN INDEX reg32 arraySubscript reg64 : 20 :
    ARRAY_STORE_SYNTHESIS(con, $$); : ARRAY_STORE_INHERITANCE(con, $$); : : {
        indexedStore(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value32)
root: ASSIGN INDEX reg32 arraySubscript arrayAssignmentRhs32 : 20 :
    ARRAY_STORE_SYNTHESIS(con, $$); : ARRAY_STORE_INHERITANCE(con, $$); : : {
        indexedStore(con, CVMRM_INT_REGS(con), $$);
    };

//
// values, usually a result of ?: expressions, live across branches.
// These get stuffed into the spill area or passed , the first part of which
// is reserved for them, based on max number of define's per block
// in this method. If possible, these values are passed as registers
// rather than spilled.

root: DEFINE_VALUE32 reg32 : 10 : DEFINE_SYNTHESIS(con, $$) : : : {
        CVMRMResource* src = popResource(con);
	if (!CVMRMstoreDefinedValue(con, $$, src, 1)) {
            CVMJITerror(con, DEFINE_USED_NODE_MISMATCH,
                        "CVMJIT: Cannot store defined value");
	}
    };

root: DEFINE_VALUE64 reg64 : 10 : DEFINE_SYNTHESIS(con, $$) : : : {
        CVMRMResource* src = popResource(con);
	if (!CVMRMstoreDefinedValue(con, $$, src, 2)) {
            CVMJITerror(con, DEFINE_USED_NODE_MISMATCH,
                        "CVMJIT: Cannot store defined value");
	}
    };

root: LOAD_PHIS : 0 : : : : {
    CVMRMloadOrReleasePhis(con, $$, CVM_TRUE /* load */);
};

root: RELEASE_PHIS : 0 : : : : {
    CVMRMloadOrReleasePhis(con, $$, CVM_FALSE /* release */);
};

%{
static void
handleUsedNode(CVMJITCompilationContext* con, CVMJITRMContext* rc,
	       CVMJITIRNode* usedNode, int target, int avoid)
{
    CVMRMResource* rp = CVMJITirnodeGetUsedOp(usedNode)->resource;
    /* 
     * Don't eagerly load DEFINE of a USED at the same location. See
     * DEFINE_SYNTHESIS for details.
     */
    if (rp->expr->regsRequired != CVMCPU_AVOID_METHOD_CALL) {
	CVMRMpinResourceEagerlyIfDesireable(rc, rp, target, avoid);
    }
    CVMRMunpinResource(rc, rp);
    pushResource(con, rp);
}
%}

reg32: USED32 : 0 : : : :
    handleUsedNode(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);

reg64: USED64 : 0 : : : :
    handleUsedNode(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);

%{

static CVMRMResource*
invokeMethod(CVMJITCompilationContext *con, CVMJITRMContext* rc,
	     CVMJITIRNodePtr invokeNode)
{
    /*
     * Safe set is the empty set.  No registers are preserved.
     * To preserve some registers, we would need to save and restore them to
     * the Java stack (caller or callee frame) on transitions.
     */
    CVMRMResource *mbptr = popResource(con);
    CVMUint16 numberOfArgs = CVMJITirnodeGetBinaryOp(invokeNode)->data;
    CVMBool emittedDirectInvoke = CVM_FALSE;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument */
    CVMSMadjustJSP(con);
#endif


    /* mov  a1, mb */
    mbptr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), mbptr,
				     CVMCPU_ARG1_REG);

    /* TODO: Possible improvements:
     * - factor target capacity into caller capacity and
     *   avoid stack overflow check.
     */
    if (CVMRMisConstant(mbptr)) {
	CVMMethodBlock* targetMb = (CVMMethodBlock *)mbptr->constant;
	int invokerIdx = CVMmbInvokerIdx(targetMb);
	if (CVMmbIs(targetMb, NATIVE) && 
	    invokerIdx != CVM_INVOKE_LAZY_JNI_METHOD)
	{
	    /* It's CNI or JNI invoke, so call it directly. */
	    CVMRMregset safeSet;
	    int logicalPC;

	    CVMassert(
	        CVMmbJitInvoker(targetMb) == (void*)CVMCCMinvokeCNIMethod ||
		CVMmbJitInvoker(targetMb) == (void*)CVMCCMinvokeJNIMethod);

	    if (invokerIdx == CVM_INVOKE_JNI_METHOD ||
		invokerIdx == CVM_INVOKE_JNI_SYNC_METHOD)
	    {
		/* JNI - NV registers are preserved */
		safeSet = CVMRM_SAFE_SET;
	    } else {
		CVMassert(invokerIdx == CVM_INVOKE_CNI_METHOD);
		safeSet = CVMRM_EMPTY_SET;
	    }

	    CVMRMmajorSpill(con, ARG1, safeSet);
	    emittedDirectInvoke = CVM_TRUE;

	    if(CVMmbJitInvoker(targetMb) == (void*)CVMCCMinvokeCNIMethod) {
		CVMJITaddCodegenComment((con, 
			"call CVMCCMinvokeCNIMethod() for %C.%M", 
			 CVMmbClassBlock(targetMb), targetMb));
	    } else {
		CVMJITaddCodegenComment((con, 
			"call CVMCCMinvokeJNIMethod() for %C.%M", 
			 CVMmbClassBlock(targetMb), targetMb));
	    }
	    
	    logicalPC = CVMJITcbufGetLogicalPC(con);
	    CVMCPUemitAbsoluteCall(con, CVMmbJitInvoker(targetMb), 
				   CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);
	    CVMJITcsSetEmitInPlace(con);
	    CVMJITcsClearEmitInPlace(con);
	    CVMJITcsBeginBlock(con);
	} 

#ifdef CVM_JIT_PATCHED_METHOD_INVOCATIONS
	else if (CVMglobals.jit.pmiEnabled && 
                 invokerIdx != CVM_INVOKE_LAZY_JNI_METHOD) {
	    int flags = CVMJITirnodeGetBinaryNodeFlag(invokeNode);
	    CVMBool isVirtual = (flags & CVMJITBINOP_VIRTUAL_INVOKE) != 0; 

	    /* CVMRMmajorSpill must be done before setting up patch record
	     * because it will record the logical PC.
	     */
	    CVMRMmajorSpill(con, ARG1, CVMRM_EMPTY_SET);
	    if (CVMJITPMIaddPatchRecord(con->mb, targetMb,
					CVMJITcbufGetPhysicalPC(con),
					isVirtual))
	    {
		/*
		 * Patch record was successfully added, so make this a direct
		 * method call.
		 */
#ifdef CVM_DEBUG_ASSERTS
		int logicalPC = CVMJITcbufGetLogicalPC(con);
#endif
		/*
		 * Now we need to add targetMb to the "callee list" of the
		 * method we are compiling. This is needed so the targetMb
		 * has an entry for this patchable call, and it needs to be
		 * removed when we are decompiled by calling
		 * CVMJITpatchTableRemovePatchRecords.
		 *
		 * We keep a count of the number of unique targetMbs (callees)
		 * for this method.
		 *
		 * WARNING: this must be done before emitting any code.
		 * Otherwise if compilation fails while emitting code,
		 * this callee will never get registered properly.
		 */
		{
		    CVMUint32 numCallees = (CVMUint32)con->callees[0];
		    CVMUint32 idx;
		    CVMBool found = CVM_FALSE;
		    /* Search list for this method or an empty slot. */
		    for (idx = 1; idx <= numCallees; idx++) {
			CVMMethodBlock* mb = 
			    (CVMMethodBlock*)con->callees[idx]; 
			if (mb == targetMb) {
			    found = CVM_TRUE;
			    break;
			}
		    }
		    if (!found) {
			/* This is a unique callee. Add to the "callee list" */
			numCallees++;
			CVMtraceJITPatchedInvokes((
			    "PMI: callee(%d) slot used 0x%x %C.%M\n",
			    numCallees,
			    targetMb, CVMmbClassBlock(targetMb), targetMb));
			CVMassert(numCallees <= con->numCallees);
			con->callees[0] = (CVMMethodBlock*)numCallees;
			con->callees[numCallees] = targetMb;
		    }
		}

		/* Do the direct method call, either to the compiled method
		   or to CVMCCMletInterpreterDoInvoke. */
		emittedDirectInvoke = CVM_TRUE;
		CVMJITcsSetEmitInPlace(con);
		if (CVMmbIsCompiled(targetMb)) {
		    int targetOffset = CVMmbStartPC(targetMb) 
			- CVMJITcbufLogicalToPhysical(con, 0);
		    CVMJITaddCodegenComment((con,
		        "patchable invoke to compiled %C.%M",
		         CVMmbClassBlock(targetMb), targetMb));
		    CVMCPUemitBranchLink(con, targetOffset);
		} else {
		    CVMJITaddCodegenComment((con, 
		        "patchable invoke to interpreted (%C.%M)",
		        CVMmbClassBlock(targetMb), targetMb));
		    CVMCPUemitAbsoluteCall(con, 
					   (void*)CVMCCMletInterpreterDoInvoke,
					   CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);
		}
#ifdef CVMCPU_HAS_DELAY_SLOT
		CVMassert(CVMJITcbufGetLogicalPC(con) ==
			  logicalPC + 2 * CVMCPU_INSTRUCTION_SIZE);
#else
		CVMassert(CVMJITcbufGetLogicalPC(con) ==
			  logicalPC + CVMCPU_INSTRUCTION_SIZE);
#endif
		CVMJITcsClearEmitInPlace(con);
		CVMJITcsBeginBlock(con);
	    }
        }
#endif /* CVM_JIT_PATCHED_METHOD_INVOCATIONS */
    }

    if (!emittedDirectInvoke) {
	/* emit indirect call to caller */
    	CVMRMmajorSpill(con, ARG1, CVMRM_EMPTY_SET);
    	CVMCPUemitInvokeMethod(con);
    }

    CVMJITcaptureStackmap(con, numberOfArgs);
    CVMSMpopParameters(con, numberOfArgs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbptr);

    /* Dump the constant pool here after a method invocation if we need to: */
    CVMRISCemitConstantPoolDumpWithBranchAroundIfNeeded(con);

    /* Bind the result resource to the invokeNode: */
    return CVMSMinvocation(con, rc, invokeNode);
}

%}

//
// Method invocation.

// Purpose: VINVOKE(parameters, methodBlock)
root: VINVOKE parameters reg32 : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
        CVMJITprintCodegenComment(("Invoke a method w/ a void return type"));
	invokeMethod(con, CVMRM_INT_REGS(con), $$);
   };

// Purpose: value32 = IINVOKE(parameters, methodBlock)
invoke32_result: IINVOKE parameters reg32 : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
	CVMRMResource* dest;
        CVMJITprintCodegenComment(("Invoke a method w/ a 32bit return type"));
	dest = invokeMethod(con, CVMRM_INT_REGS(con), $$);
	pushResource(con, dest);
   };

// Purpose: value64 = LINVOKE(parameters, methodBlock)
invoke64_result: LINVOKE parameters reg32 : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
        CVMRMResource *dest;
        CVMJITprintCodegenComment(("Invoke a method w/ a 64bit return type"));
	dest = invokeMethod(con, CVMRM_INT_REGS(con), $$);
        pushResource(con, dest);
   };

//
// Intrinsic Method invocation.

%{

#ifdef CVMJIT_INTRINSICS
#ifdef CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER
static CVMRMregset
pinIntrinsicArgs(CVMJITCompilationContext *con,
                 CVMCPUCallContext *callContext,
                 CVMJITIRNodePtr intrinsicNode,
                 CVMJITIntrinsic *irec,
		 CVMBool useRegArgs)
{
    CVMRMregset outgoingRegs = CVMRM_EMPTY_SET;
    int numberOfArgs = irec->numberOfArgs;

    /* Pin the args to respective registers: */
    if (numberOfArgs != 0) {
        CVMJITIRNode *iargNode = CVMJITirnodeGetLeftSubtree(intrinsicNode);
        struct CVMJITStackElement* sp;
        int i;
        con->cgsp -= numberOfArgs;
        sp = con->cgsp + 1;
        /* Pin the high args first because they may need to be written to
           the stack and use scratch registers while they are at it.  The
           low args will get pinned into arg registers: */
        for (i = 0; i < numberOfArgs; i++) {
            CVMRMResource *arg = sp[i].u.r;
            arg = CVMCPUCCALLpinArg(con, callContext, arg,
                        CVMJITgetTypeTag(iargNode),
                        CVMJIT_IARG_ARG_NUMBER(iargNode),
                        CVMJIT_IARG_WORD_INDEX(iargNode),
                        &outgoingRegs, useRegArgs);
            sp[i].u.r = arg;
            iargNode = CVMJITirnodeGetRightSubtree(iargNode);
        }
        CVMassert(iargNode->tag == CVMJIT_ENCODE_NULL_IARG);
    }
    return outgoingRegs;
}

static void
relinquishIntrinsicArgs(CVMJITCompilationContext *con,
                        CVMCPUCallContext *callContext,
                        CVMJITIRNodePtr intrinsicNode,
                        CVMJITIntrinsic *irec,
			CVMBool useRegArgs)
{
    int numberOfArgs = irec->numberOfArgs;

    /* Pin the args to respective registers: */
    if (numberOfArgs != 0) {
        CVMJITIRNode *iargNode = CVMJITirnodeGetLeftSubtree(intrinsicNode);
        struct CVMJITStackElement* sp;
        int i;
        sp = con->cgsp + 1;
        for (i = 0; i < numberOfArgs; i++) {
            CVMRMResource *arg = sp[i].u.r;
            CVMCPUCCALLrelinquishArg(con, callContext, arg,
                CVMJITgetTypeTag(iargNode), CVMJIT_IARG_ARG_NUMBER(iargNode),
                CVMJIT_IARG_WORD_INDEX(iargNode), useRegArgs);
            iargNode = CVMJITirnodeGetRightSubtree(iargNode);
        }
        CVMassert(iargNode->tag == CVMJIT_ENCODE_NULL_IARG);
    }
}

static void
invokeIntrinsicMethod(CVMJITCompilationContext *con,
                      CVMJITIRNodePtr intrinsicNode)
{
    CVMUint16 intrinsicID = CVMJITirnodeGetBinaryOp(intrinsicNode)->data;
    CVMJITIntrinsic *irec = &CVMglobals.jit.intrinsics[intrinsicID - 1];
    const CVMJITIntrinsicConfig *config = irec->config;
    CVMUint16 properties = config->properties;

    CVMJITprintCodegenComment(("Invoke INTRINSIC %C.%M:",
                                CVMmbClassBlock(irec->mb), irec->mb));
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument */
    CVMSMadjustJSP(con);
#endif

    if ((properties & CVMJITINTRINSIC_OPERATOR_ARGS) != 0) {
        const CVMJITIntrinsicEmitterVtbl *emitter;
        /* NOTE: The emitter is responsible for popping arguments of the
           stack, and pushing any result back on the stack as well: */
        emitter = (const CVMJITIntrinsicEmitterVtbl *)
                      config->emitterOrCCMRuntimeHelper;
        emitter->emitOperator(con, intrinsicNode);
	CVMJITprintCodegenComment(("End INTRINSIC %C.%M:",
				   CVMmbClassBlock(irec->mb), irec->mb));

    } else {
        int rtnType = CVMJITgetTypeTag(intrinsicNode);
        CVMBool okToDumpCP, okToBranchAroundCP;
        CVMRMResource *dest = NULL;
        CVMBool useJavaStack;
        CVMBool useRegArgs;
        CVMRMregset outgoingRegSet = 0;
        CVMCPUCallContext callContext;
        CVMRMResource *cceeRes = NULL;

        useJavaStack = ((properties & CVMJITINTRINSIC_JAVA_ARGS) != 0);
        useRegArgs = GET_REG_ARGS(properties);

        /* Flush the Java frame pointer to the Java stack if neccesary.  Do
           this before we pin any arguments because they can adjust the stack
           frame and hence make it difficult to compute the value of the ccee
           which may be needed for this flush operation: */
        if ((properties & CVMJITINTRINSIC_FLUSH_JAVA_STACK_FRAME) != 0) {
	    int eeReg;
#ifndef CVMCPU_EE_REG
	    CVMRMResource *eeRes =
		CVMRMgetResource(CVMRM_INT_REGS(con),
				 CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	    eeReg = CVMRMgetRegisterNumber(eeRes);
	    /* Get the ee: */
	    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
		eeReg, CVMoffsetof(CVMCCExecEnv, eeX));
#else
	    eeReg = CVMCPU_EE_REG;
#endif
	    /* Store the JFP into the stack: */
	    CVMJITaddCodegenComment((con, "flush JFP to stack"));
	    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
                CVMCPU_JFP_REG, eeReg, offsetof(CVMExecEnv, interpreterStack) +
				       offsetof(CVMStack, currentFrame));
#ifndef CVMCPU_EE_REG
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
        }

        /* Load the ccee arg if necessary.  Do this before we pin any other
           arguments because they can adjust the stack frame and hence make
           it difficult to compute the value of the ccee: */
        if ((properties & CVMJITINTRINSIC_ADD_CCEE_ARG) != 0) {
            cceeRes =
                CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					 CVMCPU_ARG1_REG, 1);
            CVMCPUemitLoadCCEE(con, CVMCPU_ARG1_REG);
            outgoingRegSet |= ARG1;
        }

        /* Prepare the outgoing arguments: */
        if (useJavaStack) {
            outgoingRegSet |= CVMRM_EMPTY_SET;
        } else {
            CVMCPUCCALLinitArgs(con, &callContext, irec, CVM_FALSE,
				useRegArgs);
            outgoingRegSet |= pinIntrinsicArgs(con, &callContext,
                                  intrinsicNode, irec, useRegArgs);
        }

        /* Do spills if necessary: */
        if ((properties & CVMJITINTRINSIC_NEED_MINOR_SPILL) != 0) {
            CVMRMminorSpill(con, outgoingRegSet);
        } else if ((properties & CVMJITINTRINSIC_NEED_MAJOR_SPILL) != 0) {
	    /* NOTE: We need to remove the outgoing registers from the
	       safeSet because the intrinsic may alter those registers
	       without preserving them.  An example of this is in
	       intrinsics that does special hardware instructions that
	       may make use of the non-volatile registers, or assembly
	       functions that uses CVMJITINTRINSIC_REG_ARGS calling
	       conventions that may pass some arguments in non-volatile
	       registers.  For those cases, those outgoing non-volatile
	       registers are essentially treated like they are volatile.
	    */
	    CVMRMregset safeSet = CVMRM_SAFE_SET & ~outgoingRegSet;
            CVMRMmajorSpill(con, outgoingRegSet, safeSet);
        }

        okToDumpCP = ((properties & CVMJITINTRINSIC_CP_DUMP_OK) != 0);
        okToBranchAroundCP = okToDumpCP &&
            ((properties & CVMJITINTRINSIC_NEED_STACKMAP) == 0);

        CVMJITaddCodegenComment((con,
            "%C.%M helper", CVMmbClassBlock(irec->mb), irec->mb));
        if ((properties & CVMJITINTRINSIC_FLUSH_JAVA_STACK_FRAME) != 0) {
            /* Emit call to intrinsic helper: */
            CVMCPUemitFlushJavaStackFrameAndAbsoluteCall(con,
                config->emitterOrCCMRuntimeHelper,
                okToDumpCP, okToBranchAroundCP);
        } else {
            /* Emit call to intrinsic helper: */
            CVMCPUemitAbsoluteCall(con, config->emitterOrCCMRuntimeHelper,
                                   okToDumpCP, okToBranchAroundCP);
        }
        CVMJITcsBeginBlock(con);

        /* Capture stackmap if necessary: */
        if ((properties & CVMJITINTRINSIC_NEED_STACKMAP) != 0) {
            /* NOTE: Intrinsic helpers are always frameless.  Hence, the
               argSize we pass to CVMJITcaptureStackmap() is always 0 i.e.
               the caller is always responsible for scanning the args. */
            CVMJITcaptureStackmap(con, 0);
        }

        /* Release the ccee arg if necessary: */
        if ((properties & CVMJITINTRINSIC_ADD_CCEE_ARG) != 0) {
            CVMRMrelinquishResource(CVMRM_INT_REGS(con), cceeRes);
        }

        /* Setup the result resource if appropriate: */
        if (useJavaStack) {
            /* Pop the arguments of the Java stack: */
            CVMSMpopParameters(con, CVMmbArgsSize(irec->mb));
            /* Bind the result resource to the intrinsicNode: */
            dest = CVMSMinvocation(con, CVMRM_INT_REGS(con), intrinsicNode);
            if (rtnType != CVM_TYPEID_VOID) {
                pushResource(con, dest);
            } else {
                /* No resource will be bound for a void return type: */
                CVMassert(dest == NULL);
            }
        } else {
            if (rtnType != CVM_TYPEID_VOID) {
                int resultWords = ((rtnType == CVM_TYPEID_LONG) ||
                                   (rtnType == CVM_TYPEID_DOUBLE)) ? 2 : 1;
                dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
						CVMCPU_RESULT1_REG,
                                                resultWords);
                CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con),
					    dest, intrinsicNode);
            }
            relinquishIntrinsicArgs(con, &callContext, intrinsicNode,
				    irec, useRegArgs);
            CVMCPUCCALLdestroyArgs(con, &callContext, irec, CVM_FALSE,
				   useRegArgs);
            /* NOTE: We cannot must not push the dest resource until after
                     we relinquish the iargs.  This is because we're relying
                     on the stack to still hold the pointers to the iarg
                     resources.  Pushing the dest resource before we
                     relinquish the iargs would cause the pointer to the first
                     iarg resource to be trashed.
            */
            if (rtnType != CVM_TYPEID_VOID) {
                pushResource(con, dest);
            }
        }
    }
}
#endif /* CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER */
#endif /* CVMJIT_INTRINSICS */

%}

iargs: NULL_IARG : 0 : : END_TARGET_IARG(con, $$); : : ;
iargs: IARG reg32 iargs : 0 : : SET_TARGET_IARG(con, $$); : : ;
iargs: IARG reg64 iargs : 0 : : SET_TARGET_IARG(con, $$); : : ;

// Purpose: Rule to allow us to avoid loading the MB into a register if we
//          don't need to do any checkinit or nullCheck.
intrinsicMB: METHOD_BLOCK : 0 : : : : ;

// Purpose: Allows checkinits and nullChecks to be performed because they can
//          be attached to the MB using sequence nodes.  This is why the cost
//          need to be higher than that of the "intrinsicMB: ICONST_32" rule
//          above so that if we don't go through this rule if we only have an
//          MB and no sequence nodes.
intrinsicMB: reg32 : 10 : : : : {
        CVMRMResource *mbptr = popResource(con);
        /* Just throw the mbptr resource away because we don't really need
           it. */
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbptr);
    };

// Purpose: VINTRINSIC(parameters, methodBlock)
effect: VINTRINSIC parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value32 = INTRINSIC32(parameters, methodBlock)
invoke32_result: INTRINSIC32 parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value64 = INTRINSIC64(parameters, methodBlock)
invoke64_result: INTRINSIC64 parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: VINTRINSIC(iargs, methodBlock)
effect: VINTRINSIC iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value32 = INTRINSIC32(iargs, methodBlock)
reg32: INTRINSIC32 iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value64 = INTRINSIC64(iargs, methodBlock)
reg64: INTRINSIC64 iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: methodBlock = METHOD_BLOCK
reg32: METHOD_BLOCK : 10 : : : : {
        CVMRMResource *dest;
        CVMMethodBlock *mb = CVMJITirnodeGetConstantAddr($$)->mb;
        CVMJITsetSymbolName((con, "mb %C.%M", CVMmbClassBlock(mb), mb));
        dest =
	    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), (CVMInt32)mb);
	CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
					    dest, GET_REGISTER_GOALS);
	/* Need this in case this constant is a CSE */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

%{

/* Purpose: Emits code to do a NULL check of an object reference. */
static void doNullCheck(CVMJITCompilationContext *con,
			CVMJITIRNodePtr thisNode,
			CVMRMregset target, CVMRMregset avoid)
{
#ifdef CVMJIT_TRAP_BASED_NULL_CHECKS
    /* 
     * The NULL check is performed by doing a fake read using the pointer.
     * If null, we get a trap, which we catch and deal with.
     */
	CVMRMResource* objRes;
	CVMRMResource* scratch;

	CVMRMsynchronizeJavaLocals(con);

	/* Get the resource for the object to be NULL-checked */
	objRes = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), objRes, target, avoid);

	/* We are going to load into this register.
	   NOTE: We allocate the scratch after pinning the objRes because we
	         don't want to make sure that the objRes gets targetted to the
		 desired register first.
	*/
	scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				   CVMRM_ANY_SET, avoid, 1);
	
	CVMJITaddCodegenComment((con,
				 trapCheckComments[CVMJITIR_NULL_POINTER]));
	
	/* LDR Rscratch, [obj] */
        CVMJITcsSetExceptionInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(scratch), CVMRMgetRegisterNumber(objRes),
            0);

	/* We are done with the NULL check side effect. Pass the object on */
	if (thisNode != NULL) {
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), objRes, thisNode);
	} else {
	    CVMRMunpinResource(CVMRM_INT_REGS(con), objRes);
	}
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	pushResource(con, objRes);
#else
	CVMRMResource* objRes;

	CVMRMsynchronizeJavaLocals(con);

	/* Get the resource for the object to be NULL-checked */
	objRes = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), objRes, target, avoid);

	/* Compare to NULL */
        CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
                                  CVMRMgetRegisterNumber(objRes), 0);
	CVMJITaddCodegenComment((con,
	    trapCheckComments[CVMJITIR_NULL_POINTER]));
        CVMCPUemitAbsoluteCallConditional(con, 
            CVMCCMruntimeThrowNullPointerExceptionGlue,
            CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_EQ);
	CVMJITcsBeginBlock(con);

	/* We are done with the NULL check side effect. Pass the object on */
	if (thisNode != NULL) {
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), objRes, thisNode);
	} else {
	    CVMRMunpinResource(CVMRM_INT_REGS(con), objRes);
	}
	pushResource(con, objRes);
#endif
}

%}

// Purpose: Return object after null-checking
reg32: NULLCHECK reg32 : 20 : : : : {
        doNullCheck(con, $$, GET_REGISTER_GOALS);
    };
    
// Purpose: mb = FETCH_MB_FROM_VTABLE(GET_VTBL(object), methodOffset)
reg32: FETCH_MB_FROM_VTABLE GET_VTBL reg32 voffMemSpec : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
        CVMCPUMemSpec *vtblIndex = popMemSpec(con);
	CVMAddr        fixupAddress = popAddress(con);
	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* vtblBase =
	   CVMRMgetResource(CVMRM_INT_REGS(con),
			    CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	CVMRMResource* dest;

        CVMJITprintCodegenComment(("Fetch mb from vtable:"));
	CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);

	/* LDR rv, [robj] */
        CVMJITaddCodegenComment((con, "Get object.cb"));
        CVMJITcsSetExceptionInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(vtblBase), CVMRMgetRegisterNumber(objPtr),
            OBJECT_CB_OFFSET);

	/* Mask off special bits from the class pointer */
	/* BIC rv, rv, #3 */
        CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
				    CVMRMgetRegisterNumber(vtblBase),
				    CVMRMgetRegisterNumber(vtblBase), 0x3,
				    CVMJIT_NOSETCC);

	/* Now find the vtable pointer in the classblock */
	/* LDR rv, [rv + CB_VTBL_OFF] */
        CVMJITaddCodegenComment((con, "Get cb.vtbl"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(vtblBase),
            CVMRMgetRegisterNumber(vtblBase), CB_VTBL_OFFSET);

	/* Now vtblBase holds the vtable pointer. Do the rest. */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
	dest = CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);

        /* LDR rdest, [rv + vtblIndex*4] */
        CVMJITaddCodegenComment((con, "method = cb.vtbl[methodIdx]"));
        CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), vtblIndex, CVMRM_ANY_SET,
				 CVMRM_EMPTY_SET);
        CVMCPUemitMemoryReference(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(vtblBase),
            CVMCPUmemspecGetToken(con, vtblIndex));
        CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), vtblIndex);

	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), vtblBase,
		CVMJITirnodeGetLeftSubtree($$));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), vtblBase);
	/* if there is resolution code to fixup, do it here */
	if (fixupAddress != 0){
	    /* go back into code generated by resolveConstant
	     * and make it read:
	     *	mov	dest, CVMCPU_RESULT1_REG
	     *	b	<here>
	     */
	    CVMAddr loadMbBranchTarget = CVMJITcbufGetLogicalPC(con);
            CVMJITcbufPushFixup(con, fixupAddress);
            CVMJITcsSetEmitInPlace(con);
	    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, 
				   CVMRMgetRegisterNumber(dest),
				   CVMCPU_RESULT1_REG, CVMJIT_NOSETCC);
	    CVMCPUemitBranch(con, loadMbBranchTarget, CVMCPU_COND_AL);
            CVMJITcsClearEmitInPlace(con);
	    CVMJITcbufPop(con);
	}

	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: MB_TEST_OUTOFLINE(objMB, candidateMB)
root: MB_TEST_OUTOFLINE reg32 aluRhs : 30 : : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    CVMCPUALURhs* candidateMB = popALURhs(con);
    CVMRMResource* mb = popResource(con);

    CVMRMpinResource(CVMRM_INT_REGS(con), mb,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUalurhsPinResource(CVMRM_INT_REGS(con),
                            CVMCPU_CMP_OPCODE, candidateMB,
                            CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    /* cmp mb, candidateMB */
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_NE,
                      CVMRMgetRegisterNumber(mb),
                      CVMCPUalurhsGetToken(con, candidateMB));

    /* branch back to the inlined method if the MB test pass */
    CVMCPUemitBranch(con,
        con->currentCompilationBlock->oolReturnAddress, CVMCPU_COND_EQ);

    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), candidateMB);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), mb);

    /* No longer need to restrict register usage within the block
     * after this point.
     */
    CVMRMremoveRegSandboxRestriction(CVMRM_INT_REGS(con),
                                     con->currentCompilationBlock);

    CVMJITcsBeginBlock(con);	
#endif
    };

// Purpose: cb = FETCH_VCB(object)
reg32: FETCH_VCB reg32 : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
  	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* dest;

        CVMJITprintCodegenComment(("Fetch vcb:"));
        CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	dest =	CVMRMgetResource(CVMRM_INT_REGS(con),
                                 GET_REGISTER_GOALS, 1);

	/* LDR rv, [robj] */
        CVMJITaddCodegenComment((con, "Get object.cb"));
        CVMJITcsSetExceptionInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
                                           CVMRMgetRegisterNumber(dest),
                                           CVMRMgetRegisterNumber(objPtr),
                                           OBJECT_CB_OFFSET);

 	/* Mask off special bits from the class pointer */
	/* BIC rv, rv, #3 */
        CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
				    CVMRMgetRegisterNumber(dest),
				    CVMRMgetRegisterNumber(dest), 0x3,
				    CVMJIT_NOSETCC);

	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
#endif
    };

// FETCH_MB_FROM_VTABLE_OUTOFLINE(cb, methodOffset)
reg32: FETCH_MB_FROM_VTABLE_OUTOFLINE reg32 voffMemSpec : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
        CVMCPUMemSpec *vtblIndex = popMemSpec(con);
        CVMRMResource* cb;
        CVMRMResource* dest;
#ifdef CVM_DEBUG_ASSERTS	
        CVMAddr fixupAddress  = popAddress(con);
#else
        popAddress(con);
#endif
        /* The incoming CB should not be trashed because
         * it comes from phi, which is not part of the reserved
         * registers.
         */
        cb = popResource(con);

        CVMJITprintCodegenComment(("Fetch mb from vtable for outofline:"));
        CVMRMpinResource(CVMRM_INT_REGS(con), cb,
                         CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        dest = CVMRMgetResource(CVMRM_INT_REGS(con),
                                CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);

	/* Now find the vtable pointer in the classblock */
	/* LDR rVTBL, [rCB + CB_VTBL_OFF] */
        CVMJITaddCodegenComment((con, "Get cb.vtbl"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
                                           CVMRMgetRegisterNumber(dest),
                                           CVMRMgetRegisterNumber(cb),
                                           CB_VTBL_OFFSET);

        /* LDR rDest, [rVTBL + vtblIndex*4] */
        CVMJITaddCodegenComment((con, "method = cb.vtbl[methodIdx]"));
        CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), vtblIndex, 
			         CVMRM_ANY_SET, CVMRM_EMPTY_SET);

        CVMCPUemitMemoryReference(con, CVMCPU_LDR32_OPCODE,
                                  CVMRMgetRegisterNumber(dest), 
                                  CVMRMgetRegisterNumber(dest),
                                  CVMCPUmemspecGetToken(con, vtblIndex));

        CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), vtblIndex);
	CVMassert(fixupAddress == 0);

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cb);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
#endif 
    };

// Purpose: mb = FETCH_MB_FROM_ITABLE(GET_ITBL(object), methodBlock)
reg32: FETCH_MB_FROM_ITABLE GET_ITBL reg32 reg32 : 30 :
    SET_AVOID_C_CALL($$); : SET_TARGET2($$, ARG2, ARG3); : : {
        CVMRMResource* mbPtr = popResource(con);
	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* dest;

        objPtr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), objPtr,
					  CVMCPU_ARG2_REG);
        mbPtr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), mbPtr,
					 CVMCPU_ARG3_REG);
        CVMRMmajorSpill(con, ARG2|ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMJITprintCodegenComment(("Fetch mb from itable:"));

	/* call the helper glue */
        CVMJITaddCodegenComment((con,
				 "call CVMCCMruntimeLookupInterfaceMBGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLookupInterfaceMBGlue"));
        CVMJITstatsRecordInc(con,
            CVMJIT_STATS_CVMCCMruntimeLookupInterfaceMB);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeLookupInterfaceMBGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);

	/* reserve a word for the guess method */
        CVMJITaddCodegenComment((con, "interface lookup guess"));
        CVMJITemitWord(con, 0);
	CVMJITcaptureStackmap(con, 0);

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbPtr);
	pushResource(con, dest);
    };

// Purpose: Stores a 32 return value into a register.
reg32:	invoke32_result: 20 : : : : {
	/* force into a register */
	CVMRMResource* operand = popResource(con);
	CVMassert(CVMRMisJavaStackTopValue(operand));
	CVMRMpinResource(CVMRM_INT_REGS(con), operand, GET_REGISTER_GOALS);
	CVMRMunpinResource(CVMRM_INT_REGS(con), operand);
	pushResource(con, operand);
    };

// Purpose: Stores a 64 return value into a register pair.
reg64:  invoke64_result: 20 : : : : {
        /* force into a register */
        CVMRMResource *operand = popResource(con);
	CVMassert(CVMRMisJavaStackTopValue(operand));
	CVMRMpinResource(CVMRM_INT_REGS(con), operand, GET_REGISTER_GOALS);
        CVMRMunpinResource(CVMRM_INT_REGS(con), operand);
        pushResource(con, operand);
    };

parameters: NULL_PARAMETER : 0 : : : : ;
parameters: IPARAMETER param32 parameters : 0 : : : : ;
parameters: LPARAMETER param64 parameters : 0 : : : : ;
param32: invoke32_result : 0 : : : : {
	/* Free! Already on Stack  */
	CVMRMResource *operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
param32: reg32 : 10 : : : : {
	CVMRMResource *operand = popResource(con);
	CVMSMpushSingle(con, CVMRM_INT_REGS(con), operand);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
param64: invoke64_result : 0 : : : : {
        /* Free! Already on Stack  */
        CVMRMResource *operand = popResource(con);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
param64: reg64 : 20 : : : : {
	CVMRMResource *operand = popResource(con);
	CVMSMpushDouble(con, CVMRM_INT_REGS(con), operand);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

// decrement reference count on the expression.
effect: reg32: 0 : : : : {
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
root: invoke32_result: 0 : : : : {
	/* the 0 cost here is a fib, but must be < the cost of a deferred
	 * pop of invoke32_result into a reg32, so that this instruction
	 * gets emitted
	 */
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);

	CVMSMpopSingle(con, NULL, NULL);
    };

effect: reg64: 0 : : : : {
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
root: invoke64_result: 0 : : : : {
	/* the 0 cost here is a fib, but must be < the cost of a deferred
	 * pop of invoke64_result into a reg64, so that this instruction
	 * gets emitted
	 */
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    
	CVMSMpopDouble(con, NULL, NULL);
    };

//
// a comparison does two things:
// generates code to set the condition codes, and also emits
// the conditional branch instruction if requested.
//

%{
static void
branchToBlock(
    CVMJITCompilationContext* con,
    CVMCPUCondCode condcode,
    CVMJITIRBlock* target);

/* convert CVMJIT_XXX condition code to a CVMCPUCondCode */
static CVMCPUCondCode
mapCondCode(CVMUint16 condition) {
    switch(condition) {
        case CVMJIT_EQ: return CVMCPU_COND_EQ;
        case CVMJIT_NE: return CVMCPU_COND_NE;
        case CVMJIT_LE: return CVMCPU_COND_LE;
        case CVMJIT_GE: return CVMCPU_COND_GE;
        case CVMJIT_LT: return CVMCPU_COND_LT;
        case CVMJIT_GT: return CVMCPU_COND_GT;
        default: CVMassert(CVM_FALSE); return 0;
    }
}

static void
compare32cc(CVMJITCompilationContext *con,
	    CVMJITIRNodePtr thisNode, int opcode)
{
    CVMCPUALURhs* rhs = popALURhs(con);
    CVMRMResource* lhs = popResource(con);
    CVMJITConditionalBranch* branch = CVMJITirnodeGetCondBranchOp(thisNode);
    CVMJITIRBlock* target = branch->target;
    CVMCPUCondCode condCode = mapCondCode(branch->condition);
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    CVMRMregSandboxResources* sandboxRes = NULL;
#endif

#ifndef CVMCPU_HAS_COMPARE
    /* pin before calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUalurhsPinResource(CVMRM_INT_REGS(con), opcode, rhs,
			    CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitCompare(con, opcode, condCode,
		      CVMRMgetRegisterNumber(lhs),
		      CVMCPUalurhsGetToken(con, rhs));
    CVMRMsynchronizeJavaLocals(con);
#ifdef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), rhs);
    /* pin after calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif

#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    if(target->mtIndex != CVMJIT_IAI_VIRTUAL_INLINE_CB_TEST_DEFAULT) {
        /* reserve registers for outofline MB test */
        sandboxRes = CVMRMgetRegSandboxResources(
            CVMRM_INT_REGS(con), target,
            CVMRM_ANY_SET, CVMRM_EMPTY_SET,
            CVMCPU_VIRTUAL_INLINE_OUTOFLINE_MB_TEST_REG_NUM);
    }
#endif

    branchToBlock(con, condCode, target);

#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    if(target->mtIndex != CVMJIT_IAI_VIRTUAL_INLINE_CB_TEST_DEFAULT) {
        CVMJITcsBeginBlock(con);
	target->oolReturnAddress = CVMJITcbufGetLogicalPC(con);
        /* relinquish the sandbox resources */
        CVMRMrelinquishRegSandboxResources(CVMRM_INT_REGS(con),
                                           sandboxRes);
    }
#endif

#ifndef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), rhs);
#endif
    CVMRMunpinAllIncomingLocals(con, target);
}

static void
compare64cc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode)
{
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMJITConditionalBranch* branch = CVMJITirnodeGetCondBranchOp(thisNode);
    CVMJITIRBlock* target = branch->target;
    CVMCPUCondCode condCode = mapCondCode(branch->condition);

#ifndef CVMCPU_HAS_COMPARE
    /* pin before calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    condCode = CVMCPUemitCompare64(con, CVMCPU_CMP64_OPCODE, condCode,
                    CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs));
    CVMRMsynchronizeJavaLocals(con);
#ifdef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    /* pin after calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif

    branchToBlock(con, condCode, target);

#ifndef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
#endif
    CVMRMunpinAllIncomingLocals(con, target);
}

#ifdef CVM_NEED_DO_FCMP_HELPER

/* Purpose: Emits code to compare 2 floats by calling a helper function. */
static void
fcomparecc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
	   CVMBool needBranch, CVMBool needSetcc)
{
    CVMJITConditionalBranch* branch = NULL;
    CVMRMResource *rhs = popResource(con);
    CVMRMResource *lhs = popResource(con);
    CVMUint32 nanResult;
    int flags;
    
    /* If needBranch is TRUE, then we know this is CVMJITConditionalBranch.
     * Otherwise is is a CVMJITBinaryOp. */
    if (needBranch) {
	branch = CVMJITirnodeGetCondBranchOp(thisNode);
	flags = branch->flags;
    } else {
	flags = CVMJITirnodeGetBinaryNodeFlag(thisNode);
    }

    if (flags & CVMJITCMPOP_UNORDERED_LT) {
        nanResult = -1;
    } else {
        nanResult = 1;
    }

    /* Pin the input to the first two arguments because the helper expects it
       there: */
    lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, CVMCPU_ARG1_REG);
    rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, CVMCPU_ARG2_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, ARG1|ARG2);

    CVMJITaddCodegenComment((con, "do fcmp"));
    CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFCmp);
    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, nanResult);

    /* Emit the call to the helper to compute the result: */
    CVMJITaddCodegenComment((con, "call CVMCCMruntimeFCmp"));
    CVMJITsetSymbolName((con, "CVMCCMruntimeFCmp"));
    CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeFCmp,
                           CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
    CVMJITcsBeginBlock(con);

    /* if the needBranch is true, then we need to convert the {-1,0,1}
     * into a boolean condition code and do a conditional branch */
    if (needBranch) {
	CVMJITConditionalBranch* branch =
	    CVMJITirnodeGetCondBranchOp(thisNode);
	CVMCPUCondCode cc = mapCondCode(branch->condition);
	if (needSetcc) {
	    CVMJITaddCodegenComment((con, "set condition code"));
	    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, cc,
			      CVMCPU_RESULT1_REG, CVMCPUALURhsTokenConstZero);
	}
	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
	branchToBlock(con, cc, branch->target);
	CVMRMunpinAllIncomingLocals(con, branch->target);
    }

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
}

#endif /* CVM_NEED_DO_FCMP_HELPER */

#ifdef CVM_NEED_DO_DCMP_HELPER

/* Purpose: Emits code to compare 2 doubles by calling a helper. */
static void
dcomparecc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
	   CVMBool needBranch, CVMBool needSetcc)
{
    CVMJITConditionalBranch* branch = NULL;
    CVMRMResource *rhs = popResource(con);
    CVMRMResource *lhs = popResource(con);
    CVMUint32 nanResult;
    int flags;
    
    /* If needBranch is TRUE, then we know this is CVMJITConditionalBranch.
       Otherwise is is a CVMJITBinaryOp. */
    if (needBranch) {
	branch = CVMJITirnodeGetCondBranchOp(thisNode);
	flags = branch->flags;
    } else {
	flags = CVMJITirnodeGetBinaryNodeFlag(thisNode);
    }

    if (flags & CVMJITCMPOP_UNORDERED_LT) {
        nanResult = -1;
    } else {
        nanResult = 1;
    }

    /* Pin the input to the first two arguments because the helper expects it
       there: */
    lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, CVMCPU_ARG1_REG);
    rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, CVMCPU_ARG3_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, ARG1|ARG2|ARG3|ARG4);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        /* Both arguments are doubleword */
        CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                      CVMCPU_ARG1_REG);
        CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG2_REG,
                                      CVMCPU_ARG3_REG);
    }
#endif

    /* Emit the call to the helper to compute the result: */
    if (nanResult == -1) {
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDCmpl);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDCmpl"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDCmpl"));
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeDCmpl,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
    } else {
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDCmpg);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDCmpg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDCmpg"));
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeDCmpg,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
    }
    CVMJITcsBeginBlock(con);

    /* if the needBranch is true, then we need to convert the {-1,0,1}
     * into a boolean condition code and do a conditional branch */
    if (needBranch) {
	CVMCPUCondCode cc = mapCondCode(branch->condition);
	if (needSetcc) {
	    CVMJITaddCodegenComment((con, "set condition code"));
	    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, cc,
			      CVMCPU_RESULT1_REG, CVMCPUALURhsTokenConstZero);
	}
	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
	branchToBlock(con, cc, branch->target);
	CVMRMunpinAllIncomingLocals(con, branch->target);
    }

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
}

#endif /* CVM_NEED_DO_DCMP_HELPER */

%}

root: BCOND_INT reg32 aluRhs : 20 : : : :
        compare32cc(con, $$, CVMCPU_CMP_OPCODE);

root: BCOND_INT reg32 INEG32 aluRhs : 20 : : : :
        compare32cc(con, $$, CVMCPU_CMN_OPCODE);

root: BCOND_LONG reg64 reg64 : 20 : : : :
        compare64cc(con, $$);

root: BCOND_FLOAT reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        fcomparecc(con, $$,
		   CVM_TRUE /* needBranch */, CVM_TRUE /* needSetcc */);
    };

root: BCOND_DOUBLE reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        dcomparecc(con, $$,
		   CVM_TRUE /* needBranch */, CVM_TRUE /* needSetcc */);
    };

// Purpose: value32{-1,0,1} = LCMP(valueLong1, valueLong2)
reg32: LCMP reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLCmp"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLCmp"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLCmp);
        longBinary2WordHelper(con, (void*)CVMCCMruntimeLCmp, $$, CVM_FALSE);
    };


// Purpose: value32{-1,0,1} = FCMPL(valueFloat1, valueFloat2) by helper
reg32: FCMPL reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        CVMRMResource *dest;
        fcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);  
        pushResource(con, dest);
    };

// Purpose: value32{-1,0,1} = FCMPG(valueFloat1, valueFloat2) by helper
reg32: FCMPG reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        CVMRMResource *dest;
        fcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };


// Purpose: value32{-1,0,1} = DCMPL(valueDouble1, valueDouble2)
reg32: DCMPL reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        CVMRMResource *dest;
        dcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

// Purpose: value32{-1,0,1} = DCMPG(valueDouble1, valueDouble2)
reg32: DCMPG reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        CVMRMResource *dest;
        dcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

%{

#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
void
patchInstruction(CVMJITCompilationContext* con)
{
    CVMCPUInstruction* patchedInstructions = (CVMCPUInstruction*)
	(((CVMUint32)&con->gcCheckPcs->pcEntries[con->gcCheckPcsSize]
	  + sizeof(CVMCPUInstruction) - 1)
	 & ~(sizeof(CVMCPUInstruction)-1));
    CVMUint32      pcOffset;

    CVMassert(con->gcCheckPcsIndex < con->gcCheckPcsSize);

    patchedInstructions[con->gcCheckPcsIndex] =
	*(CVMCPUInstruction*)CVMJITcbufGetPhysicalPC(con);
    /* save away this offset in the gcCheckPcs table */
    pcOffset = CVMJITcbufGetLogicalPC(con) + 
	(con->codeEntry - con->codeBufAddr);
    CVMassert(pcOffset <= 0xffff); /* Make sure it fits */
    con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = pcOffset;
}
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */

static void
loadIncomingLocals(CVMJITCompilationContext* con,
		   CVMJITIRBlock *b, CVMBool spilledPhis)
{
#ifdef CVM_JIT_REGISTER_LOCALS
    /*
     * Backwards branch targets need to explicitly load all locals so OSR
     * will work. However, normally we skip this by setting up
     * b->logicalAddress to point after the loading of the locals.
     *
     * Note that we also need to load incoming locals after doing a gc to
     * reload ref locals.
     */
    if (b->incomingLocalsCount > 0) {
	CVMassert(spilledPhis == 0);
	/* This is the address we will OSR to */
	b->loadLocalsLogicalAddress = CVMJITcbufGetLogicalPC(con);
	CVMtraceJITCodegen((
            "\tL%d:\t%d:\t@ entry point when locals need to be loaded\n",
            CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));
	CVMRMpinAllIncomingLocals(con, b, CVM_TRUE);
	CVMRMunpinAllIncomingLocals(con, b);
    }
#endif /* CVM_JIT_REGISTER_LOCALS */
}

/*
 * At block entry, if we know this is the target of a backwards branch,
 * check for GC rendezvous request.
 */
void
CVMJITcheckGC(CVMJITCompilationContext* con, CVMJITIRBlock *b)
{
#if !defined(CVMJIT_TRAP_BASED_GC_CHECKS) && !defined(CVMJIT_PATCH_BASED_GC_CHECKS)
    /*
     * Generate code to do a gc rendezvous inline if one has been requested.
     */
    CVMBool spilledPhis;
    CVMRMResource* scratch;
    int scratchReg;
    int cvmGlobalsReg;

    CVMJITcsSetEmitInPlace(con);

    scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
			       CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
    scratchReg = CVMRMgetRegisterNumber(scratch);
    
    /* This is where branches to the block will branch to. */
    CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
        CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));

    /* this is where we branch to when a gc is needed (do_gc label) */
    b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

    /* spill phis */
    spilledPhis = CVMRMspillPhis(con, CVMRM_SAFE_SET);

    CVMJITprintCodegenComment(("Do GC Check:"));

#ifdef CVMCPU_CVMGLOBALS_REG
    cvmGlobalsReg = CVMCPU_CVMGLOBALS_REG;
#else
    /* load CVMglobals */
    cvmGlobalsReg = scratchReg;
    CVMJITaddCodegenComment((con, "CVMglobals"));
    CVMJITsetSymbolName((con, "CVMglobals"));
    CVMCPUemitLoadConstant(con, scratchReg, (CVMInt32)&CVMglobals);
#endif

    /* load CVMglobals.cstate[CVM_GC_SAFE].request */
    CVMJITaddCodegenComment((con, "CVMglobals.cstate[CVM_GC_SAFE].request;"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
	scratchReg, cvmGlobalsReg,
	offsetof(CVMGlobalState, cstate[CVM_GC_SAFE].request));

    /* check if gc requested */
    CVMJITaddCodegenComment((con, "If GC is requested,"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_NE,
			      scratchReg, 0);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);

    /* gc if requested */
    CVMJITaddCodegenComment((con, "CVMCCMruntimeGCRendezvousGlue"));
    CVMCPUemitAbsoluteCallConditional(con, CVMCCMruntimeGCRendezvousGlue,
				      CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK,
				      CVMCPU_COND_NE);

    /* Get a stackmap. This is a GC point */
    CVMJITcaptureStackmap(con, 0); 

    /* reload phis */
    if (spilledPhis) {
	CVMRMreloadPhis(con, CVMRM_SAFE_SET);
    }

    /* load incoming locals if necessary */
    loadIncomingLocals(con, b, spilledPhis);

    b->logicalAddress = CVMJITcbufGetLogicalPC(con);
    CVMJITcsClearEmitInPlace(con);

    return;
#else /* !CVMJIT_TRAP_BASED_GC_CHECKS && !CVMJIT_PATCH_BASED_GC_CHECKS */

#if defined(CVM_AOT) || defined(CVM_MTASK)
    /*
     * If either AOT or MTASK is enabled, use trap based GC for
     * precompiled/AOT code to avoid patching for the read only
     * code. For dynamic compiled code use patch based GC for
     * performance purpose.
     */
    if (CVMglobals.jit.isPrecompiling)
#endif
#ifdef CVMJIT_TRAP_BASED_GC_CHECKS
    {
        /*
          Just generate an instruction that will cause a trap (crash) when
          a gc is requested. There a 3 cases to worry about:

          (1) Trap-based Incoming Locals:
          ---------------------------
          bk->loadLocalsLogicalAddress:
          gc_return:
            <stackmap>
            load incoming locals
          bk->gcLogicalAddress:
            ldr  rGC,offset(gc_return)(rGC)
          bk->logicalAddress:
      
          -Trap redirects execution to CVMCCMruntimeGCRendezvousGlue,
           and sets LR to gc_return. The offset to gc_return is encoded
           in the ldr trap instruction.
      
          (2) Trap-based Incoming Phis:
          -------------------------
          do_gc:
            spill phis
            CVMCCMruntimeGCRendezvousGlue
            <stackmap>
            load phis
          bk->gcLogicalAddress:
            ldr  rGC,-offset(do_gc)(rGC)
          bk->logicalAddress:
    
          -Trap redirects execution to do_gc based on offset embedded in the
           trap instruction. Note that the offset is negative to indicate
           that execution needs to resume at the offset rather than just
           setting LR to the offset.
     
          (3) Trap-based Normal:
          ------------------
            <stackmap>
          bk->gcLogicalAddress:
            ldr  rGC,0(rGC)
          bk->logicalAddress:

          -Trap redirects execution to CVMCCMruntimeGCRendezvousGlue, 
           and sets LR to the trap instruction. The offset of 0 is 
           encoded the ldr trap instruction, so the trap handler knows 
           to set LR to be the same as the trap instruction.
        */

        CVMBool spilledPhis;
        CVMInt32 gcLogicalAddress;

        /*this is where we branch to when a gc is needed (do_gc label)*/
        gcLogicalAddress = CVMJITcbufGetLogicalPC(con);
        CVMJITcsSetEmitInPlace(con);

        /* spill phis */
        spilledPhis = CVMRMspillPhis(con, CVMRM_SAFE_SET);

        /* We only need an inlined unconditional gcRendezvous if there are
         * incoming phis.
         */
        if (spilledPhis) {
  	    /* unconditional gc  */
	    CVMJITaddCodegenComment((con, "CVMCCMruntimeGCRendezvousGlue"));
	    CVMCPUemitAbsoluteCall(con, CVMCCMruntimeGCRendezvousGlue,
			           CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);
        }

        if (CVMJITcbufGetLogicalPC(con) == 
	    CVMJITgetPreviousStackmapLogicalPC(con)) {
	    /* 6314307: If the previous block ended with a method call, then
             * this current pc already has a stackmap that includes the
	     * arguments to that call. When we generate a 2nd stackmap for
	     * this pc below, it won't end up getting used. If this
	     * gcCheck results in a gc, then non-existent stack items will
	     * be scanned. We need to force an extra instruction before
	     * the gcCheck instruction so the 2nd stackmap will be found.
	     */
	    CVMCPUemitNop(con);
	    /* we don't want GC to return to the nop or it will use the
	       wrong stackmap */
  	    CVMassert(gcLogicalAddress ==
		      CVMJITcbufGetLogicalPC(con) - CVMCPU_INSTRUCTION_SIZE);
	    gcLogicalAddress += CVMCPU_INSTRUCTION_SIZE;
        }

        /* Get a stackmap. This is a GC point */
        CVMJITcaptureStackmap(con, 0);

        /* reload phis */
        if (spilledPhis) {
	    CVMRMreloadPhis(con, CVMRM_SAFE_SET);
        }
    
        /* load incoming locals if necessary */
        loadIncomingLocals(con, b, spilledPhis);

#ifdef CVMCPU_HAS_VOLATILE_GC_REG
        /* rGC is not setup in an NV register. Get it from the ccee. Note
         * that it is normally setup before the backwards branch, which will
         * branch after this code. This one is in case we GC, in which case
         * rGC will be trashed before we re-enter the block.
         */
        CVMJITaddCodegenComment((con, "rGC = ccee->gcTrapAddr"));
        CVMCPUemitCCEEReferenceImmediate(con,
				     CVMCPU_LDR32_OPCODE, CVMCPU_GC_REG,
				     offsetof(CVMCCExecEnv, gcTrapAddr));
#endif

        /* This is where branches to the block will branch to. Note that 
         * we branch after the rGC setup above. In order to reduce load 
         * latency issues, we require that rGC be setup before doing the 
         * branch.
         */
        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
			b->blockID, CVMJITcbufGetLogicalPC(con)));
        b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

        {
	    /* Emit the instruction that will trap when a gc is
	     * requested. Note that the offset is the distance to the
	     * instructions above to handle the actual gcRendezvous. The SEGV
	     * handler will use this value to redirect execution to the above
	     * code.
	     */
	    int gcOffset = b->gcLogicalAddress - gcLogicalAddress;
	    CVMassert(gcOffset <=
		      sizeof(void*) * CVMJIT_MAX_GCTRAPADDR_WORD_OFFSET);
	    if (spilledPhis) {
                /* encode that there are incoming phis */
	        gcOffset = -gcOffset;
	    }
	    CVMJITaddCodegenComment((con, "gc trap instruction"));
	    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
					   CVMCPU_GC_REG, CVMCPU_GC_REG,
					   gcOffset);
            CVMassert((*((CVMCPUInstruction*)CVMJITcbufGetPhysicalPC(con)-1) &
                CVMCPU_GCTRAP_INSTRUCTION_MASK) == CVMCPU_GCTRAP_INSTRUCTION);
        }

        b->logicalAddress = CVMJITcbufGetLogicalPC(con);

        CVMJITcsClearEmitInPlace(con);
        return;
    }
#endif /* CVMJIT_TRAP_BASED_GC_CHECKS */

#if defined(CVM_AOT) || defined(CVM_MTASK)
    else
#endif

#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
    {
        /*
         * Generate a call to do the rendezvous, but also patch over the call
         * so normally it is not made until a gc is requested. This way there
         * is no overhead involved at the gc checkpoint.
         */
        int spilled;

        CVMJITcsSetEmitInPlace(con);
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        CVMassert(con->SMjspOffset == 0);
#endif

        /*
          There are 3 forms of backwards branch targets we worry about:

          (1) Patch-based Incoming Locals:
          -------------------------------
          do_gc:
              gcRendezvous    <-- patched until a GC is needed
          bk->loadLocalsLogicalAddress:
          load incoming locals
          bk->logicalAddress:
      
          (2) Patch-based Incoming Phis:
          ------------------------------
          do_gc:
              spill phis
              gcRendezvous    <-- patched until a GC is needed
              load phis
          bk->logicalAddress:
      
          (3) Patch-based Normal:
          -----------------------
          do_gc:
              gcRendezvous    <-- patched until a GC is needed
          bk->logicalAddress:
      
          When patching gcRendezvous, we do 1 of two things:
          -If there is no delay slot, gcRendezvous is patched by rewinding
           and overwriting it with whatever instruction would naturally
           come next.
          -If there is delay slot, gcRendezvous is patched with a nop.
      
          -All branches to the block are to bk->logicalAddress, thus avoiding
           any gcRendezvous overhead.
          -When doing a fallthrough, an explicit branch to bk->logicalAddress
           is used, unless there is no phis or incoming locals, in which case
           a fallthrough is allowed.
      
          -If there are incoming locals, the first MAP_PC node in the block
           is remapped to bk->loadLocalsLogicalAddress for OSR. There is no 
           OSR when there are incoming phis, and no remapping is needed in 
           the normal case.
         */

        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ Patchable GC Check point\n",
             CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));

        /* this is where we branch to when a gc is needed (do_gc label) */
        b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

        spilled = CVMRMspillPhisCount(con, CVMRM_SAFE_SET);

        /*
         * Need to spill phis before doing a gc and reload them after the gc
         * in case either they are ref phis or are in volatile registers.
         * See case (2) above.
         */
        if (spilled > 0) {
	    CVMJITprintCodegenComment(("******* PHI spill %C.%M\n",
				       CVMmbClassBlock(con->mb), con->mb));
	    CVMRMspillPhis(con, CVMRM_SAFE_SET);
        }

        if (CVMJITcbufGetLogicalPC(con) == 
	    CVMJITgetPreviousStackmapLogicalPC(con)) {
	    /* If the previous block ended with a method call, then this
	     * current pc already has a stackmap that includes the
	     * arguments to that call. When we generate a 2nd stackmap for
	     * this pc below, it won't end up getting used. If this
	     * gcCheck results in a gc, then non-existent stack items will
	     * be scanned. We need to force an extra instruction before
	     * the gcCheck instruction so the 2nd stackmap will be found.
	     */
   	    /*
	     * NOTE: We could do better here. Currently we don't always
	     * flush JSP when making method calls, so the stackmap is our
	     * only indicator of how much we need to scan. If we always
	     * flush JSP, then the scanner can just use the previous
	     * stackmap and stop scanning when topOfStack is reached. This
	     * is a fix that was made in 1.0, and also allows us to
	     * remove another fix involving exception handling when
	     * TOS isn't flushed. See bug #4732902.  
	    */
	    CVMCPUemitNop(con);
        }

        /* 1. Emit the gc rendezvous instruction. Tell the emitter that
         *    we are going to rewind the code buffer. */
        CVMCPUemitGcCheck(con, CVM_TRUE);
        /* 2. rewind to just before the rendezvous instruction */
        CVMJITcbufRewind(con, sizeof(CVMCPUInstruction));
        /* 3. save away the gc rendezvous instruction. */
        patchInstruction(con);

#ifdef CVMCPU_NUM_NOPS_FOR_GC_PATCH
        {
            int i;
            /* We only patch one instruction, which is the branch or call to
             * CVMCCMruntimeGCRendezvousGlue. For platforms like Sparc that
             * have a delay slot after a call(or branch), we put nop's at
             * the patch location and also one instruction after patch, so
             * when GC is patched we can guarantee that the delay slot does 
             * not contain any unexpected instruction. On platforms that 
             * require multiple instructions to do the call (like powerpc 
             * when we don't copy to the code cache) we also need to 
             * generate a nop so we don't attempt to return to the call 
             * instruction, but instead return to the instruction after it.
             * Otherwise if it gets repatched before we return, we executed
             * the call instruction without executing the code to setup the
             * call properly.
             */
            for (i = 0; i < CVMCPU_NUM_NOPS_FOR_GC_PATCH; i++) {
                CVMCPUemitNop(con);
            }
        }
#endif

        /* Get a stackmap. This is a GC point */
        CVMJITcaptureStackmap(con, 0);

        /* reload the spilled phis. See case (2) above. */
        if (spilled > 0) {
	    CVMRMreloadPhis(con, CVMRM_SAFE_SET);
        }

        /* load incoming locals if necessary */
        loadIncomingLocals(con, b, spilled > 0);

        /*
         * This is where branches to the block will branch to. Note that
         * we are smart here and make branches to this block branch after
         * the loading of locals, since the locals will already be loaded
         */
        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
            CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));
        b->logicalAddress = CVMJITcbufGetLogicalPC(con);
        CVMJITcsClearEmitInPlace(con);
    }
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */
#endif /* !CVMJIT_TRAP_BASED_GC_CHECKS && !CVMJIT_PATCH_BASED_GC_CHECKS */
}

static void
branchToBlock(
    CVMJITCompilationContext* con,
    CVMCPUCondCode condcode,
    CVMJITIRBlock* target)
{
     CVMJITFixupElement** fixupList = NULL;
     CVMUint32 targetAddress = target->logicalAddress;

     CVMJITcsSetEmitInPlace(con);

    /*
     * If we are branching to a block that we haven't compiled yet,
     * then add the instruction to the list that must be fixed up
     * once the target block is compiled.
     */
    if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
	if (condcode == CVMCPU_COND_AL) {
	    fixupList = &(target->branchFixupList);
	} else {
	    fixupList = &(target->condBranchFixupList);
	}
    }

#if !defined(CVMJIT_PATCH_BASED_GC_CHECKS) && !defined(CVMJIT_TRAP_BASED_GC_CHECKS)
    if (CVMJITirblockIsBackwardBranchTarget(target)) {
	targetAddress = target->gcLogicalAddress;
    }
    CVMJITaddCodegenComment((con, "branch to block L%d",
                            CVMJITirblockGetBlockID(target)));
    CVMCPUemitBranchNeedFixup(con, targetAddress, condcode, fixupList);
#else /* !CVMJIT_TRAP_BASED_GC_CHECKS && !CVMJIT_PATCH_BASED_GC_CHECKS */

#if defined(CVM_AOT) || defined(CVM_MTASK)
    /* Compiling AOT/warmup code, use trap based GC */
    if (CVMglobals.jit.isPrecompiling)
#endif
#ifdef CVMJIT_TRAP_BASED_GC_CHECKS
    {
        if (CVMJITirblockIsBackwardBranchTarget(target)) {
	    targetAddress = target->gcLogicalAddress;
#ifdef CVMCPU_HAS_VOLATILE_GC_REG
	    /* rGC is not setup in an NV register. Get it from the ccee: */
	    CVMJITaddCodegenComment((con, "rGC = ccee->gcTrapAddr"));
	    CVMCPUemitCCEEReferenceImmediate(con,
					 CVMCPU_LDR32_OPCODE, CVMCPU_GC_REG,
					 offsetof(CVMCCExecEnv, gcTrapAddr));
#endif
        }
        CVMJITaddCodegenComment(
            (con, "branch to block L%d", CVMJITirblockGetBlockID(target)));
        CVMCPUemitBranchNeedFixup(con, targetAddress, condcode, fixupList);
    }
#endif
#if defined(CVM_AOT) || defined(CVM_MTASK)
    else
#endif
#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
    {
        /* 
         * We need to do a patchable branch to backwards branch targets so we
         * don't execute gcRendezvous code everytime.
         */
        if (CVMJITirblockIsBackwardBranchTarget(target))
        {
            CVMCPUInstruction* patchedInstructions = (CVMCPUInstruction*)
                (((CVMUint32)&con->gcCheckPcs->pcEntries[con->gcCheckPcsSize]
                  + sizeof(CVMCPUInstruction) - 1)
                  & ~(sizeof(CVMCPUInstruction)-1));

            CVMassert(con->gcCheckPcsIndex < con->gcCheckPcsSize);
	    CVMJITaddCodegenComment(
                (con, "branch to block L%d", CVMJITirblockGetBlockID(target)));

            if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
                /* This must be a forward branch. */
                CVMCPUemitBranchNeedFixup(con, targetAddress,
                                          condcode, fixupList);
                patchedInstructions[con->gcCheckPcsIndex] = 0;
                con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = 0;
            } else {
                int rewindOffset = 0;
	        int branchPC = CVMJITcbufGetLogicalPC(con);
	        int patchPC = 0;

	        CVMassert(target->gcLogicalAddress != 0);
	        CVMassert(target->gcLogicalAddress <= targetAddress);

                /* No need to patch the backwards branch if 
                 * (targetAddress == target->gcLogicalAddress)
                 */
                if (targetAddress != target->gcLogicalAddress) {
                    CVMJITprintCodegenComment(
                        ("Patchable backwards branch:")); 
	            /* first emit branch to instructions that will gc */
                    CVMCPUemitBranch(con, 
                         target->gcLogicalAddress, condcode);

#ifdef CVMCPU_HAS_DELAY_SLOT
	            /* NOTE: This is not going to work for SH4. The
	               CVMCCMruntimeGCRendezvousGlue() call is going to take
                       more than two instruction, including the nop in the 
                       delay slot. We need an HPI #define for the number 
                       of instructions. Also, the patchInstruction() call 
                       below needs to be done for each instruction used in 
                       the branch, instead of assuming there is just one.
	            */
	            patchPC = CVMJITcbufGetLogicalPC(con) -
		        2 * CVMCPU_INSTRUCTION_SIZE;
#else
	            patchPC = CVMJITcbufGetLogicalPC(con) -
		        1 * CVMCPU_INSTRUCTION_SIZE;
#endif

                    /* If the platform has branch delay slot (such as SPARC
                     * 'call' instruction), space is reserved for the GC 
                     * check patching instruction at the beginning of a 
                     * backward branch target (see checkGC()).
                     *
                     * Normally, the reserved space is skipped when branching
                     * to the target for performance reason. When GC is 
                     * requested, the branch instruction will be patched to 
                     * the beginning of the target, so that GCRendezvous 
                     * can be invoked.
                     */
	            /* NOTE: This is not going to work for SH4. The two 
                       backwards branches we emit (one above and then the 
                       2nd a bit further down) might take a different 
                       number of instructions, given the SH4's limited 
                       conditional branch range (256 bytes). The 2nd might 
                       be in range and the 1st not (since it is a bit
		       further away). The fix is to count the number of 
                       instructions the first branch took, and if the 2nd 
                       takes fewer, fill it out with nops.
	            */
                    rewindOffset = CVMJITcbufGetLogicalPC(con) - patchPC;
                    CVMJITcbufRewind(con, rewindOffset);
                    /* save away the branch instruction. */
	            patchInstruction(con);
                    /* reemit branch */
	            CVMJITcbufRewind(con, patchPC - branchPC);
                } else {
                    /* no patch for this backwards branch. */
                    patchedInstructions[con->gcCheckPcsIndex] = 0;
                    con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = 0;
                }
	        /* now emit branch to normal block entry which won't gc */
	        CVMJITaddCodegenComment((con, "branch to block L%d, skip GC",
                                         CVMJITirblockGetBlockID(target)));
                CVMCPUemitBranch(con, targetAddress, condcode);
                if (targetAddress != target->gcLogicalAddress) {
	            if (CVMJITcbufGetLogicalPC(con) - patchPC !=
                                                        rewindOffset) {
		        CVMJITerror(con, CANNOT_COMPILE,
                            "CVMJIT: Inconsistent backwards branch instructions");
	            }
                }
            }
         } else {
	    CVMJITaddCodegenComment(
                (con, "branch to block L%d", CVMJITirblockGetBlockID(target)));
	    CVMCPUemitBranchNeedFixup(con, targetAddress,
                                      condcode, fixupList);
         }
     }
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */
#endif /* !CVMJIT_TRAP_BASED_GC_CHECKS && !CVMJIT_PATCH_BASED_GC_CHECKS */
     CVMJITcsClearEmitInPlace(con);
}

static void
jsrToBlock(
    CVMJITCompilationContext* con,
    CVMJITIRBlock* target)
{
    CVMJITaddCodegenComment((con, "jsr to block L%d",
                             CVMJITirblockGetBlockID(target)));
    if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
        CVMCPUemitBranchLinkNeedFixup(con, target->logicalAddress,
                                      &(target->branchFixupList));
    } else {
        CVMCPUemitBranchLink(con, target->logicalAddress);
    }
}

%}

root:	TABLESWITCH reg32 : 40 : : : : {
	CVMJITTableSwitch* ts = CVMJITirnodeGetTableSwitchOp($$);
	int i;
	int low = ts->low;
        int high = ts->high;
        CVMRMResource* key = popResource(con);
        int keyRegNo;

        CVMJITprintCodegenComment(("tableswitch"));
	CVMRMsynchronizeJavaLocals(con);

    if (CVMRMisConstant(key) && CVMRMgetConstant(key) == 0) {
            /* We know the key is a constant 0. So this can be simplified. */
            if (low <= 0 && high >= 0) {
                CVMJITIRBlock *target = ts->tableList[0-low];
                branchToBlock(con, CVMCPU_COND_AL, target);
            } else {
                branchToBlock(con, CVMCPU_COND_AL, ts->defaultTarget);
            }
        } else {
	    /*
	     * After pinning the key resource, we are going to modify
	     * the register it gets pinned to. This causes problems because
	     * the resource may be associated with a local or constant
	     * that will be used later. It could also be a DEFINE node,
	     * which means its refcount > 1. This is handled below.
	     */
            CVMRMpinResource(CVMRM_INT_REGS(con), key,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	    if (CVMRMgetRefCount(CVMRM_INT_REGS(con), key) > 1) {
		/* If there is more than one reference to this resource,
		 * then make a clone and use the clone as the key. This
		 * way we can change it without affecting the other
		 * references.
		 */
		CVMRMResource* clone =
		    CVMRMcloneResourceStrict(CVMRM_INT_REGS(con), key,
					     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
		CVMRMunpinResource(CVMRM_INT_REGS(con), key);
		key = clone;
		keyRegNo = CVMRMgetRegisterNumber(key);
	    } else {
		/* The refcount is 1, but there may be a cached local
		 * or constant referencing this resource. We must
		 * relinquish it and then grab it again to force any
		 * cached referenced to be decached.
		 */
		keyRegNo = CVMRMgetRegisterNumber(key);
		CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
		key =
		    CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), keyRegNo, 1);
	    }

	    /* subtract "low" from "key" since branch table is 0-based */
            if (low != 0) {
	        int opcode;    /* add or sub */
   	        int absLow;    /* absolute value of low */
	        if (low > 0) {
                    opcode = CVMCPU_SUB_OPCODE;
		    absLow = low;
	        } else {
	            opcode = CVMCPU_ADD_OPCODE;
	    	    absLow = -low;
	        }
                CVMCPUemitBinaryALUConstant(con, opcode, keyRegNo, keyRegNo,
				   	    absLow, CVMJIT_NOSETCC);
            }

            /* compare (key - low) to (high - low) */
	    {
	        CVMUint32 highLow = high - low;
                CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
					  CVMCPU_COND_HI, keyRegNo, highLow);
	    }

	    /* emit branch for the default target */
            branchToBlock(con, CVMCPU_COND_HI, ts->defaultTarget);

	    CVMCPUemitTableSwitchBranch(con, keyRegNo);
            CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);

	    /* emit branches for each table element */
	    for (i = 0; i < ts->nElements; i++) {
	        CVMJITIRBlock* target = ts->tableList[i];
                branchToBlock(con, CVMCPU_COND_AL, target);
	    }
        }
    };

%{
typedef struct {
    CVMUint16 low;
    CVMUint16 high;
    CVMUint16 prevIndex;
    CVMInt32 logicalAddress; /* address that branches to this node */
} CVMLookupSwitchStackItem;
 
static void
pushLookupNode(CVMLookupSwitchStackItem* stack, CVMUint8* tos, 
               CVMUint16 low, CVMUint16 high, CVMUint16 prevIndex,
	       CVMInt32 logicalAddress)
{
    CVMLookupSwitchStackItem* item = &stack[*tos];
    CVMtraceJITCodegen(("---> Pushing #%d (low=%d index=%d high=%d prev=%d)\n",
                       *tos, low, (low + high)/2, high, prevIndex));
    item->low   = low;
    item->high  = high;
    item->prevIndex  = prevIndex;
    item->logicalAddress = logicalAddress;
    ++(*tos);
}

static CVMLookupSwitchStackItem*
popLookupNode(CVMLookupSwitchStackItem stack[], CVMUint8* tos)
{
    CVMLookupSwitchStackItem* item;
    if (*tos == 0) {
	return 0;
    }
    --(*tos);
    item = &stack[*tos];
    CVMtraceJITCodegen(("<--- Popping #%d (low=%d index=%d high=%d prev=%d)\n",
                       *tos, item->low, (item->low + item->high) / 2,
                       item->high, item->prevIndex));
    return item;
}
%}

root:	LOOKUPSWITCH reg32 : 40 : : : : {
	/*
	 * There are really 4 ways to do a lookup switch based on
	 * whether you not you want to do a binary search and whether or not
	 * you want a table driven loop, or an unrolled loop.
	 *
	 *   1. binary search with loop
	 *   2. binary search with unrolled loop
	 *   3. linear search with loop
	 *   4. linear search with unrolled loop
	 *
	 * Here are advantages and disadvantages of all:
	 *
	 *   -Unrolled loops are always faster then their loop counterparts,
	 *    but for large tables they are almost 50% bigger. The breakeven
	 *    point for size seems to be around 10 entries.
	 *   -Unrolled loops require little if any loads from memory, whereas
	 *    loops require 1 or 2 loads, depending on how the branch is
	 *     handled.
	 *   -Unrolled loop only require a register for the key and possibly
	 *    for a large temporary constant. loops require more registers,
	 *    espcially when doing a binary search.
	 *   -(1) is faster than (3) for large data sets, but slower for small.
	 *   -(2) is almost always faster than (4). It's only disadvantage
	 *    to (4) is that it may cause more constant values to be loaded
	 *    into a register  rather than used immediate value, but this 
	 *    is rare.
	 *
	 * In summary, a binary search with an unrolled loop is fastest and 
	 * the most register efficient and is worth the extra space overhead
         * for large tables, espcially since most lookup tables tend to be on
	 * the smaller size, so this is what we chose.
	 *
	 * To generate the code we need to traverse the sorted lookupswitch
	 * table in a manner similar to the way a binary search would. The
	 * difference is that we aren't looking for anything, but instead
	 *  have to visit each node and generate code for it.
	 *
	 * For each node visited we do the following (it helps to visualize the
	 * table organized as a balanced binary tree):
	 *  1. generate compare code to see if this node is a match.
	 *  2. If there is a "left subtree", push a context for it so we can
	 *     generate code for it later and generate a "blt" to it.
	 *  3. Generate a "beq" to branch to the correct target for a match
	 *  4. If there is a "right subtree", make it the next node to visit.
	 *  5. If there is not a "right subtree", pop the topmost node off of
	 *     the stack and make it the next node to visit. If there is 
	 *     nothing on the stack then we are done.
	 *
	 * There's one more thing that makes the code a bit tricky. We don't
	 * use cmp to compare the key to the matchValue. We use sub instead
	 * (or add if the value is negative). The advantage of this is that if
	 * the table has a bunch of values in a narrow range, but all are 
	 * > 255, then we can avoid having to do an ldr for each one of them.
	 * Instead, we just ldr the first one we need to check, and the rest
	 * of the time we can just base the "compare" values on the difference
	 * between the matchValue of the current node and the matchValue of
	 * the previous node we visited.
	 */
	CVMJITLookupSwitch* ls = CVMJITirnodeGetLookupSwitchOp($$);
        CVMRMResource* key = popResource(con);
	int keyreg;
        CVMRMResource* scratch;
	int scratchreg;
     	CVMUint16 low  = 0;	
    	CVMUint16 high = ls->nPairs - 1;
    	CVMUint16 index = (low + high) / 2;
    	CVMLookupSwitchStackItem stack[16]; /* enough for nPairs == 64k */
    	CVMUint8  tos = 0;
	CVMBool useCmp; /* use cmp rather than adds and subs */
        int branchToDefaultPC = -1;
#ifdef CVMCPU_HAS_ALU_SETCC
	CVMInt32  prevMatchValue = 0;
#endif
        CVMJITcsSetEmitInPlace(con);

        CVMJITprintCodegenComment(("lookupswitch"));
	CVMRMsynchronizeJavaLocals(con);

	/* if there are no pairs, then just branch to the target. */
	if (ls->nPairs == 0) {
            branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
	    goto done;
	}

#ifndef CVMCPU_HAS_ALU_SETCC
	useCmp = CVM_TRUE;
#else
	{
	    /* if (hi - lo < 0), then that means it is actually greater than
	     * MININT, so we need to use compares rather than adds/subs.
	     */
	    int lo = ls->lookupList[0].matchValue;
	    int hi = ls->lookupList[ls->nPairs - 1].matchValue;
	    useCmp = (lo < 0) && (hi > 0) && (hi - lo < 0);
	}
#endif

	if (CVMRMisConstant(key) && CVMRMgetConstant(key) == 0) {
            /* We know the key is a constant 0. So this can be simplified. */
            int i;
            for (i = 0; i < ls->nPairs; i++) {
                if (ls->lookupList[i].matchValue == 0) {
                    /* If we found a match, then branch to its handler. */
                    branchToBlock(con, CVMCPU_COND_AL, ls->lookupList[i].dest);
                    goto done;
                }
            }
            branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
            goto done;
        }

	/*
	 * After pinning the key resource, we are going to modify
	 * the register it gets pinned to. This causes problems because
	 * the resource may be associated with a local or constant
	 * that will be used later. It could also be a DEFINE node,
	 * which means its refcount > 1. This is handled below.
	 */
	CVMRMpinResource(CVMRM_INT_REGS(con), key,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	if (CVMRMgetRefCount(CVMRM_INT_REGS(con), key) > 1) {
	    /* If there is more than one reference to this resource,
	     * then make a clone and use the clone as the key. This
	     * way we can change it without affecting the other
	     * references.
	     */
	    CVMRMResource* clone =
		CVMRMcloneResourceStrict(CVMRM_INT_REGS(con), key,
					 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	    CVMRMunpinResource(CVMRM_INT_REGS(con), key);
	    key = clone;
	    keyreg = CVMRMgetRegisterNumber(key);
	} else {
	    /* The refcount is 1, but there may be a cached local
	     * or constant referencing this resource. We must
	     * relinquish it and then grab it again to force any
	     * cached referenced to be decached.
	     */
	    keyreg = CVMRMgetRegisterNumber(key);
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
	    key =
		CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), keyreg, 1);
	}
	
	/*
	 * We may need a scratch register for loading large constant
	 * values into. Get it now (and allow for any necessary spills).
	 * because it will be to late once we start generating code (we
	 * don't want the regman state to change)
	 */
	scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				   CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	scratchreg = CVMRMgetRegisterNumber(scratch);
	
	
	/*
	 * On each iteration, look at the lookupswitch node at index,
	 * generate the appropriate code, push state for a "lower" node
	 * to be generated if necessary, and setup state for next "higher"
	 * node to be generated if necessary. We are all done what there
	 * are no nodes on the stack and no "higher" nodes left.
	 */
	con->inConditionalCode = CVM_TRUE;
	while (1) {
	    /*
	     * diff is the absolute value of the difference between this
	     * nodes matchValue and the previous nodes matchValue. It needs
	     * to be an absolute value and an uint because it may be larger
	     * than MAXINT.
	     */
#ifdef CVMCPU_HAS_ALU_SETCC
	    CVMUint16      prevIndex;
#endif
	    CVMInt32       matchValue = ls->lookupList[index].matchValue;
	    
	    CVMtraceJITCodegen((".match%d (low=%d index=%d high=%d)\n",
				index, low, index, high));
#ifdef CVMCPU_HAS_ALU_SETCC
	    if (!useCmp) {
		CVMUint32      diff;
		int            opcode; /* do we add or subtract diff */
		/* compute diff */
		if (prevMatchValue > matchValue) {
		    opcode = CVMCPU_ADD_OPCODE;
		    diff = prevMatchValue - matchValue;
		} else {
		    opcode = CVMCPU_SUB_OPCODE;
		    diff = matchValue - prevMatchValue;
		}
		
		/* add or subtract diff from key and setcc.
		 * WARNING: don't let CVMCPUemitBinaryALUConstant() load a
		 * large constant into a register. This will change the
		 * regman state, which is a no-no in conditionally executed
		 * code. Instead, manually load the large constant here
		 * using CVMCPUemitLoadConstant(), which won't affect the
		 * regman state.
		 */
		if (CVMCPUalurhsIsEncodableAsImmediate(opcode, diff)) {
		    CVMCPUemitBinaryALUConstant(con, opcode,
		        keyreg, keyreg, diff, CVMJIT_SETCC);
		} else {
		    CVMCPUemitLoadConstant(con, scratchreg, diff);
		    CVMCPUemitBinaryALURegister(con, opcode,
			keyreg, keyreg, scratchreg, CVMJIT_SETCC);
		}
	    }
#endif
	    
	    /*
	     * There are 3 states we can be in.
	     * 1. low < index < high, which means that if we don't have
	     *    a match, we need to deal both with the possiblity of
	     *    index being too big or too large.
	     * 2. low == index < high, which means that if we don't have
	     *    a match, we only need to deal with the possibility of
	     *    index being too small.
	     * 3. low == index == high, which means either we find a
	     *    a match this time or the lookup failed.
	     * NOTE: low < index == high is not possible.
	     */
	    if (low != high) {          /* (1) and (2) */
		if (index != low) {     /* (1) only */
		    if (useCmp) {
			/* WARNING: don't let regman know about constant */
			if (CVMCPUalurhsIsEncodableAsImmediate(
                                CVMCPU_CMP_OPCODE, matchValue)) {
			    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
				CVMCPU_COND_LT, keyreg, matchValue);
			} else {
			    CVMCPUemitLoadConstant(con, scratchreg,
						   matchValue);
			    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
			        CVMCPU_COND_LT, keyreg, scratchreg);
			}
		    }
		    /* If key is too small, branch to node that will handle
		     * a smaller match, but first push a context so we can
		     * generate code for that node later and also backpatch
		     * this reference to it.
		     */
		    CVMCPUemitBranch(con, 0, CVMCPU_COND_LT);
#ifdef CVMCPU_HAS_DELAY_SLOT
                    pushLookupNode(stack, &tos, low, index - 1, index,
                                   CVMJITcbufGetLogicalPC(con) -
				    2 * CVMCPU_INSTRUCTION_SIZE);
#else
                    pushLookupNode(stack, &tos, low, index - 1, index,
                                   CVMJITcbufGetLogicalPC(con) -
                                    1 * CVMCPU_INSTRUCTION_SIZE);
#endif
		}
		if (useCmp) {
		    /* WARNING: don't let regman know about constant */
		    if (CVMCPUalurhsIsEncodableAsImmediate(
                            CVMCPU_CMP_OPCODE, matchValue)) {
			CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
			    CVMCPU_COND_EQ, keyreg, matchValue);
		    } else {
			CVMCPUemitLoadConstant(con, scratchreg, matchValue);
			CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, scratchreg);
		    }
		}
		/* If we found a match, then branch to its handler. */
		branchToBlock(con, CVMCPU_COND_EQ,
			      ls->lookupList[index].dest);
		/* Setup for node to handle the next higher range. */
#ifdef CVMCPU_HAS_ALU_SETCC
		prevIndex = index;
#endif
		low = index + 1;
	    } else {                     /* (3) low == index == high */
		CVMLookupSwitchStackItem* item;
		if (useCmp) {
		    /* WARNING: don't let regman know about constant */
		    if (CVMCPUalurhsIsEncodableAsImmediate(
                            CVMCPU_CMP_OPCODE, matchValue)) {
			CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, matchValue);
		    } else {
			CVMCPUemitLoadConstant(con, scratchreg, matchValue);
			CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, scratchreg);
		    }
		}
		
		/* If EQ we have a match */
		branchToBlock(con, CVMCPU_COND_EQ, ls->lookupList[index].dest);

                /* Else we have a lookup failure and need to branch to the
                   default target. Note we are only allowed one backward branch
                   to the default target in this lookupswitch, due to how
                   gcCheckPcsSize is computed by the front end. Therefore
                   if we've already generated a backward branch to the default,
                   target we'll just branch to this branch instead.
                */
                if (branchToDefaultPC == -1) {
                    if (CVMJITirblockIsBackwardBranchTarget(ls->defaultTarget))
                    {
                        branchToDefaultPC = CVMJITcbufGetLogicalPC(con);
                    }
                    branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
                } else {
                    CVMJITaddCodegenComment((con,
                        "branch to default target branch"));
		    CVMCPUemitBranch(con, branchToDefaultPC, CVMCPU_COND_AL);
                }
		
		/* See if there are still some more nodes to generate. */
		item = popLookupNode(stack, &tos);
		if (item == NULL) {
		    break;
		}
		
		/* Setup for the handling of the popped node */
		low = item->low;
		high = item->high;
#ifdef CVMCPU_HAS_ALU_SETCC
		prevIndex = item->prevIndex;
#endif
		
		/* backpatch the branch in the node that created this node. */
		CVMJITfixupAddress(con,
				   item->logicalAddress,
				   CVMJITcbufGetLogicalPC(con),
				   CVMJIT_COND_BRANCH_ADDRESS_MODE);
	    }
	    
	    /* setup index and prevMatchValue for the next node */
	    index = (low + high) / 2;
#ifdef CVMCPU_HAS_ALU_SETCC
	    prevMatchValue = ls->lookupList[prevIndex].matchValue;
#endif
	}
	con->inConditionalCode = CVM_FALSE;
	
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
 done:;
        CVMJITcsClearEmitInPlace(con);
    };

%{

static void
emitBoundsCheck(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
		CVMBool isConstIndex, CVMInt32 constIndex)
{
    CVMRMResource* arrayLength = popResource(con);
    CVMRMResource* arrayIndex = NULL;

    CVMRMpinResource(CVMRM_INT_REGS(con), arrayLength,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    /* compare arrayLength to the index */
    if (isConstIndex) {
	/* index is the constant passed in */
	CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS,
				  CVMRMgetRegisterNumber(arrayLength),
				  constIndex);
    } else {
	/* array index is the resource on the stack */
	arrayIndex = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), arrayIndex,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS,
				  CVMRMgetRegisterNumber(arrayLength),
				  CVMRMgetRegisterNumber(arrayIndex));
    }
    
    /* trap if array bounds check failed */
    CVMRMsynchronizeJavaLocals(con);
    CVMJITaddCodegenComment((con,trapCheckComments[CVMJITIR_ARRAY_INDEX_OOB]));


/* IAI - 12 */
#if defined(IAI_CS_EXCEPTION_ENHANCEMENT) && !defined(IAI_CS_EXCEPTION_ENHANCEMENT2) 
    CVMJITcsSetExceptionInstruction(con);
#endif
/* IAI - 12 */


#ifdef IAI_CS_EXCEPTION_ENHANCEMENT2
    CVMJITcsSetArrayIndexOutofBoundsBranch(con);
#endif

    CVMCPUemitAbsoluteCallConditional(con,
        (void*)CVMCCMruntimeThrowArrayIndexOutOfBoundsExceptionGlue,
        CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_LS);

/* IAI - 12 */
#ifndef IAI_CS_EXCEPTION_ENHANCEMENT
    CVMJITcsBeginBlock(con);
#endif
/* IAI - 12 */

    /* push the arrayIndex resource on the stack */
    if (isConstIndex) {
        pushIConst32(con, constIndex);
    } else {
	pushResource(con, arrayIndex); 
	CVMRMunpinResource(CVMRM_INT_REGS(con), arrayIndex);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrayLength);
}
%}

// Purpose: BOUNDS_CHECK(index, arrayLength)
reg32: BOUNDS_CHECK reg32 reg32 : 20: : : : {
        emitBoundsCheck(con, $$, CVM_FALSE, 0);
    };

// Purpose: BOUNDS_CHECK(index, arrayLength)
iconst32Index: BOUNDS_CHECK ICONST_32 reg32 : 20: : : : {
        CVMInt32 arrayIndex =
	    CVMJITirnodeGetConstant32(CVMJITirnodeGetLeftSubtree($$))->j.i;
        emitBoundsCheck(con, $$, CVM_TRUE, arrayIndex);
    };

iconst32Index: ICONST_32 : 0 : : : :
        pushIConst32(con, CVMJITirnodeGetConstant32($$)->j.i);

root:	JSR : 10 : : : : {
        CVMJITIRBlock* target = CVMJITirnodeGetBranchOp($$)->branchLabel;
	CVMRMsynchronizeJavaLocals(con);

	/* We need a major spill because unlike goto, we will be returning. */
	CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);

	CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
	jsrToBlock(con, target);
	CVMRMunpinAllIncomingLocals(con, target);

	/* We no longer do checkGC() here, because we don't have the proper
           local state for RET at this point. checkGC() is now done at 
           the beginning of JSR return targets. */
    };

reg32:  JSR_RETURN_ADDRESS : 0 : : : : {
	CVMRMResource* dest;

	/* Get a resource to put the JSR return address in. */
	dest = CVMRMgetResourceStrict(CVMRM_INT_REGS(con),  
	    CVMCPU_JSR_RETURN_ADDRESS_SET & CVMRM_ANY_SET,
	    ~(CVMCPU_JSR_RETURN_ADDRESS_SET & CVMRM_ANY_SET), 1);
	/* force the return address into dest */
        CVMCPUemitLoadReturnAddress(con, CVMRMgetRegisterNumber(dest));
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

root:   RET reg32 : 10 : : : : {
	CVMRMResource* src = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMCPUemitRegisterBranch(con, CVMRMgetRegisterNumber(src));
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	CVMJITcsBeginBlock(con);
	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
    };

root:	GOTO	: 10 : : : : {
        CVMJITIRBlock* target = CVMJITirnodeGetBranchOp($$)->branchLabel;

	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);

	branchToBlock(con, CVMCPU_COND_AL, target);

	CVMRMunpinAllIncomingLocals(con, target);
	CVMJITcsBeginBlock(con);

	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
    };

%{
void
emitReturn(CVMJITCompilationContext* con, CVMJITRMContext* rp, int size) {
        void* helper;
        CVMRMResource *src;
	if (size == 0) {
	    src = NULL;
	} else {
	    src  = popResource(con);
	    CVMassert(size == src->size);
	    CVMRMpinResource(rp, src,
			     rp->anySet, CVMRM_EMPTY_SET);
	    CVMRMstoreReturnValue(rp, src);
	}
	CVMCPUemitPopFrame(con, size);
	if (CVMmbIs(con->mb, SYNCHRONIZED)) {
	    CVMJITaddCodegenComment((con, "goto CVMCCMreturnFromSyncMethod"));
	    CVMJITsetSymbolName((con, "CVMCCMreturnFromSyncMethod"));
	    helper = (void*)CVMCCMreturnFromSyncMethod;
	} else {
	    CVMJITaddCodegenComment((con, "goto CVMCCMreturnFromMethod"));
	    CVMJITsetSymbolName((con, "CVMCCMreturnFromMethod"));
	    helper = (void*)CVMCCMreturnFromMethod;
	}
        /* Emit the one-way ticket home: */
        CVMCPUemitReturn(con, helper);
	CVMJITresetSymbolName(con);
	if (src != NULL) {
	    CVMRMrelinquishResource(rp, src);
	}
	CVMJITcsBeginBlock(con);
	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
}
%}

root:   RETURN	: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, NULL, 0);
    };

root:	IRETURN reg32: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, CVMRM_INT_REGS(con), 1);
    };

root:   LRETURN reg64: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, CVMRM_INT_REGS(con), 2);
    };

root:	ATHROW reg32: 10 : : SET_TARGET1($$, ARG3); : : {
	CVMRMResource* src  = popResource(con);
	CVMRMsynchronizeJavaLocals(con);
        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);
	/* A "major" spill is not needed here.  We are leaving,
	  so we only care about the locals */
#if 0
	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);
#endif
        CVMJITaddCodegenComment((con, "goto CVMCCMruntimeThrowObjectGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeThrowObjectGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeThrowObject);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeThrowObjectGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
        CVMJITcsBeginBlock(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

%{

/*
 * instanceof and checkcast generate very similar code sequences:
 * though they call different helper functions, in other respects they
 * differ by only one instruction.
 */
static void
instanceTest(
    CVMJITCompilationContext*	con,
    CVMJITIRNodePtr             thisNode,
    CVMBool                     isInstanceOf)
{
    CVMRMResource *cbRes = popResource(con);
    CVMRMResource *src = popResource(con);
    int srcReg;
    void *helperFunction;
#ifdef CVM_TRACE_JIT
    char *helperFunctionName;
#endif
#ifdef IAI_CACHEDCONSTANT_INLINING
    CVMInt32 branchPC;
    CVMInt32 targetPC;
#endif

    if (isInstanceOf){
        CVMJITprintCodegenComment(("Do instanceof:"));
        helperFunction = (void*)CVMCCMruntimeInstanceOfGlue;
#ifdef CVM_TRACE_JIT
        helperFunctionName = "CVMCCMruntimeInstanceOfGlue";
#endif
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeInstanceOf);
    } else {
        CVMJITprintCodegenComment(("Do checkcast:"));
        helperFunction = (void*)CVMCCMruntimeCheckCastGlue;
#ifdef CVM_TRACE_JIT
        helperFunctionName = "CVMCCMruntimeCheckCastGlue";
#endif
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeCheckCast);
    }

    CVMJITprintCodegenComment(("Note: ARG3=cb to check against"));

    /* Make sure we flush dirty locals if we might exit without
       a "major" spill */
    CVMRMsynchronizeJavaLocals(con);

    cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
				     CVMCPU_ARG3_REG);
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), src,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);
    srcReg = CVMRMgetRegisterNumber(src);

    /* A "major" spill is not needed here.  The helpers return
       without becoming gc-safe except when an exception is thrown,
       and in that case, we only care about the locals which are always
       flushed to memory anyway. */
    CVMRMminorSpill(con, ARG3);

    /* do shortcut tests: null can cast to anything, but
     * isn't instanceof anything
     * We can do the test and set ARG1 to 0 in one instruction.
     */
    CVMJITaddCodegenComment((con, "set cc to \"eq\" if object is null"));
#ifdef CVMCPU_HAS_ALU_SETCC
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, CVMCPU_ARG1_REG, srcReg,
			   CVMJIT_SETCC);
#else
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, CVMCPU_ARG1_REG, srcReg,
			   CVMJIT_NOSETCC);
    /* NOTE: normally we would emit a compare instruction here after
     * having called an ALU emitter with CVMJIT_SETCC. Howver, the setcc is
     * for the benefit of the assembler glue, so platforms without SETCC
     * support will just need to do the compare in the glue code.
     */
#endif

    CVMJITcsSetEmitInPlace(con);
#ifdef IAI_CACHEDCONSTANT
    /* Add cached constant into constant entry list and also
     * generate code to load the address of the constant into ARG4 */
    CVMJITsetSymbolName((con, "%s cachedCb at offset %d",
			 (isInstanceOf ? "instanceof" : "checkcast"),
			 CVMJITcbufGetLogicalPC(con)));
    CVMJITgetRuntimeCachedConstant32(con);
#endif /* IAI_CACHEDCONSTANT */

#ifdef IAI_CACHEDCONSTANT_INLINING	
    /*
     * Inline quick tests normally found in checkcastglue and instanceofglue.
     */

    /* Generate branch around inline check if object is NULL. This will
     * be patched below when we know the address of the target. */
    branchPC = CVMJITcbufGetLogicalPC(con);
    targetPC = branchPC;
    CVMJITaddCodegenComment((con, 
	(isInstanceOf ? "beq .instanceofDone" : "beq .checkcastDone")));
    CVMCPUemitBranch(con, targetPC, CVMCPU_COND_EQ);

    /* ldr ARG2, [ARG1]  @ a2 = object.cb */
    CVMJITaddCodegenComment((con, "a2 = object.cb"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
	CVMCPU_ARG2_REG, CVMCPU_ARG1_REG, 0);

    /* ldr ARG1, [ARG4] @ load the guess cb */
    CVMJITaddCodegenComment((con, "load the guess cb"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
	CVMCPU_ARG1_REG, CVMCPU_ARG4_REG, 0);

    /* bic ARG2, ARG2, #3 @ mask off low bits of object cb*/
    CVMJITaddCodegenComment((con, "mask off low bits of object cb"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
	CVMCPU_ARG2_REG, CVMCPU_ARG2_REG, 0x3, CVMJIT_NOSETCC);

    /* cmp ARG1, ARG2 @ see if guess is correct */
    CVMJITaddCodegenComment((con, "see if guess is correct"));
    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, 
    	CVMCPU_COND_NE, CVMCPU_ARG1_REG, CVMCPU_ARG2_REG);

    if (isInstanceOf) {
	/* mov RESULT1, #1 */
	CVMJITaddCodegenComment((con, "assume guess is correct"));
	CVMCPUemitLoadConstant(con, CVMCPU_RESULT1_REG, 1);
    }

    /* call the helper glue if cachedCB is not equal to objectCB */
    CVMJITaddCodegenComment((con, "call %s if guess not correct",
			     helperFunctionName));
    CVMJITsetSymbolName((con, helperFunctionName));
    CVMCPUemitAbsoluteCallConditional(con, helperFunction,
                        CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, CVMCPU_COND_NE);
#else /* !IAI_CACHEDCONSTANT_INLINING */
    /* call the helper glue with the cc set to "eq" if the object is null */
    CVMJITaddCodegenComment((con, "call %s", helperFunctionName));
    CVMJITsetSymbolName((con, helperFunctionName));
    CVMCPUemitAbsoluteCall(con, helperFunction, 
			   CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
#endif /* !IAI_CACHEDCONSTANT_INLINING */

#ifndef IAI_CACHEDCONSTANT
    /* reserve a word for the guessCB */
    CVMJITaddCodegenComment((con, "guessCB i.e. last successful cast CB"));
    CVMJITemitWord(con, 0);
#endif /* IAI_CACHEDCONSTANT */

    CVMtraceJITCodegen(("\t\t.%s\n",
			isInstanceOf ? "instanceofDone:" : "checkcastDone:"));

#ifdef IAI_CACHEDCONSTANT_INLINING
    /* patch branch around inline check when object is null */
    targetPC = CVMJITcbufGetLogicalPC(con);
    CVMJITcbufPushFixup(con, branchPC);
    CVMJITaddCodegenComment((con, 
	(isInstanceOf ? "beq .instanceofDone" : "beq .checkcastDone")));
    CVMCPUemitBranch(con, targetPC, CVMCPU_COND_EQ);
    CVMJITcbufPop(con);
#endif

    CVMJITcaptureStackmap(con, 0);
    CVMJITcsClearEmitInPlace(con);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);

    if (!isInstanceOf) {
        /* For checkcast, return the original object:*/
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, thisNode);
        pushResource(con, src);
    } else {
        CVMRMResource *dest;
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
        pushResource(con, dest);
    }
    CVMJITcsBeginBlock(con);
}

%}

// Purpose: CHECKCAST(object, classBlock)
reg32: CHECKCAST reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2_1($$, ARG3); : : {
        instanceTest(con, $$, CVM_FALSE);
    };

// Purpose: value32 = INSTANCEOF(object, classBlock)
reg32: INSTANCEOF reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2_1($$, ARG3); : : {
        instanceTest(con, $$, CVM_TRUE);
    };

// Purpose: MONITOR_ENTER(object)
root: MONITOR_ENTER reg32 : 90 : : SET_TARGET1($$, ARG3); : : {
	CVMRMResource* src  = popResource(con);

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);
	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMonitorEnterGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMonitorEnterGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMonitorEnter);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMonitorEnterGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

// Purpose: MONITOR_EXIT(object)
effect: MONITOR_EXIT reg32 : 90 : : SET_TARGET1($$, ARG3); : : {
	CVMRMResource* src  = popResource(con);
        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMonitorExitGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMonitorExitGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMonitorExit);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMonitorExitGlue, 
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH);
        CVMJITcsBeginBlock(con);
	CVMJITcaptureStackmap(con, 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

%{

static CVMRMResource*
resolveConstant(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
		CVMRMregset target, CVMRMregset avoid)
{
    /*
     * For most cases:
     *          @ do a major spill first
     *
     * startpc:
     *          ldr     Rx, cachedConstant(PC)
     *          ldr     ARG3, =cpIndex    @ or: mov ARG3, =cpIndex.
     *          bl      =helper
     * cachedConstant:
     *          .word   -1
     * resolveReturn1:
     *          ldr     Rx, cachedConstant(PC)
     * resolveReturn2:
     *          @ capture stackmap here
     *
     * The helper will store the resolved constant at cachedConstant,
     * which it locates by the value in the link register.
     *
     * If static initialization of a class is needed, the helper will
     * do the initialization in a non-recursive way via the
     * interpreter loop, which will resume execution at resolveReturn1
     * when initialization is complete.
     *
     * If static initialization is currently being done by the current
     * thread, then the helper just returns immediately to
     * resolveReturn1.
     *
     * If static initialization is not needed, then the *second*
     * instruction of the above is patched to be the following:
     *
     *          b resolveReturn2
     *
     * NOTE: It's possible to do the above without the first ldr. However
     * there are two advantages to doing it this. The first is that by
     * doing the ldr and then the branch, you avoid a processor stall
     * if the first instruction after resolveReturn2 references the
     * cachedConstant, which it normally does. The 2nd is that we can't
     * safely patch the first instruction generated, because it might
     * be the first instruction of a basic block, and therefor may get
     * patched to handle a gc-rendezvous.
     *
     * If the platform does not support PC relative load, the 
     * `ldr Rx, cachedConstant(PC)' instructions can be implemented as:
     *
     *          ldr      Rx, =physicalPC + offsetToCachecConstantPC 
     *          ldr      Rx, [Rx, #0] 
     *
     *	For vtbl offset resolution, the pattern is a little different:
     *          @ do a major spill first
     *
     * startpc:
     *          ldr     Rx, cachedConstant(PC)
     *          ldr     ARG3, =cpIndex    @ or: mov ARG3, =cpIndex.
     *          bl      =helper
     * resolveReturn0:
     *          mov	rdest, RESULT1
     *		b	haveMB
     * cachedConstant:
     *          .word   -1
     * resolveReturn1:
     *          ldr     Rx, cachedConstant(PC)
     * resolveReturn2:
     *          @ capture stackmap here
     *
     * Since this sort of resolution cannot require running a static
     * initializer, we don't have any reason to consider it. However,
     * this resolution can yield an MB that does not have a vtable
     * offset. In this case, the helper will return the MB pointer directly
     * and return to resolveReturn0. In this case no patching will get
     * done and we'll continue to call the helper each time. Since both
     * the instructions at resolveReturn1 will need to be patched, we
     * push the address on the stack for patching in the FETCH_MB_FROM_VTABLE
     * rule. This special odd case never happens in real life, so
     * there is no real performance penalty here (just the code expansion).
     */

    CVMRMResource *dest;
    int destRegNo;
    int loadCachedConstantPC;
    int cachedConstantOffset;
    void *helper = NULL;
#ifdef CVM_TRACE_JIT
    char *helperName = NULL;
#endif
    CVMConstantPool* cp;
    CVMJITIRNode*    cpIndexNode;
    CVMUint16        cpIndex;
    CVMBool          needsCheckInit;
    CVMBool	     resolveVtblOffset = CVM_FALSE;
    CVMAddr	     returnMbPC = 0;

#undef setHelper

#ifdef CVM_TRACE_JIT
#define setHelper(x)				\
    helper = (void*)x##Glue;				\
    helperName = #x; 				\
    CVMJITstatsRecordInc(con, CVMJIT_STATS_##x)
#else
#define setHelper(x)				\
    helper = (void*)x##Glue;				\
    CVMJITstatsRecordInc(con, CVMJIT_STATS_##x)
#endif

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument currently on the stack */
    CVMSMadjustJSP(con);
#endif


    cp = CVMcbConstantPool(CVMmbClassBlock(con->mb));
    cpIndexNode = CVMJITirnodeGetLeftSubtree(thisNode);
    cpIndex = CVMJITirnodeGetConstant32(cpIndexNode)->j.i;
    needsCheckInit =
       ((CVMJITirnodeGetUnaryNodeFlag(thisNode) & CVMJITUNOP_CLASSINIT) != 0);

    if (needsCheckInit) {
        switch(CVMJITirnodeGetTag(cpIndexNode) >> CVMJIT_SHIFT_OPCODE) {
            case CVMJIT_CONST_NEW_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a class:"));
                setHelper(CVMCCMruntimeResolveNewClassBlockAndClinit);
                break;
            case CVMJIT_CONST_GETSTATIC_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static field:"));
                setHelper(CVMCCMruntimeResolveGetstaticFieldBlockAndClinit);
                break;
            case CVMJIT_CONST_PUTSTATIC_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static field:"));
                setHelper(CVMCCMruntimeResolvePutstaticFieldBlockAndClinit);
                break;
            case CVMJIT_CONST_STATIC_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static method:"));
                setHelper(CVMCCMruntimeResolveStaticMethodBlockAndClinit);
                break;
            default:
                CVMassert(CVM_FALSE); /* Unexpected irnode tag. */
        }
    } else {
        switch(CVMJITirnodeGetTag(cpIndexNode) >> CVMJIT_SHIFT_OPCODE) {
            case CVMJIT_CONST_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a class:"));
                setHelper(CVMCCMruntimeResolveClassBlock);
                break;
            case CVMJIT_CONST_ARRAY_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an array class:"));
                setHelper(CVMCCMruntimeResolveArrayClassBlock);
                break;
            case CVMJIT_CONST_GETFIELD_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an instance field:"));
                setHelper(CVMCCMruntimeResolveGetfieldFieldOffset);
                break;
            case CVMJIT_CONST_PUTFIELD_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an instance field:"));
                setHelper(CVMCCMruntimeResolvePutfieldFieldOffset);
                break;
            case CVMJIT_CONST_SPECIAL_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a 'special' method:"));
                setHelper(CVMCCMruntimeResolveSpecialMethodBlock);
                break;
            case CVMJIT_CONST_INTERFACE_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an 'interface' method:"));
                setHelper(CVMCCMruntimeResolveMethodBlock);
                break;
            case CVMJIT_CONST_METHOD_TABLE_INDEX_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a method table index:"));
                setHelper(CVMCCMruntimeResolveMethodTableOffset);
		resolveVtblOffset = CVM_TRUE;
                break;
            default:
                CVMassert(CVM_FALSE); /* Unexpected irnode tag. */
        }
    }

    /* Note: It is important to spill and evict every register because we may
       not return from the helper via a normal route.  It's almost like an
       exception being thrown except that we may return to this method and
       expect its state to be preserved just as if we haven't done the
       resolution yet. */
    CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);
    CVMJITcsSetEmitInPlace(con);
    loadCachedConstantPC = CVMJITcbufGetLogicalPC(con);

    /*
     * Load the cached constant. We don't know exactly where it will be
     * yet, so we'll need to come back and patch this instruction later.
     */
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
    destRegNo = CVMRMgetRegisterNumber(dest);
    CVMJITaddCodegenComment((con, "load cachedConstant"));
    CVMCPUemitMemoryReferencePCRelative(con, CVMCPU_LDR32_OPCODE,
					destRegNo, 0);

    /* 
     * Set ARG3 = cpIndex. This instruction eventually gets patched to branch
     * around the call to the helper once class initialization is done.
     */
    CVMJITaddCodegenComment((con, "ARG3 = cpIndex"));
    CVMJITsetSymbolName((con, "cpIndex"));
    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, (CVMInt32)cpIndex);

    /*
     * Do the helper call. Note that we can't allow the contant pool to be
     * dumped or else the helper glue won't be able to locate the instruction
     * to patch.
     */
    CVMJITaddCodegenComment((con, "call %s", helperName));
    CVMJITsetSymbolName((con, helperName));
    CVMCPUemitAbsoluteCall(con, helper, CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);

    if (resolveVtblOffset){
	returnMbPC = CVMJITcbufGetLogicalPC(con);
	pushAddress(con, returnMbPC);
	/* these instructions to get patched later */
	CVMJITaddCodegenComment((con, "return MB directly"));
        CVMJITcsSetEmitInPlace(con);
	CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, CVMCPU_RESULT1_REG,
			       CVMCPU_RESULT1_REG, CVMJIT_NOSETCC);
	CVMCPUemitBranch(con, 0, CVMCPU_COND_AL);
        CVMJITcsClearEmitInPlace(con);
    }

    /* fixup earlier load now that we know the pc of the cachedConstant */
    cachedConstantOffset = CVMJITcbufGetLogicalPC(con) - loadCachedConstantPC;
    CVMJITcbufPushFixup(con, loadCachedConstantPC);
    CVMJITcsSetEmitInPlace(con);
    CVMJITaddCodegenComment((con, "load cachedConstant"));
    CVMCPUemitMemoryReferencePCRelative(con, CVMCPU_LDR32_OPCODE,
					destRegNo, cachedConstantOffset);
    CVMJITcsClearEmitInPlace(con);
    CVMJITcbufPop(con);

    /* emit the cachedConstant word */
    CVMJITaddCodegenComment((con, "cachedConstant"));
    CVMJITemitWord(con, -1);

    CVMJITcaptureStackmap(con, 0); /* This is the return PC in the frame. */

    /*
     * Load the cached constant, which is in the word before the
     * current instruction.
     */
    CVMJITaddCodegenComment((con, "load cachedConstant"));
    CVMCPUemitMemoryReferencePCRelative(con, CVMCPU_LDR32_OPCODE, destRegNo,
					-(CVMInt32)sizeof(CVMUint32));
    CVMJITcsClearEmitInPlace(con);
    CVMJITcsBeginBlock(con);

    /* Occupy the target register with the resolved value: */
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    return dest;
}

static void
doCheckInit(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode)
{
    /*
     *          @ do a major spill first
     *
     *          ldr     ARG3, =cb      @ load cb to intialize
     *          bl      =helper        @ call helper to intialize cb
     *          @ capture stackmap here
     *
     *          @ If the class is aready initialized, then the helper will
     *          @ patch the first instruction above, and we end up with the 
     *          @ following:
     *
     *          b       checkinitDone
     *          bl      =helper        @ call helper to intialize cb
     * checkinitDone:
     */

    CVMClassBlock *cb =
        CVMJITirnodeGetConstantAddr(CVMJITirnodeGetLeftSubtree(thisNode))->cb;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument currently on the stack */
    CVMSMadjustJSP(con);
#endif

    /* Note: It is important to spill and evict every register because we may
       not return from the helper via a normal route.  It's almost like an
       exception being thrown except that we may return to this method and
       expect its state to be preserved just as if we haven't done the
       checkinit yet. */
    CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);

    CVMJITprintCodegenComment(("Do checkinit:"));

    /*
     * Load the cb into an argument register for the helper. This instruction
     * eventually gets patched to branch around the call to the helper once
     * the class has been intialized.
     */
    CVMJITaddCodegenComment((con, "ARG3 = %C", cb));
    CVMJITsetSymbolName((con, "%C", cb));
    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, (CVMUint32)cb);

    /* 
     * Do the helper call. Note that we can't allow the contant pool to be
     * dumped or else the helper glue won't be able to locate the instruction
     * to patch.
     */
    CVMJITaddCodegenComment((con,
			     "call CVMCCMruntimeRunClassInitializerGlue"));
    CVMJITsetSymbolName((con, "CVMCCMruntimeRunClassInitializerGlue"));
    CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeRunClassInitializer);
    CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeRunClassInitializerGlue,
                           CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);
    CVMJITcsBeginBlock(con);
    CVMJITcaptureStackmap(con, 0); /* This is the return PC in the frame. */

    CVMJITprintCodegenComment(("checkinitDone:"));
}

%}

reg32: RESOLVE_REF : 90 : SET_AVOID_C_CALL($$); : : : {
        pushResource(con, resolveConstant(con, $$, GET_REGISTER_GOALS));
    };

// a register containing a vtable offset
// it is dynamically resolved.
voffMemSpec: RESOLVE_REF : 90 : SET_AVOID_C_CALL($$); : : : {
        CVMRMResource *operand = resolveConstant(con, $$, GET_REGISTER_GOALS);
        pushMemSpecRegister(con, CVM_TRUE, operand);
    };

// a fixed vtable offset. It is already resolved
// by compilation time so of course we can mark it as
// not needing patching.
voffMemSpec: ICONST_32 : 0 : : : : {
	pushAddress(con, 0);
        pushMemSpecImmediate(con, CVMJITirnodeGetConstant32($$)->j.i);
    };


// Purpose: CHECKINIT(cb, valueNode)
// Returns: valueNode after the cb has been checkinited.
reg32: CHECKINIT CLASS_BLOCK reg32 : 90 : SET_AVOID_C_CALL($$); : : : {
        doCheckInit(con, $$);
    };

effect: MAP_PC : 0 : : : : {
	CVMCompiledPcMapTable* pcMapTable = con->pcMapTable;
        CVMUint16 javaPc = CVMJITirnodeGetTypeNode($$)->mapPcNode.javaPcToMap;
	CVMUint16 compiledPc = CVMJITcbufGetLogicalPC(con);

	CVMCompiledPcMap* pcMap = 
	   &(pcMapTable->maps[con->mapPcNodeCount++]);

	CVMassert(con->mapPcNodeCount <= pcMapTable->numEntries);

        if (compiledPc == con->currentCompilationBlock->logicalAddress) {
#ifdef CVM_JIT_REGISTER_LOCALS
	    /*
	     * If this is the MAP_PC node at the start of a basic block that
	     * is also a backwards branch target, then we need to set the
	     * compiledPc to the real start of the block that will load
	     * incoming locals. This is necessary for OSR to work.
	     */
	    if (con->currentCompilationBlock->loadLocalsLogicalAddress != 0) {
                compiledPc =
		    con->currentCompilationBlock->loadLocalsLogicalAddress;
	    }
#endif /* CVM_JIT_REGISTER_LOCALS */

            if (CVMJITirblockIsBackwardBranchTarget(
                            con->currentCompilationBlock))
            {
                CVMemitThreadSchedHook(con);
            }

        }

	pcMap->javaPc = javaPc;
	pcMap->compiledPc = compiledPc;

	CVMJITprintCodegenComment((
            "MAP_PC idepth=%d javaPc=%d compiledPc=%d",
	    con->inliningDepth, javaPc, compiledPc));
    };

%{
#ifdef CVMJIT_COUNT_VIRTUAL_INLINE_STATS
 /* Count and report the hits and misses for each invocation of an
    inlined virtual invoke.
 */
typedef struct CVMMethodInfoStat CVMMethodInfoStat;
struct CVMMethodInfoStat {
    CVMMethodInfoStat *next;
    CVMMethodBlock *mb;
    CVMUint32 hit;
    CVMUint32 miss;
};

CVMMethodInfoStat *virtualInlineHitMissList = NULL;

static CVMMethodInfoStat*
findMethodInfo(CVMMethodBlock *mb)
{
    CVMMethodInfoStat *info = virtualInlineHitMissList;
    while (info != NULL) {
        if (info->mb == mb) {
            return info;
        }
        info = info->next;
    }
    return NULL;
}

static void
insertMethodInfo(CVMMethodBlock *mb, CVMBool isHit)
{
    CVMMethodInfoStat *info = calloc(1, sizeof(CVMMethodInfoStat));
    info->mb = mb;
    if (isHit) {
        info->hit++;
    } else {
        info->miss++;
    }
    info->next = virtualInlineHitMissList;
    virtualInlineHitMissList = info;
}


static void
reportVirtualInlineHitMiss(CVMMethodBlock *mb, CVMBool isHit)
{
    CVMExecEnv *ee = CVMgetEE();
    CVMMethodInfoStat *info;

    /*
    CVMconsolePrintf("Virt/Interf INLINE: %s %C.%M\n",
                     (isHit ? "Hit " : "Miss"),
                     CVMmbClassBlock(mb), mb);
    */
    CVMsysMicroLock(ee, CVM_CODE_MICROLOCK);
    info = findMethodInfo(mb);
    if (info != NULL) {
        if (isHit) {
            info->hit++;
        } else {
            info->miss++;
        }
    } else {
        insertMethodInfo(mb, isHit);
    }
    CVMsysMicroUnlock(ee, CVM_CODE_MICROLOCK);
}

void CVMJITprintVirtualInlineHitMiss()
{
    CVMExecEnv *ee = CVMgetEE();
    CVMMethodInfoStat *info;

    CVMsysMicroLock(ee, CVM_CODE_MICROLOCK);
    info = virtualInlineHitMissList;
    CVMconsolePrintf("Virtual/Interface Inline Hit Miss stats:\n");
    while (info != NULL) {
	CVMUint32 hit = info->hit;
        CVMUint32 miss = info->miss;
        CVMUint32 total = hit + miss;
        CVMconsolePrintf("   h %d (%2.2f%%): m %d (%2.2f%%): %C.%M\n",
			hit, (float)hit/(float)total * 100.0,
			miss, (float)miss/(float)total * 100.0,
 			CVMmbClassBlock(info->mb), info->mb);
        info = info->next;
    }
    CVMconsolePrintf("END of stats\n");
    CVMsysMicroUnlock(ee, CVM_CODE_MICROLOCK);
}

#endif /* CVMJIT_COUNT_VIRTUAL_INLINE_STATS */
%}

root: OUTOFLINEINVOKE : 0 : : : : {
    printInliningInfo($$, "Out-of-line invocation");
#ifdef CVMJIT_COUNT_VIRTUAL_INLINE_STATS
    {
        CVMJITIRNode*    mbNode;
        CVMInt32 constant;
        CVMMethodBlock *mb;
        CVMBool isHit;
        CVMRMResource *mbRes;
        CVMRMResource *isHitRes;

        mbNode = CVMJITirnodeGetLeftSubtree($$);
        constant = CVMJITirnodeGetConstantAddr(mbNode)->vAddr;
        mb = (CVMMethodBlock *)(constant & ~0x1);
        isHit = (CVMBool)(constant & 0x1);

        CVMconsolePrintf("INLINING %s %C.%M in %C.%M\n",
			 isHit?"Hit ":"Miss", CVMmbClassBlock(mb), mb,
                         CVMmbClassBlock(con->mb), con->mb);

        mbRes =
	    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), (CVMInt32)mb);
        isHitRes =
	    CVMRMbindResourceForConstant32(CVMRM_INT_REGS(con), isHit);
        mbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					 mbRes, CVMCPU_ARG1_REG);
        isHitRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					    isHitRes, CVMCPU_ARG2_REG);

        /* Spill the outgoing registers if necessary: */
        CVMRMminorSpill(con, ARG1|ARG2);

        CVMCPUemitAbsoluteCall(con, (void*)reportVirtualInlineHitMiss,
                               CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH);

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbRes);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), isHitRes);
    }
#endif /* CVMJIT_COUNT_VIRTUAL_INLINE_STATS */
};

