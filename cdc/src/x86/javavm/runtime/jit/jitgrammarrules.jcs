//
//
// @(#)jitgrammarrules.jcs	1.14 06/10/13
//
// Portions Copyright  2000-2008 Sun Microsystems, Inc. All Rights
// Reserved.  Use is subject to license terms.
// DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER
// 
// This program is free software; you can redistribute it and/or
// modify it under the terms of the GNU General Public License version
// 2 only, as published by the Free Software Foundation.
// 
// This program is distributed in the hope that it will be useful, but
// WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
// General Public License version 2 for more details (a copy is
// included at /legal/license.txt).
// 
// You should have received a copy of the GNU General Public License
// version 2 along with this work; if not, write to the Free Software
// Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
// 02110-1301 USA
// 
// Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa
// Clara, CA 95054 or visit www.sun.com if you need additional
// information or have any questions.
//

//
// converting CVM IR subset to RISC assembler
//


%name CVMJITCompileExpression
%type CVMJITIRNodePtr
%goal root
%opcode CVMJITgetMassagedIROpcode
%left CVMJITirnodeGetLeftSubtree
%right CVMJITirnodeGetRightSubtree
%setstate IRRecordState
%getstate IRGetState

%leaf LOCAL32 //  Java 1-word local
%leaf LOCAL64 // Java 2-word local
%unary STATIC32 // Java 1-word static
%unary STATIC64 // Java 2-word static
%unary STATIC64VOL // Java 2-word volatile static

//
// this represents the first evaluation of a 1-word CSE
%unary IDENT32
// this represents the first evaluation of a 2-word CSE
%unary IDENT64

//
// the one word pointer that appears (in ARG1) on entry to a catch.
%leaf EXCEPTION_OBJECT
//
// make a new object. Has a class block beneath it.
%unary NEW_OBJECT

// make an array of obj. Has class block of element type, and a dimension.
%binary NEW_ARRAY_REF
%binary MULTI_NEW_ARRAY_REF

// make an array of a basic type. Has only a dimension.
%unary  NEW_ARRAY_BASIC

// This just forces evaluation of an expression.
// It is discarded.
//%unary FOR_EFFECT

// This just forces evaluation of an expression.
// It is used as a CSE, and
// forces ordering of expression evaluation in funky cases.
//%unary FOR_TEMP

//
// this roots an expression which is the
// definition of a value live across a branch.
//
%unary DEFINE_VALUE32
%unary DEFINE_VALUE64
// for loading all DEFINE values that need to be in registers
%leaf LOAD_PHIS
// for releasing the registers used by phi values
%leaf RELEASE_PHIS
// the values as they appear when used in an expression.
%leaf USED32
%leaf USED64

//
// we distinguish among a couple of constants.
// This actually requires some target-dependent re-writing of
// the IROpcode the first time we encounter the target
// independent integer-constant nodes. This is done by
// CVMJITgetMassagedIROpcode.
// These are mutually exclusive, since we have to classify a constant
// without context. Thus the classes for intersections. Also, of course,
// all mode 3 constants are mode 2 constants, (and we may wish to constrain
// mode 3 to be unsigned 8 bit values, in which case they'd also all be
// mode 1's)

%leaf 	ICONST_32	// all the others.
%leaf 	ICONST_64	// big numbers.
%leaf   STRING_ICELL_CONST // StringICell pointer from cp.

%leaf   METHOD_BLOCK	// from context, either a mb or something like it.
%leaf   METHOD_CONTEXT
%leaf	CLASS_BLOCK

%binary ASSIGN		// assignment

//
// the mechanism of Java method invocation.
//
%binary INVOKE32I	// return type is integer (32-bits)
%binary INVOKE64I       // return type is integer (64-bits)
%binary VINVOKE		// return type is void
%binary PARAMETER32
%binary PARAMETER64
%leaf	NULL_PARAMETER
%unary  GET_VTBL
%unary  GET_ITBL
%binary FETCH_MB_FROM_VTABLE
%binary FETCH_MB_FROM_ITABLE

// #ifdef IAI_VIRTUAL_INLINE_CB_TEST
%unary  FETCH_VCB
%binary FETCH_MB_FROM_VTABLE_OUTOFLINE
%binary MB_TEST_OUTOFLINE
// #endif

//
// memory accesses
//
%unary  FETCH32		// memory fetch from STATIC or INDEX or FIELDREF
%unary	FETCH64
%binary INDEX  		// an array index operation
%unary  ALENGTH		// array length, of type integer
%unary  NULLCHECK	// null checked object
%binary FIELDREFOBJ     // object ref field of object
%binary FIELDREF32	// one-word field of object
%binary FIELDREF64	// two-word field of object
%binary FIELDREF64VOL	// two-word volatile field of object

%binary ISEQUENCE_R
%binary LSEQUENCE_R
%binary VSEQUENCE_R
%binary ISEQUENCE_L
%binary LSEQUENCE_L
%binary VSEQUENCE_L

//
// operations on word-sized values
//
%unary  INEG32		// one-word integer unary -
%unary  NOT32           // one-word integer (x == 0)?1:0
%unary  INT2BIT32       // one-word integer (x != 0)?1:0

%binary IADD32		// one-word integer +
%binary ISUB32		// one-word integer binary -
%binary IMUL32		// one-word integer *
%binary IDIV32		// one-word integer /
%binary IREM32		// one-word integer %
%binary AND32		// one-word bitwise AND
%binary OR32		// one-word bitwise OR
%binary XOR32		// one-word bitwise XOR
%binary SLL32		// one-word <<
%binary SRL32		// one-word >>>
%binary SRA32		// one-word >>

//
// operations on long integer
//
%unary  INEG64		// integer unary -
%binary IADD64		// integer +
%binary ISUB64		// integer binary -
%binary IMUL64		// integer *
%binary IDIV64		// integer /
%binary IREM64		// integer %
%binary AND64		// bitwise AND
%binary OR64		// bitwise OR
%binary XOR64		// bitwise XOR
%binary SLL64		// <<
%binary SRL64		// >>>
%binary SRA64		// >>

%binary LCMP

//
// operations on floats
//
%unary  FNEG            // one-word float unary -
%binary FADD            // one-word float +
%binary FSUB            // one-word float binary -
%binary FMUL            // one-word float *
%binary FDIV            // one-word float /
%binary FREM            // one-word float %

%binary FCMPL
%binary FCMPG

//
// operations on doubles
//
%unary  DNEG            // two-word double unary -
%binary DADD            // two-word double +
%binary DSUB            // two-word double binary -
%binary DMUL            // two-word double *
%binary DDIV            // two-word double /
%binary DREM            // two-word double %

%binary DCMPL
%binary DCMPG

//
// some conversions
%unary I2B
%unary I2C
%unary I2S
%unary I2L
%unary I2F
%unary I2D
%unary L2I
%unary L2F
%unary L2D
%unary F2D
%unary F2I
%unary F2L
%unary D2F
%unary D2I
%unary D2L

//
// control operations
//
%unary TABLESWITCH
%unary LOOKUPSWITCH
%binary BCOND_INT	// conditional branch
%binary BCOND_LONG	// conditional branch
%binary BCOND_FLOAT	// conditional branch
%binary BCOND_DOUBLE	// conditional branch
%binary BOUNDS_CHECK    // array index out of bounds check
%leaf  GOTO
%leaf  RETURN


%leaf  JSR                 // a lot like goto, but is returned to
%leaf  JSR_RETURN_ADDRESS  // binds lr to the incomming jsr return address
%unary RET

// Some of the following return opcodes are NOT NEEDED because they
// can be handle by equivalents.  The mapping is done by
// CVMJITgetMassagedIROpcode() (see the case for CVMJIT_RETURN_VALUE).

%unary IRETURN
%unary LRETURN

//%unary FRETURN    // Mapped into IRETURN by CVMJITgetMassagedIROpcode().
//%unary DRETURN    // Mapped into LRETURN by CVMJITgetMassagedIROpcode().

%unary ATHROW
%binary CHECKCAST
%binary INSTANCEOF

//
// synchronization
//
%unary MONITOR_ENTER
%unary MONITOR_EXIT

//
// lazy resolution & checkinit
//
%leaf RESOLVE_REF
%binary CHECKINIT

//
// mapping java pc's to compiled pc's
//
%leaf MAP_PC

//
// Inlining info
//
%unary BEGININLINING
%leaf VENDINLINING
%leaf OUTOFLINEINVOKE

// Intrinsic nodes
%binary VINTRINSIC
%binary INTRINSIC32
%binary INTRINSIC64
%binary IARG
%leaf   NULL_IARG

//
// A root can be an embeddable effect
//
root: effect : 0 : : : : ;

// Purpose: LOCAL64 = value64.
//
// CVM_64 : this rule might be overridden by a machine specific one
//
root:	ASSIGN LOCAL64 reg64 : 20 : : : : {
	CVMRMResource * rhs = popResource(con);
	CVMJITLocal   * lhs = CVMJITirnodeGetLocal(
	    CVMJITirnodeGetLeftSubtree($$));
	CVMRMpinResource(CVMRM_INT_REGS(con), rhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMRMstoreJavaLocal(CVMRM_INT_REGS(con), rhs, 2, CVM_FALSE,
			    lhs->localNo);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    };

%{

static void
binaryHelper(
    CVMJITCompilationContext *con,
    void* helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero,
    int argSize, /* total size in words of all arguments */
    int resultSize);

/*
 * Inlining start
 */
static void
beginInlining(CVMJITCompilationContext* con, CVMJITIRNodePtr thisNode)
{
    CVMJITUnaryOp* unop = CVMJITirnodeGetUnaryOp(thisNode);
    CVMJITConstantAddr* constAddr;   
    CVMJITMethodContext* mc;
    CVMUint16 pcStart = CVMJITcbufGetLogicalPC(con);
    CVMJITInliningInfoStackEntry* newEntry;

    /* SVMC_JIT rr 2004-02-24: (CHECK_INIT cb value) -> (SEQUENCE (FOR_EFFECT (CHECK_INIT cb)) value)
     * see doCheckInitOnCB() for details
     *
     * SVMC_JIT sw 2004-02-27: changed assertion because IR optimization
     * can change the expression to eliminate the check:
     * (SEQUENCE (FOR_EFFECT (CONST_JAVA_NUMERIC_32) value))
     */
    /* FIXME. Not compilable. */
    /*CVMassert(CVMJITirnodeIsConstMC(unop->operand)
	      || (CVMJITirnodeIsSequenceR(unop->operand)
		  && CVMJITirnodeIsForEffect(
		         CVMJITirnodeGetLeftSubtree(unop->operand))
		  && (CVMJITirnodeIsConstant32Node(
                         CVMJITirnodeGetLeftSubtree(
			     CVMJITirnodeGetLeftSubtree(unop->operand)))
		      ||
		      CVMJITirnodeIsClassCheckInit(
                         CVMJITirnodeGetLeftSubtree(
			     CVMJITirnodeGetLeftSubtree(unop->operand))))));*/
    if (CVMJITirnodeIsConstMC(unop->operand)) {
	constAddr = CVMJITirnodeGetConstantAddr(unop->operand);
    } else {
	constAddr = CVMJITirnodeGetConstantAddr(CVMJITirnodeGetRightSubtree(unop->operand));
    }
    
    mc = constAddr->mc; 
    /* Start a new inlining entry */
    CVMassert(con->inliningDepth < con->maxInliningDepth);
    newEntry = &con->inliningStack[con->inliningDepth++];
    /* Record the starting point and mc of this inlining record */
    /* The end point will be recorded by the corresponding END_INLINING node */
    newEntry->pcOffset1 = pcStart;
    newEntry->mc = mc;

    CVMJITprintCodegenComment(("Begin inlining of %C.%M (start pc=%d):",
			       CVMmbClassBlock(mc->mb), mc->mb, pcStart));
}

/*
 * We have finished inlined code. Record this PC.
 */
static void
endInlining(CVMJITCompilationContext* con, CVMJITIRNodePtr thisNode)
{
    CVMJITInliningInfoStackEntry* topEntry;
    CVMCompiledInliningInfoEntry* recordedEntry;
    CVMUint16 pcEnd = CVMJITcbufGetLogicalPC(con);
    CVMJITMethodContext *mc;
    CVMUint16 count = CVMJITirnodeGetNull(thisNode)->data;

    CVMassert(count > 0);
    while (count-- > 0) {
        CVMassert(con->inliningDepth > 0);
        topEntry = &con->inliningStack[--con->inliningDepth];
        mc = topEntry->mc;
        CVMassert(topEntry->pcOffset1 > 0);
        CVMassert(topEntry->mc != NULL);

        CVMassert(con->inliningInfoIdx < con->numInliningInfoEntries);
        /* Now commit another "permanent" entry */
        recordedEntry = &con->inliningInfo->entries[con->inliningInfoIdx++];
        recordedEntry->pcOffset1 = topEntry->pcOffset1;
        recordedEntry->mb = mc->mb;
        recordedEntry->pcOffset2 = pcEnd;
        recordedEntry->invokePC = mc->invokePC;
        recordedEntry->flags = 0;

        {
  	    recordedEntry->firstLocal = mc->firstLocal;
	    if (CVMmbIs(mc->mb, SYNCHRONIZED)) {
	        CVMassert(!CVMmbIs(mc->mb, STATIC));
	        recordedEntry->syncObject = mc->syncObject;
	    } else {
               recordedEntry->syncObject = 0;
            }
        }

        CVMJITprintCodegenComment(("End inlining of %C.%M (end pc=%d):",
			           CVMmbClassBlock(recordedEntry->mb),
			           recordedEntry->mb,
			           pcEnd));
    }
}

#ifdef CVM_DEBUG
static void
printInliningInfo(CVMJITCompilationContext *con, CVMJITIRNode* thisNode, const char* what)
{
    CVMJITUnaryOp* unop = CVMJITirnodeGetUnaryOp(thisNode);

    if (CVMJITirnodeIsConstMB(unop->operand)) {
	CVMMethodBlock* mb;
	CVMJITConstantAddr* constAddr;
	constAddr = CVMJITirnodeGetConstantAddr(unop->operand);
	mb = constAddr->mb;
	CVMJITprintCodegenComment(("%s of %C.%M:", what,
				   CVMmbClassBlock(mb), mb));
    } else {
	CVMJITprintCodegenComment(("%s (node id %d):", what,
				   CVMJITirnodeGetID(unop->operand)));
    }
}
#else
#define printInliningInfo(con, thisNode, what)
#endif

/*
 * For nodes of the form:
 * NODE .... reg<width>
 * 
 * Pass the reg<width> value onto NODE
 */
static void
passLastEvaluated(CVMJITCompilationContext* con, CVMJITRMContext* rc, 
		  CVMJITIRNodePtr thisNode)
{
    /* Associate the last evaluated sub-node on to its parent */
    CVMRMResource* arg = popResource(con);
    CVMRMoccupyAndUnpinResource(rc, arg, thisNode);
    pushResource(con, arg);
}

/* Purpose: Converts a constant into reg32 value. */
static void
const2Reg32(CVMJITCompilationContext *con, CVMJITRMContext *rc,
	    CVMJITIRNodePtr thisNode)
{
    CVMInt32 constant = CVMJITirnodeGetConstant32(thisNode)->j.i;
    CVMRMResource *dest = CVMRMbindResourceForConstant32(rc, constant);
    /* Need this in case this constant is a CSE */
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

/* Purpose: Converts a constant into reg32 value. */
static void
const2RegAddr(CVMJITCompilationContext *con, CVMJITRMContext *rc,
	    CVMJITIRNodePtr thisNode)
{
    CVMAddr constant = CVMJITirnodeGetConstantAddr(thisNode)->vAddr;
    CVMRMResource *dest = CVMRMbindResourceForConstantAddr(rc, constant);
    /* Need this in case this constant is a CSE */
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

#ifdef CVMJIT_INTRINSICS

/* Get absolute value of srcReg and set condition codes:
     adds    rDest, rSrc, #0
     neglt   rDest, rSrc
*/
static void emitAbsolute(CVMJITCompilationContext* con,
			 int destReg, int srcReg)
{
    CVMCPUemitBinaryALU(con, CVMCPU_ADD_OPCODE,
        destReg, srcReg, CVMCPUALURhsTokenConstZero, CVMJIT_SETCC);
#ifndef CVMCPU_HAS_ALU_SETCC
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LT,
        srcReg, CVMCPUALURhsTokenConstZero);
#endif
    CVMCPUemitUnaryALUConditional(con, CVMCPU_NEG_OPCODE,
        destReg, srcReg, CVMJIT_NOSETCC, CVMCPU_COND_LT);
}

CVMJITRegsRequiredType
CVMJITRISCintrinsicDefaultGetRequired(CVMJITCompilationContext *con,
                                      CVMJITIRNode *intrinsicNode,
                                      CVMJITRegsRequiredType argsRequiredSet)
{
    return argsRequiredSet;
}

CVMRMregset
CVMJITRISCintrinsicDefaultGetArgTarget(CVMJITCompilationContext *con,
                                       int typeTag, CVMUint16 argNumber,
                                       CVMUint16 argWordIndex)
{
    return CVMRM_ANY_SET;
}

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
CVMJITRegsRequiredType
CVMJITRISCintrinsicSimpleLockReleaseGetRequired(
    CVMJITCompilationContext *con,
    CVMJITIRNode *intrinsicNode,
    CVMJITRegsRequiredType argsRequiredSet)
{
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS
    /* During the release, we may call CVMCCMruntimeSimpleSyncUnlock,
     * which requires ARG1 and ARG2
     */
    return argsRequiredSet | ARG1 | ARG2 | CVMCPU_AVOID_C_CALL;
#else
    return argsRequiredSet;
#endif
}


CVMRMregset
CVMJITRISCintrinsicSimpleLockReleaseGetArgTarget(
    CVMJITCompilationContext *con,
    int typeTag, CVMUint16 argNumber,
    CVMUint16 argWordIndex)
{
    CVMassert(argNumber == 0);
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS
    /* During the release, we may call CVMCCMruntimeSimpleSyncUnlock,
     * which requires "this" to be in ARG2
     */
    return ARG2;
#else
    return CVMRM_ANY_SET;
#endif
}
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */

/* intrinsic emitter for Thread.currentThread(). */
static void
java_lang_Thread_currentThread_EmitOperator(CVMJITCompilationContext *con,
                                            CVMJITIRNode *intrinsicNode)
{   
   struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    CVMRMResource* dest =
	CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    int destReg = CVMRMgetRegisterNumber(dest);
    int eeReg;

    /*
        ldr eeReg, [sp, #OFFSET_CVMCCExecEnv_ee]           @ Get ee.
        ldr rDest, [eeReg, #OFFSET_CVMExecEnv_threadICell] @ Get threadICell.
        ldr rDest, [rDest]                                 @ Get thread obj.
    */

#ifdef CVMCPU_EE_REG
    eeReg = CVMCPU_EE_REG;
#else
    eeReg = destReg;
    /* Get the ee from the ccee: */
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDRADDR_OPCODE,
        eeReg, offsetof(CVMCCExecEnv, eeX));
#endif
    /* Get the thread icell from the ee: */
    CVMJITaddCodegenComment((con, "destReg = ee->threadICell"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDRADDR_OPCODE,
        destReg, eeReg, offsetof(CVMExecEnv, threadICell));

    /* Get the thread object from the thread icell: */
    CVMJITaddCodegenComment((con, "destReg = *ee->threadICell"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDRADDR_OPCODE,
        destReg, destReg, 0);

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

static void
iabsEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    CVMRMResource* src = popResource(con);
    CVMRMResource* dest =
	CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), src, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    emitAbsolute(con, CVMRMgetRegisterNumber(dest),
                 CVMRMgetRegisterNumber(src));
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_MICROLOCK && \
    CVM_MICROLOCK_TYPE == CVM_MICROLOCK_SWAP_SPINLOCK

/*
 * Intrinsic emitter for spinlock microlock version of CVM.simpleLockGrab().
 *
 * Grabs CVMglobals.objGlobalMicroLock using atomic swap. If it fails,
 * returns FALSE. If successful, checks if the object is already locked.
 * If locked, releases CVMglobals.objGlobalMicroLock and returns FALSE.
 * Otherwise returns TRUE.
 */

static void
simpleLockGrabEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* dest;
    CVMRMResource* microLock;
    int objRegID, objHdrRegID, destRegID, microLockRegID;
    int fixupPC1, fixupPC2; /* To patch the conditional branches */

    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);

    dest = CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
    objHdr = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), obj, CVMRM_ANY_SET, CVMRM_SAFE_SET);

    objRegID    = CVMRMgetRegisterNumber(obj);
    objHdrRegID = CVMRMgetRegisterNumber(objHdr);
    destRegID   = CVMRMgetRegisterNumber(dest);

    /* load microlock address into microLockRegID */
    CVMJITsetSymbolName((con, "&CVMglobals.objGlobalMicroLock"));
    microLock = CVMRMgetResourceForConstant32(
        CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	(CVMUint32)&CVMglobals.objGlobalMicroLock);
    microLockRegID = CVMRMgetRegisterNumber(microLock);
    /* preload the address to help caching */
    CVMJITaddCodegenComment((con, "tmp = CVMglobals.objGlobalMicroLock"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       destRegID, microLockRegID, 0);
    /* Get microlock LOCKED flag */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_LOCKED"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_MICROLOCK_LOCKED);
    /* atomic swap LOCKED into the microlock */
    CVMJITaddCodegenComment((con,
	"swp(CVMglobals.objGlobalMicroLock, CVM_MICROLOCK_LOCKED)"));
    CVMCPUemitAtomicSwap(con, destRegID, microLockRegID);
    /* check if microlock is already locked */
    CVMJITaddCodegenComment((con, "check if microlock is locked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      destRegID, CVM_MICROLOCK_LOCKED);
    /* branch if microlock already locked */
    CVMJITaddCodegenComment((con, "br failed if microlock is locked"));
    fixupPC1 = CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* assume not locked and set intrinsic result */
    CVMJITaddCodegenComment((con, "assume not locked: result = true"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_TRUE);
    /* get sync bits from object header */
    CVMJITaddCodegenComment((con, "get obj sync bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
				objHdrRegID, objHdrRegID,
				CVM_SYNC_MASK, CVMJIT_NOSETCC);
    /* check if object is unlocked */
    CVMJITaddCodegenComment((con, "check if obj unlocked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      objHdrRegID, CVM_LOCKSTATE_UNLOCKED);
    /* branch if object not locked */
    CVMJITaddCodegenComment((con, "br done if object is not locked"));
    fixupPC2 = CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC2 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* Object is locked. Release microlock */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitLoadConstant(con, destRegID, CVM_MICROLOCK_UNLOCKED);
    CVMJITaddCodegenComment((con,
	"CVMglobals.objGlobalMicroLock = CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
				       destRegID, microLockRegID, 0);
    /* Failure target. Make instrinsic return false. */
    CVMtraceJITCodegen(("\t\tfailed:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    CVMCPUemitLoadConstant(con, destRegID, CVM_FALSE);
    /* "done" target. No change is made instrinc result. */
    CVMtraceJITCodegen(("\t\tdone:\n"));
    CVMJITfixupAddress(con, fixupPC2, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

#ifdef CVM_DEBUG
    /* For Debug builds, we do the following:
     *
     * 1. Set the ee's microlock depth to 0 or 1 based on success.
     * 2. Set CVMglobals.jit.currentSimpleSyncMB to the Simple Sync
     *    mb we are currently generating code for.
     *
     * (1) is done so C code will assert if the micrlock gets out
     * of balance. Note we don't assert in here in the generated code
     * because it is too ugly.
     *
     * (2) is done in case there is ever a problem, we can find out
     * the last Simple Sync method called by looking in CVMglobals.
     * It is disabled with #if 0 by default.
     */
    {
	/* 1. Set the ee's microlock depth to 0 or 1 based on success. */
	int eeReg;
#ifndef CVMCPU_EE_REG
	CVMRMResource *eeRes =
	    CVMRMgetResource(CVMRM_INT_REGS(con),
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	eeReg = CVMRMgetRegisterNumber(eeRes);
	/* Get the ee: */
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeReg,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeReg = CVMCPU_EE_REG;
#endif
	/* Set the ee's microlock depth. We just set it to the result
	 * of this intrinsic, which will be 0 or 1. */
	CVMJITaddCodegenComment((con, "ee->microLock = <result>"));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
            destRegID, eeReg, offsetof(CVMExecEnv, microLock));
#ifndef CVMCPU_EE_REG
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    }
    /* 
     * The following debugging code is disabled for now, but can be enabled
     * if Simple Sync methods are suspected of causing problems, like a
     * deadlock or assert.
     */
#if 0
    {
	/* Store the mb of the currently executing Simple Sync method into
	 * CVMglobals.jit.currentSimpleSyncMB. */
	CVMRMResource* currentSimpleSyncMBRes;
	CVMRMResource* simpleSyncMBRes;
	CVMJITMethodContext* mc = con->inliningStack[con->inliningDepth-1].mc;
	/* load CVMglobals.jit.currentSimpleSyncMB address into a register */
	CVMJITsetSymbolName((con, "&CVMglobals.jit.currentSimpleSyncMB"));
	currentSimpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)&CVMglobals.jit.currentSimpleSyncMB);
	/* load the Simple Sync mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M", mc->cb, mc->mb));
	simpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)mc->mb);
	/* Store the Simple Sync mb into CVMglobals.jit.currentSimpleSyncMB. */
	CVMJITaddCodegenComment((con,
				 "CVMglobals.jit.currentSimpleSyncMB = %C.%M",
				 mc->cb, mc->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(simpleSyncMBRes),
	    CVMRMgetRegisterNumber(currentSimpleSyncMBRes), 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), currentSimpleSyncMBRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), simpleSyncMBRes);
    }
    {
	/* Store the mb of the currently executing method into
	 * CVMglobals.jit.currentMB. */
	CVMRMResource* currentMBRes;
	CVMRMResource* mbRes;
	/* load CVMglobals.jit.currentMB address into a register */
	CVMJITsetSymbolName((con, "&CVMglobals.jit.currentMB"));
	currentMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)&CVMglobals.jit.currentMB);
	/* load the current mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M",
			     CVMmbClassBlock(con->mb), con->mb));
	mbRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)con->mb);
	/* Store the Simple Sync mb into CVMglobals.jit.currentMB. */
	CVMJITaddCodegenComment((con,
				 "CVMglobals.jit.currentMB = %C.%M",
				 CVMmbClassBlock(con->mb), con->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(mbRes),
	    CVMRMgetRegisterNumber(currentMBRes), 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), currentMBRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbRes);
    }
#endif /* 0 */
#endif /* CVM_DEBUG */

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), microLock);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

/*
 * Intrinsic emitter for microlock version of CVM.simpleLockRelease().
 *
 * Stores CVM_MICROLOCK_UNLOCKED into CVMglobals.objGlobalMicroLock.
 */
static void
simpleLockReleaseEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* tmp;
    CVMRMResource* microLock;
    int tmpRegID, microLockRegID;

    popResource(con); /* pop the "this" argument, which we don't use */
    tmp = CVMRMgetResource(CVMRM_INT_REGS(con),
			   CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);

    tmpRegID = CVMRMgetRegisterNumber(tmp);

    /* get CVM_MICROLOCK_UNLOCKED value */
    CVMJITaddCodegenComment((con, "CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitLoadConstant(con, tmpRegID, CVM_MICROLOCK_UNLOCKED);

#ifdef CVM_DEBUG
    /* Set the ee's microlock depth to 0, which is the same as
     * CVM_MICROLOCK_UNLOCKED. */
    {
	int eeReg;
#ifndef CVMCPU_EE_REG
	CVMRMResource *eeRes =
	    CVMRMgetResource(CVMRM_INT_REGS(con),
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	eeReg = CVMRMgetRegisterNumber(eeRes);
	/* Get the ee: */
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeReg,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeReg = CVMCPU_EE_REG;
#endif
	CVMassert(CVM_MICROLOCK_UNLOCKED == 0);
	CVMJITaddCodegenComment((con, "ee->microLock = 0"));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
					   tmpRegID, eeReg,
					   offsetof(CVMExecEnv, microLock));
#ifndef CVMCPU_EE_REG
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    }
#endif /* CVM_DEBUG */

    /* load microlock address into microLockRegID */
    CVMJITsetSymbolName((con, "&CVMglobals.objGlobalMicroLock"));
    microLock = CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
				  CVMRM_ANY_SET, CVMRM_SAFE_SET,
				  (CVMUint32)&CVMglobals.objGlobalMicroLock);
    microLockRegID = CVMRMgetRegisterNumber(microLock);
    /* release the microlock */
    CVMJITaddCodegenComment((con,
	"CVMglobals.objGlobalMicroLock = CVM_MICROLOCK_UNLOCKED"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
				       tmpRegID, microLockRegID, 0);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), tmp);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), microLock);
}

#elif CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS

/* See x86 version in second half of file */
static void
simpleLockGrabEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode);

static void
simpleLockReleaseEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode);

#else
#error Unsupported locking type for CVMJIT_SIMPLE_SYNC_METHODS
#endif
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */

static void
doIntMinMax(
    CVMJITCompilationContext * con,
    CVMBool	min,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    int cclhs, ccrhs;
    int destRegID, lhsRegID, rhsRegID;
    if (min){
        cclhs = CVMCPU_COND_LE;
        ccrhs = CVMCPU_COND_GT;
    } else {
        cclhs = CVMCPU_COND_GE;
        ccrhs = CVMCPU_COND_LT;
    }
    /* REDUCE INSTRUCTION COUNT BY LOOKING AT REF COUNTS? */
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    destRegID = CVMRMgetRegisterNumber(dest);
    lhsRegID = CVMRMgetRegisterNumber(lhs);
    rhsRegID = CVMRMgetRegisterNumber(rhs);
#ifdef CVMCPU_HAS_CONDITIONAL_ALU_INSTRUCTIONS
    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, cclhs,
			      lhsRegID, rhsRegID);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      lhsRegID, CVMJIT_NOSETCC, cclhs);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      rhsRegID, CVMJIT_NOSETCC, ccrhs),
#else
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, destRegID,
			   lhsRegID, CVMJIT_NOSETCC);
    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, ccrhs,
			      lhsRegID, rhsRegID);
    CVMCPUemitMoveRegisterConditional(con, CVMCPU_MOV_OPCODE, destRegID,
                                      rhsRegID, CVMJIT_NOSETCC, ccrhs),
#endif
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

static void
imaxEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    doIntMinMax(con, CVM_FALSE, intrinsicNode, GET_REGISTER_GOALS);
}

static void
iminEmitOperator(CVMJITCompilationContext *con, CVMJITIRNode *intrinsicNode)
{
    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);
    doIntMinMax(con, CVM_TRUE, intrinsicNode, GET_REGISTER_GOALS);
}

const CVMJITIntrinsicEmitterVtbl
   CVMJITRISCintrinsicThreadCurrentThreadEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    java_lang_Thread_currentThread_EmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIAbsEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    iabsEmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIMaxEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    imaxEmitOperator,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicIMinEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    iminEmitOperator,
};

#ifdef CVMJIT_SIMPLE_SYNC_METHODS
const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicSimpleLockGrabEmitter =
{
    CVMJITRISCintrinsicDefaultGetRequired,
    CVMJITRISCintrinsicDefaultGetArgTarget,
    simpleLockGrabEmitter,
};

const CVMJITIntrinsicEmitterVtbl CVMJITRISCintrinsicSimpleLockReleaseEmitter =
{
    CVMJITRISCintrinsicSimpleLockReleaseGetRequired,
    CVMJITRISCintrinsicSimpleLockReleaseGetArgTarget,
    simpleLockReleaseEmitter,
};
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */

#endif  /* CVMJIT_INTRINSICS */

static void
shortenInt(
    CVMJITCompilationContext* con,
    int rightshiftop,
    int shiftwidth,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* src = popResource(con);
    CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					   target, avoid, 1);
    int destregno = CVMRMgetRegisterNumber(dest);
    CVMRMpinResource(CVMRM_INT_REGS(con), src, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitShiftByConstant(con, CVMCPU_SLL_OPCODE, destregno,
                              CVMRMgetRegisterNumber(src), shiftwidth);
    CVMCPUemitShiftByConstant(con, rightshiftop, destregno, destregno, shiftwidth);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    pushResource(con, dest);
}

/* Purpose: Fetches an instance field from an object. */
static void fetchField(CVMJITCompilationContext *con,
		       CVMJITRMContext* rc,
                       CVMJITIRNodePtr thisNode,
                       CVMRMregset target, CVMRMregset avoid,
                       CVMInt32 opcode, int fieldSize)
{
    CVMCPUMemSpec *fieldOffset = popMemSpec(con);
    CVMRMResource *objPtr = popResource(con);
    CVMRMResource *dest;
    dest = CVMRMgetResource(rc, target, avoid, fieldSize);
    CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMJITaddCodegenComment((con, "= getfield(obj, fieldIdx);"));
    CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitMemoryReference(con, opcode,
        CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(objPtr),
        CVMCPUmemspecGetToken(con, fieldOffset));
    CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

#define GETFIELD_SYNTHESIS(con, p)   L_BINARY_SYNTHESIS((con), (p))
#define GETFIELD_INHERITANCE(con, p) L_BINARY_INHERITANCE((con), (p))

/* Purpose: Sets a value to an instance field of an object. */
static void setField(CVMJITCompilationContext *con,
		     CVMJITRMContext* rc,
                     CVMInt32 opcode)
{
    CVMRMResource *src = popResource(con);
    CVMCPUMemSpec *fieldOffset = popMemSpec(con);
    CVMRMResource *objPtr = popResource(con);

    CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(rc, src,    rc->anySet, CVMRM_EMPTY_SET);

    CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset,
			     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUemitMemoryReference(con, opcode,
        CVMRMgetRegisterNumber(src), CVMRMgetRegisterNumber(objPtr),
        CVMCPUmemspecGetToken(con, fieldOffset));
    CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
    CVMRMrelinquishResource(rc, src);
}

/* pops a Java stack value into a register */
static void
forceJavaStackTopValueIntoRegister(
    CVMJITCompilationContext* con,
    CVMJITRMContext* dstCtx,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource* operand = popResource(con);
    if (CVMRMisJavaStackTopValue(operand)) {
	CVMRMpinResource(dstCtx, operand, target, avoid);
    }
    CVMRMunpinResource(dstCtx, operand);
    pushResource(con, operand);
}

/* convert CVMJIT_XXX condition code to a CVMCPUCondCode */
static CVMCPUCondCode
mapCondCode(CVMUint16 condition) {
    switch(condition) {
        case CVMJIT_EQ: return CVMCPU_COND_EQ;
        case CVMJIT_NE: return CVMCPU_COND_NE;
        case CVMJIT_LE: return CVMCPU_COND_LE;
        case CVMJIT_GE: return CVMCPU_COND_GE;
        case CVMJIT_LT: return CVMCPU_COND_LT;
        case CVMJIT_GT: return CVMCPU_COND_GT;
        default: CVMassert(CVM_FALSE); return 0;
    }
}

typedef struct {
    CVMUint16 low;
    CVMUint16 high;
    CVMUint16 prevIndex;
    CVMInt32 logicalAddress; /* address that branches to this node */
} CVMLookupSwitchStackItem;

static void
pushLookupNode(CVMLookupSwitchStackItem* stack, CVMUint8* tos,
               CVMUint16 low, CVMUint16 high, CVMUint16 prevIndex,
	       CVMInt32 logicalAddress)
{
    CVMLookupSwitchStackItem* item = &stack[*tos];
    CVMtraceJITCodegen(("---> Pushing #%d (low=%d index=%d high=%d prev=%d)\n",
                       *tos, low, (low + high)/2, high, prevIndex));
    item->low   = low;
    item->high  = high;
    item->prevIndex  = prevIndex;
    item->logicalAddress = logicalAddress;
    ++(*tos);
}

static CVMLookupSwitchStackItem*
popLookupNode(CVMLookupSwitchStackItem stack[], CVMUint8* tos)
{
    CVMLookupSwitchStackItem* item;
    if (*tos == 0) {
	return 0;
    }
    --(*tos);
    item = &stack[*tos];
    CVMtraceJITCodegen(("<--- Popping #%d (low=%d index=%d high=%d prev=%d)\n",
                       *tos, item->low, (item->low + item->high) / 2,
                       item->high, item->prevIndex));
    return item;
}

/* Purpose: Emits a call to a Unary CCM helper. */
static void
unaryHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMRMregset outgoingSpillRegSet,
    int resultSize)
{
    CVMRMResource *src = popResource(con);
    CVMRMResource *dest;
    /* Pin the input to CVMCPU_ARG1_REG because the helper expects it there: */
    src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src, CVMCPU_ARG1_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, outgoingSpillRegSet);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        int argSize = CVMRMgetSize(src);
        if (argSize == 2) {
            /* argSize = 2 means the argument is doubleword */
            CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                          CVMCPU_ARG1_REG);
        }
    }
#endif

    /* Emit the call to the helper to compute the result: */
    CVMCPUemitAbsoluteCall(con, helperAddress,
			   CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, 
			   src->nregs);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (resultSize == 2) {
            /* resultSize = 2 means the result is doubleword. */
            CVMCPUemitMoveFrom64BitRegister(con, CVMCPU_RESULT1_REG,
                                            CVMCPU_RESULT1_REG);
        }
    }
#endif

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_RESULT1_REG,
				    resultSize);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

/* Purpose: Emits a call to a Binary CCM helper which return a word result. */
static void
wordBinaryHelper(
    CVMJITCompilationContext *con,
    void* helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 2, 1);
}

/* Purpose: Emits a call to a Binary CCM helper which return a dword result. */
static void
longBinaryHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 4, 2);
}

/* Purpose: Emits a call to a Binary CCM helper which return a dword result,
 * but whose 2nd argument is 32-bit, not 64-bit. */
static void
longBinaryHelper2(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero )
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 3, 2);
}

/* Purpose: Emits a call to a Binary CCM helper which return a word result. */
static void
longBinary2WordHelper(
    CVMJITCompilationContext *con,
    void *helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero)
{
    binaryHelper(con, helperAddress, thisNode, checkZero, 4, 1);
}

static void
branchToBlock(
    CVMJITCompilationContext* con,
    CVMCPUCondCode condcode,
    CVMJITIRBlock* target);

#ifdef CVM_NEED_DO_FCMP_HELPER

/* Purpose: Emits code to compare 2 floats by calling a helper function. */
static void
fcomparecc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
	   CVMBool needBranch, CVMBool needSetcc)
{
    CVMRMResource *rhs = popResource(con);
    CVMRMResource *lhs = popResource(con);
    CVMUint32 nanResult;
    int flags;
    
    /* If needBranch is TRUE, then we know this is CVMJITConditionalBranch.
     * Otherwise is is a CVMJITBinaryOp. */
    if (needBranch) {
	flags = CVMJITirnodeGetCondBranchOp(thisNode)->flags;
    } else {
	flags = CVMJITirnodeGetBinaryNodeFlag(thisNode);
    }

    if (flags & CVMJITCMPOP_UNORDERED_LT) {
        nanResult = -1;
    } else {
        nanResult = 1;
    }

    /* Pin the input to the first two arguments because the helper expects it
       there: */
    lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, CVMCPU_ARG1_REG);
    rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, CVMCPU_ARG2_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, ARG1|ARG2);

    CVMJITaddCodegenComment((con, "do fcmp"));
    CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFCmp);
    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, nanResult);

    /* Emit the call to the helper to compute the result: */
    CVMJITaddCodegenComment((con, "call CVMCCMruntimeFCmp"));
    CVMJITsetSymbolName((con, "CVMCCMruntimeFCmp"));
    CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeFCmp, CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, 3);

    /* if the needBranch is true, then we need to convert the {-1,0,1}
     * into a boolean condition code and do a conditional branch */
    if (needBranch) {
	CVMJITConditionalBranch* branch =
	    CVMJITirnodeGetCondBranchOp(thisNode);
	CVMCPUCondCode cc = mapCondCode(branch->condition);
	if (needSetcc) {
	    CVMJITaddCodegenComment((con, "set condition code"));
	    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, cc,
			      CVMCPU_RESULT1_REG, CVMCPUALURhsTokenConstZero);
	}
	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
	branchToBlock(con, cc, branch->target);
	CVMRMunpinAllIncomingLocals(con, branch->target);
    }

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
}

#endif /* CVM_NEED_DO_FCMP_HELPER */

#ifdef CVM_NEED_DO_DCMP_HELPER

/* Purpose: Emits code to compare 2 doubles by calling a helper. */
static void
dcomparecc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
	   CVMBool needBranch, CVMBool needSetcc)
{
    CVMRMResource *rhs = popResource(con);
    CVMRMResource *lhs = popResource(con);
    CVMUint32 nanResult;
    int flags;
    
    /* If needBranch is TRUE, then we know this is CVMJITConditionalBranch.
       Otherwise is is a CVMJITBinaryOp. */
    if (needBranch) {
	flags = CVMJITirnodeGetCondBranchOp(thisNode)->flags;
    } else {
	flags = CVMJITirnodeGetBinaryNodeFlag(thisNode);
    }

    if (flags & CVMJITCMPOP_UNORDERED_LT) {
        nanResult = -1;
    } else {
        nanResult = 1;
    }

    /* Pin the input to the first two arguments because the helper expects it
       there: */
    lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, CVMCPU_ARG1_REG);
    rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, CVMCPU_ARG3_REG);

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, ARG1|ARG2|ARG3|ARG4);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        /* Both arguments are doubleword */
        CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                      CVMCPU_ARG1_REG);
        CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG2_REG,
                                      CVMCPU_ARG3_REG);
    }
#endif

    /* Emit the call to the helper to compute the result: */
    if (nanResult == -1) {
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDCmpl);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDCmpl"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDCmpl"));
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeDCmpl,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, 4);
    } else {
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDCmpg);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDCmpg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDCmpg"));
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeDCmpg,
                               CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, 4);
    }

    /* if the needBranch is true, then we need to convert the {-1,0,1}
     * into a boolean condition code and do a conditional branch */
    if (needBranch) {
	CVMJITConditionalBranch* branch =
	    CVMJITirnodeGetCondBranchOp(thisNode);
	CVMCPUCondCode cc = mapCondCode(branch->condition);
	if (needSetcc) {
	    CVMJITaddCodegenComment((con, "set condition code"));
	    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, cc,
			      CVMCPU_RESULT1_REG, CVMCPUALURhsTokenConstZero);
	}
	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
	branchToBlock(con, cc, branch->target);
	CVMRMunpinAllIncomingLocals(con, branch->target);
    }

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
}

#endif /* CVM_NEED_DO_DCMP_HELPER */
%}

// Purpose: STATIC32(staticFieldSpec) = value32.
root: ASSIGN STATIC32 memSpec reg32 : 20 : : : : {
        CVMJITprintCodegenComment(("Do putstatic:"));
        CVMJITaddCodegenComment((con,
            "putstatic(staticFieldAddr, value{I|F})"));
        setStaticField(con, CVMRM_INT_REGS(con), CVMCPU_STR32_OPCODE);
    };

// Purpose: STATIC64(staticFieldSpec) = value64.
root: ASSIGN STATIC64 memSpec reg64 : 20 : : : : {
        CVMJITprintCodegenComment(("Do putstatic:"));
        CVMJITaddCodegenComment((con,
            "putstatic(staticFieldAddr, value{L|D})"));
        setStaticField(con, CVMRM_INT_REGS(con), CVMCPU_STR64_OPCODE);
};

// Purpose: STATIC64VOL(staticFieldSpec) = value64.
root: ASSIGN STATIC64VOL regAddr reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG3, ARG1) : : {
	CVMRMResource* rhs = popResource(con);
	CVMRMResource* lhs = popResource(con);

	/* Swap the arguments because the runtime helper function will expect
	   the 64-bit source value to come first followed by the static field
	   address: */
	pushResource(con, rhs);
	pushResource(con, lhs);

        CVMJITprintCodegenComment(("Do volatile putstatic:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimePutstatic64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimePutstatic64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimePutstatic64Volatile);

	/* Call the helper function: */
        longBinaryHelper2(con, (void*)CVMCCMruntimePutstatic64Volatile, $$,
			  CVM_FALSE);
    };

aluRhs: ICONST_32 : 0 : : : :
    pushALURhsConstant(con, CVMJITirnodeGetConstant32($$)->j.i);

// Purpose: Converts a value32 into an aluRhs.
aluRhs: reg32 : 0 : : : : {
	CVMRMResource* operand = popResource(con);
        pushALURhsResource(con, operand);
};

memSpec: ICONST_32 : 0 : : : : {
        pushMemSpecImmediate(con, CVMJITirnodeGetConstant32($$)->j.i);
};

// Purpose: value32 = value32 << (const32 & 0x1f).
reg32: SLL32 reg32 ICONST_32 : 20 : : : : {
   doIntShift(con, CVMCPU_SLL_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value32 = value32 << (value32 & 0x1f).
reg32: SLL32 reg32 reg32 : 20 : : : : {
   doRegShift(con, CVMCPU_SLL_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value32 = value32 >>> (const32 & 0x1f). unsigned shift right.
reg32: SRL32 reg32 ICONST_32 : 20 : : : : {
   doIntShift(con, CVMCPU_SRL_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value32 = value32 >>> (value32 & 0x1f). unsigned shift right.
reg32: SRL32 reg32 reg32 : 20 : : : : {
   doRegShift(con, CVMCPU_SRL_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value32 = value32 >> (const32 & 0x1f). signed shift right.
reg32: SRA32 reg32 ICONST_32 : 20 : : : : {
   doIntShift(con, CVMCPU_SRA_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value32 = value32 >> (value32 & 0x1f). signed shift right.
reg32: SRA32 reg32 reg32 : 20 : : : : {
   doRegShift(con, CVMCPU_SRA_OPCODE, $$, GET_REGISTER_GOALS);
};

//
// "R" Sequences:
//
//         SEQUENCE_R
//         /      \
//      effect   expr
//
// evaluates to the value of 'expr'. 'effect' does not produce a value.
//

%{
#define SEQUENCE_R_INHERITANCE(thisNode, rc) { \
    SET_ATTRIBUTE_TYPE(0, CVMJIT_EXPRESSION_ATTRIBUTE_TARGET_AVOID); \
    goal_top[0].attributes[0].u.rs.target = CVMRM_GET_ANY_SET(rc);   \
    goal_top[0].attributes[0].u.rs.avoid  = CVMRM_EMPTY_SET;         \
    goal_top[0].attributes[1] = goal_top[-1].curr_attribute[0];      \
}
%}

reg32:  ISEQUENCE_R effect reg32 : 0 : :
        SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

reg64:  LSEQUENCE_R effect reg64 : 0 : :
	SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

// "type-less" sequence. 
reg32:  VSEQUENCE_R effect reg32 : 1 : : 
	SEQUENCE_R_INHERITANCE($$, CVMRM_INT_REGS(con)); : : ;

//
// "L" Sequences:
//
//         SEQUENCE_L
//         /      \
//      expr    effect
//
// evaluates to the value of 'expr'. 'effect' does not produce a value.
//

%{
#define SEQUENCE_L_INHERITANCE(thisNode, rc) { \
    SET_ATTRIBUTE_TYPE(1, CVMJIT_EXPRESSION_ATTRIBUTE_TARGET_AVOID); \
    goal_top[0].attributes[0] = goal_top[-1].curr_attribute[0];      \
    goal_top[0].attributes[1].u.rs.target = CVMRM_GET_ANY_SET(rc);   \
    goal_top[0].attributes[1].u.rs.avoid  = CVMRM_EMPTY_SET;         \
}
%}

reg32:  ISEQUENCE_L reg32 effect : 0 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

reg64:  LSEQUENCE_L reg64 effect : 0 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : {
    passLastEvaluated(con, CVMRM_INT_REGS(con), $$);
};

// "type-less" sequence. 
reg32:  VSEQUENCE_L reg32 effect : 1 : :
	SEQUENCE_L_INHERITANCE($$, CVMRM_INT_REGS(con)); : : ;

root: BEGININLINING ICONST_32: 0 : : : : {
    beginInlining(con, $$);
};

root: BEGININLINING METHOD_CONTEXT: 0 : : : : {
     beginInlining(con, $$);
 };

// SVMC_JIT rr 19Sep2003:
// When inlining a method of a class loaded while compiling
// (-XcompilingCausesClassLoading=true), begin inlining has a
// CLASS_CHECKINIT child node.
root: BEGININLINING regAddr: 0 : : : : {
     beginInlining(con, $$);
 };

// The following node is to mark the end of the inlining of a method

// "type-less" endinlining
effect:  VENDINLINING : 0 : : : : {
    endInlining(con, $$);
};

regObj: EXCEPTION_OBJECT : 0 : : : : {
	/* it appears in ARG1, so we initially bind it to ARG1. */
	CVMRMResource* dest =
	    CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_ARG1_REG, 1);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
};

reg32: ICONST_32 : 20 : : : :
        const2Reg32(con, CVMRM_INT_REGS(con), $$);

%{
#if 0
#define IDENT_SYNTHESIS(con, thisNode)			\
{							\
    if (!CVMJIT_JCS_DID_PHASE(IRGetState(thisNode),	\
	CVMJIT_JCS_STATE_SYNTHED))			\
    {							\
	/* same as DEFAULT_SYNTHESIS_CHAIN  */		\
	DEFAULT_SYNTHESIS1(con, thisNode);		\
    } else {						\
	/* Leaf node */					\
    }							\
}

#define IDENT_INHERITANCE(con, thisNode)		\
{							\
    CVMassert(!CVMJIT_DID_SEMANTIC_ACTION(thisNode));	\
    DEFAULT_INHERITANCE_CHAIN(con, thisNode);		\
}
#endif
%}

%dag reg32: IDENT32 reg32 : 0 : 
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$);: : {
	CVMRMResource* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popResource(con);
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, $$);
	    /* CVMconsolePrintf("Initial evaluation of "); */
	} else {
	    src = CVMRMfindResource(CVMRM_INT_REGS(con), $$);
	    /* CVMconsolePrintf("Reiteration of "); */
	    CVMassert(src != NULL);
	}
	/*
	CVMconsolePrintf("Fixed IDENT32 ID %d, resource 0x%x\n",
	    $$->nodeID, src);
	*/
	pushResource(con, src);
};

// This handles IDENT32 over a BOUNDS_CHECK with a constant index
%dag iconst32Index: IDENT32 iconst32Index : 0 :
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	CVMInt32 idx;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    idx = popIConst32(con);
	    CVMJITirnodeGetIdentOp($$)->backendData = idx;
	} else {
	    idx = CVMJITirnodeGetIdentOp($$)->backendData;
	}
	pushIConst32(con, idx);
    };

%dag arrayIndex: IDENT32 arrayIndex : 0 : 
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	ScaledIndexInfo* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popScaledIndexInfo(con);
	    CVMJITidentitySetDecoration(con, (CVMJITIdentityDecoration*)src,
					$$);
	} else {
	    src = (ScaledIndexInfo*)CVMJITidentityGetDecoration(con, $$);
	    CVMassert((src == NULL) ||
		      CVMJITidentityDecorationIs(con, $$, SCALEDINDEX));
	    /* CVMconsolePrintf("Reiteration of "); */
            CVMassert(src != NULL);
	}
	/*
	CVMconsolePrintf("IDENT32 ID %d, resource 0x%x\n",
	    $$->nodeID, src);
	*/
	pushScaledIndexInfo(con, src);
};

%dag reg64: IDENT64 reg64 : 0 : 
    IDENT_SYNTHESIS(con, $$); : IDENT_INHERITANCE(con, $$); : : {
	CVMRMResource* src;
	if (!CVMJIT_DID_SEMANTIC_ACTION($$)){
	    src = popResource(con);
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, $$);
	} else {
	    src = CVMRMfindResource(CVMRM_INT_REGS(con), $$);
            CVMassert(src != NULL);
	}
	pushResource(con, src);
};

// Purpose: valueFloat = -valueFloat.
reg32: FNEG reg32 : 90 : SET_AVOID_C_CALL($$); : SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFNeg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFNeg"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFNeg);
        unaryHelper(con, (void*)CVMCCMruntimeFNeg, $$, ARG1, 1);
    };

// Purpose: valueFloat = valueFloat + valueFloat.
reg32: FADD reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFAdd"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFAdd"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFAdd);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFAdd, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat - valueFloat.
reg32: FSUB reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFSub"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFSub"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFSub);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFSub, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat * valueFloat.
reg32: FMUL reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFMul"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFMul"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFMul);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFMul, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat / valueFloat.
reg32: FDIV reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFDiv);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFDiv, $$, CVM_FALSE);
    };

// Purpose: valueFloat = valueFloat % valueFloat.
reg32: FREM reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeFRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeFRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeFRem);
        wordBinaryHelper(con, (void*)CVMCCMruntimeFRem, $$, CVM_FALSE);
    };

// Purpose: value32 = I2C(value32)
reg32: I2C reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2c:"));
        shortenInt(con, CVMCPU_SRL_OPCODE, 16, $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = I2S(value32)
reg32: I2S reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2s:"));
        shortenInt(con, CVMCPU_SRA_OPCODE, 16, $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = I2B(value32)
reg32: I2B reg32: 20 : : : : {
        CVMJITprintCodegenComment(("Do i2b:"));
        shortenInt(con, CVMCPU_SRA_OPCODE, 24, $$, GET_REGISTER_GOALS);
    };

// Purpose: valueDouble = (double) valueLong.
reg64: L2D reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeL2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeL2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeL2D);
        unaryHelper(con, (void*)CVMCCMruntimeL2D, $$, ARG1|ARG2, 2);
    };

// Purpose: valueFloat = (float) valueLong.
reg32: L2F reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeL2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeL2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeL2F);
        unaryHelper(con, (void*)CVMCCMruntimeL2F, $$, ARG1|ARG2, 1);
    };

// Purpose: valueInt = (int) valueLong.
reg32: L2I reg64 : 10 : : : : {
	CVMRMResource* src = popResource(con);
        int            srcReg;
	CVMRMResource* dest;

          /* get source register */
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        srcReg = CVMRMgetRegisterNumber( src );
          /* try to reuse source register */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	dest = CVMRMgetResource(CVMRM_INT_REGS(con),
				GET_REGISTER_GOALS, 1);
	/* now emit the conversion instruction(s) */
        CVMCPUemitLong2Int(con, CVMRMgetRegisterNumber(dest), srcReg);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueDouble = (double) valueInt.
reg64: I2D reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeI2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeI2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeI2D);
        unaryHelper(con, (void*)CVMCCMruntimeI2D, $$, ARG1, 2);
    };

// Purpose: valueFloat = (float) valueInt.
reg32: I2F reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeI2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeI2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeI2F);
        unaryHelper(con, (void*)CVMCCMruntimeI2F, $$, ARG1, 1);
    };

// Purpose: valueDouble = (double) valueFloat.
reg64: F2D reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2D"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2D"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2D);
        unaryHelper(con, (void*)CVMCCMruntimeF2D, $$, ARG1, 2);
    };

// Purpose: valueInt = (int) valueFloat.
reg32: F2I reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2I"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2I"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2I);
        unaryHelper(con, (void*)CVMCCMruntimeF2I, $$, ARG1, 1);
    };

// Purpose: valueLong = (long) valueFloat.
reg64: F2L reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeF2L"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeF2L"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeF2L);
        unaryHelper(con, (void*)CVMCCMruntimeF2L, $$, ARG1, 2);
    };

// Purpose: valueFloat = (float) valueDouble.
reg32: D2F reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2F"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2F"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2F);
        unaryHelper(con, (void*)CVMCCMruntimeD2F, $$, ARG1|ARG2, 1);
    };

// Purpose: valueInt = (int) valueDouble.
reg32: D2I reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2I"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2I"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2I);
        unaryHelper(con, (void*)CVMCCMruntimeD2I, $$, ARG1|ARG2, 1);
    };

// Purpose: valueLong = (long) valueDouble.
reg64: D2L reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeD2L"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeD2L"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeD2L);
        unaryHelper(con, (void*)CVMCCMruntimeD2L, $$, ARG1|ARG2, 2);
    };

reg64: IADD64 reg64 reg64 : 20 : : : : {
  longBinaryOp(con, CVMCPU_ADD64_OPCODE, $$, GET_REGISTER_GOALS);
};
reg64: ISUB64 reg64 reg64 : 20 : : : : {
  longBinaryOp(con, CVMCPU_SUB64_OPCODE, $$, GET_REGISTER_GOALS);
};
reg64: AND64  reg64 reg64 : 20 : : : : {
  longBinaryOp(con, CVMCPU_AND64_OPCODE, $$, GET_REGISTER_GOALS);
};
reg64: OR64   reg64 reg64 : 20 : : : : {
  longBinaryOp(con, CVMCPU_OR64_OPCODE, $$, GET_REGISTER_GOALS);
};
reg64: XOR64  reg64 reg64 : 20 : : : : {
 longBinaryOp(con, CVMCPU_XOR64_OPCODE, $$, GET_REGISTER_GOALS);
};
// Purpose: valueDouble = -valueDouble.
reg64: DNEG reg64 : 90 : SET_AVOID_C_CALL($$); : SET_TARGET1($$, ARG1); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDNeg"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDNeg"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDNeg);
        unaryHelper(con, (void*)CVMCCMruntimeDNeg, $$, ARG1|ARG2, 2);
    };

// Purpose: valueDouble = valueDouble + valueDouble.
reg64: DADD reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDAdd"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDAdd"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDAdd);
        longBinaryHelper(con, (void*)CVMCCMruntimeDAdd, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble - valueDouble.
reg64: DSUB reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDSub"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDSub"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDSub);
        longBinaryHelper(con, (void*)CVMCCMruntimeDSub, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble * valueDouble.
reg64: DMUL reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDMul"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDMul"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDMul);
        longBinaryHelper(con, (void*)CVMCCMruntimeDMul, $$, CVM_FALSE);
    };

// Purpose: valueDouble = valueDouble / valueDouble.
reg64: DDIV reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDDiv"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDDiv"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDDiv);
        longBinaryHelper(con, (void*)CVMCCMruntimeDDiv, $$, CVM_FALSE);
    };

// Purpose: remember variable arraySubscript
arraySubscript: reg32 : 0 : : : :{
	CVMRMResource* operand = popResource(con);
        pushScaledIndexInfoReg(con, operand);
    };

// Purpose: remember constant arraySubscript
arraySubscript: iconst32Index : 0 : : : :{
        CVMInt32 index = popIConst32(con);
        pushScaledIndexInfoImmediate(con, index);
    };

// Purpose: value64 = FETCH64(INDEX(arrayObject, arraySubscript))
arrayIndex: INDEX regObj arraySubscript : 20 : : : : {
        indexedAddr(con, $$, GET_REGISTER_GOALS);
    };

// Purpose: value64 = FETCH64(INDEX(arrayObject, arraySubscript))
reg64: FETCH64 INDEX regObj arraySubscript : 20 :
    ARRAY_LOAD_SYNTHESIS(con, $$); : ARRAY_LOAD_INHERITANCE(con, $$); : : {
        indexedLoad(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = FETCH32(INDEX(arrayObject, arraySubscript))
reg32: FETCH32 INDEX regObj arraySubscript : 20 :
    ARRAY_LOAD_SYNTHESIS(con, $$); : ARRAY_LOAD_INHERITANCE(con, $$); : : {
        indexedLoad(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value64 = FETCH64(INDEX(arrayObject, arraySubscript))
reg64: FETCH64 arrayIndex : 20 : :  : : {
        CVMJITprintCodegenComment(("Do *slotAddr64:"));
        fetchArraySlot(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: value32 = FETCH32(INDEX(arrayObject, arraySubscript))
reg32: FETCH32 arrayIndex : 20 : :  : : {
        CVMJITprintCodegenComment(("Do *slotAddr32:"));
        fetchArraySlot(con, CVMRM_INT_REGS(con), $$, GET_REGISTER_GOALS);
    };

// Purpose: ASSIGN(FIELDREF32(obj,fieldOffset), value32)
root: ASSIGN FIELDREF32 regObj memSpec reg32 : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do putfield:"));
        CVMJITaddCodegenComment((con,
            "putfield(obj, fieldOffset, value{I|F});"));
        setField(con, CVMRM_INT_REGS(con), CVMCPU_STR32_OPCODE);
    };

// Purpose: ASSIGN(FIELDREF64(obj,fieldOffset), value64)
root: ASSIGN FIELDREF64 regObj memSpec reg64 : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do putfield:"));
        CVMJITaddCodegenComment((con,
            "putfield(obj, fieldOffset, value{L|D});"));
        setField(con, CVMRM_INT_REGS(con), CVMCPU_STR64_OPCODE);
    };

// Purpose: Convert the regAddr to a reg32.
// NOTE: This is needed by FIELDREF64VOL because it takes a reg32 offset that
//       can either be a constant (a resolved value), or a regAddr (a
//       unresolved value).  The redAddr is essentially a reg32 anyway.
//       However, even though the cost of this conversion is essentially 0,
//       we still give is a minimal cost value.  This is so that it doesn't
//       create unnecessary ambiguities in the rules matching.
reg32: regAddr : 10 : : : : ;

// Purpose: ASSIGN(FIELDREF64VOL(obj,fieldOffset), value64)
root: ASSIGN FIELDREF64VOL regObj reg32 reg64 : 90 :
    SET_AVOID_C_CALL($$); : PUTFIELD_INHERITANCE(con, $$);
    SET_TARGET3_WO_INHERITANCE($$, ARG3, ARG4, ARG1); : : {

	CVMRMResource* value = popResource(con);
	CVMRMResource* fieldOffset = popResource(con);
	CVMRMResource* obj; /* Leave the obj on the stack for the null check. */

	/* Note: we need to do the NULL check for this object reference because
	   the CCM helper can't do it: */
	doNullCheck(con, NULL, ARG3, ARG1|ARG2|ARG4);

	/* Now do the put field: */
	obj = popResource(con); /* OK, to pop the obj now. */

	value = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					 value, CVMCPU_ARG1_REG);
	obj = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
				       obj, CVMCPU_ARG3_REG);
	fieldOffset = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con),
					       fieldOffset, CVMCPU_ARG4_REG);

	/* Spill the outgoing registers if necessary: */
	CVMRMminorSpill(con, ARG1|ARG2|ARG3|ARG4);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
	/* Shuffle the 64-bit arg first: */
	CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, CVMCPU_ARG1_REG);
	/* Shuffle the remaining 32-bit args: */
	CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
			       CVMCPU_ARG2_REG, CVMCPU_ARG3_REG,
			       CVMJIT_NOSETCC);
	CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
			       CVMCPU_ARG3_REG, CVMCPU_ARG4_REG,
			       CVMJIT_NOSETCC);
#endif

        CVMJITprintCodegenComment(("Do volatile putfield:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimePutfield64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimePutfield64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimePutfield64Volatile);

	CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimePutfield64Volatile,
			       CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, 4);
	
	/* Release resources and publish the result: */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), value);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), fieldOffset);
    };

//
// The rhs of a 32-bit array store
//
arrayAssignmentRhs32: reg32 : 0 : : : : ;

//
// No narrowing conversions on the rhs of an array store needed, since
// they are going to be expanded straight back anyway.
//
// Make them all nops
//
arrayAssignmentRhs32: I2S reg32 : 0 : : : : ;
arrayAssignmentRhs32: I2B reg32 : 0 : : : : ;
arrayAssignmentRhs32: I2C reg32 : 0 : : : : ;

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value64)
root: ASSIGN arrayIndex reg64 : 20 : :  : : {
        CVMJITprintCodegenComment(("*slotAddr64 = reg:"));
        storeArraySlot(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value32)
root: ASSIGN arrayIndex arrayAssignmentRhs32 : 20 : : : : {
        CVMJITprintCodegenComment(("*slotAddr32 = reg:"));
        storeArraySlot(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value64)
root: ASSIGN INDEX regObj arraySubscript reg64 : 20 :
    ARRAY_STORE_SYNTHESIS(con, $$); : ARRAY_STORE_INHERITANCE(con, $$); : : {
        indexedStore(con, CVMRM_INT_REGS(con), $$);
    };

// Purpose: ASSIGN(INDEX(arrayObject, arraySubscript), value32)
root: ASSIGN INDEX regObj arraySubscript arrayAssignmentRhs32 : 20 :
    ARRAY_STORE_SYNTHESIS(con, $$); : ARRAY_STORE_INHERITANCE(con, $$); : : {
        indexedStore(con, CVMRM_INT_REGS(con), $$);
    };

//
// values, usually a result of ?: expressions, live across branches.
// These get stuffed into the spill area or passed , the first part of which
// is reserved for them, based on max number of define's per block
// in this method. If possible, these values are passed as registers
// rather than spilled.

root: DEFINE_VALUE32 reg32 : 10 : : : : {
        CVMRMResource* src = popResource(con);
	if (!CVMRMstoreDefinedValue(con, $$, src, 1)) {
	    return -2;  /* fail */
	}
    };

root: DEFINE_VALUE64 reg64 : 10 : : : : {
        CVMRMResource* src = popResource(con);
	if (!CVMRMstoreDefinedValue(con, $$, src, 2)) {
	    return -2;  /* fail */
	}
    };

root: LOAD_PHIS : 0 : : : : {
    CVMRMloadOrReleasePhis(con, $$, CVM_TRUE /* load */);
};

root: RELEASE_PHIS : 0 : : : : {
    CVMRMloadOrReleasePhis(con, $$, CVM_FALSE /* release */);
};

reg32: USED32 : 0 : : : :
    pushResource(con, CVMJITirnodeGetUsedOp($$)->resource );

reg64: USED64 : 0 : : : :
    pushResource(con, CVMJITirnodeGetUsedOp($$)->resource );

//
// Method invocation.

// Purpose: VINVOKE(parameters, methodBlock)
root: VINVOKE parameters regAddr : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
        CVMJITprintCodegenComment(("Invoke a method w/ a void return type"));
	invokeMethod(con, $$);
   };

// Purpose: methodBlock = METHOD_BLOCK
regAddr: METHOD_BLOCK : 10 : : : : {
        CVMRMResource *dest;
        CVMMethodBlock *mb = CVMJITirnodeGetConstantAddr($$)->mb;
        dest = CVMRMbindResourceForConstantAddr(CVMRM_INT_REGS(con), (CVMAddr)mb);
        CVMJITsetSymbolName((con, "mb %C.%M", CVMmbClassBlock(mb), mb));
	/* Need this in case this constant is a CSE */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// SVMC_SCP rr 2004-06-04: rule only needed when checking
// initialization of an inlined static method
// Purpose: methodContext = METHOD_CONTEXT
regAddr: METHOD_CONTEXT : 10 : : : : {
        CVMRMResource *dest;
        CVMJITMethodContext *mc = CVMJITirnodeGetConstantAddr($$)->mc;
        dest = CVMRMbindResourceForConstantAddr(CVMRM_INT_REGS(con), (CVMAddr)mc);
        CVMJITsetSymbolName((con, "mc %C.%M", CVMmbClassBlock(mc->mb), mc->mb));
	/* Need this in case this constant is a CSE */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// SVMC_JIT rr 2004-01-23: proper typing of method invocations
// Method invocations used to be integer typed. Returned floating
// point values had to be converted (loaded into a integer register,
// spilled, loaded again into a floating point register). I changed
// this such that INVOKE nodes are not folded any more to
// CVM_TYPEID_INT and CVM_TYPEID_LONG by CVMJITgetMassagedIROpcode().
//
// Purpose: int value32 = INVOKE32I(parameters, methodBlock)
invoke32i_result: INVOKE32I parameters regAddr : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
	CVMRMResource* dest;
        CVMJITprintCodegenComment(("Invoke a method w/ a int32 return type"));
	dest = invokeMethod(con, $$);
	pushResource(con, dest);
   };

// Purpose: int value64 = INVOKE64I(parameters, methodBlock)
invoke64i_result: INVOKE64I parameters regAddr : 40 : SET_AVOID_METHOD_CALL($$); :
    SET_TARGET2_1($$, ARG1); : : {
        CVMRMResource *dest;
        CVMJITprintCodegenComment(("Invoke a method w/ a int64 return type"));
	dest = invokeMethod(con, $$);
        pushResource(con, dest);
   };

iargs: NULL_IARG : 0 : : END_TARGET_IARG(con, $$); : : ;
iargs: IARG reg32 iargs : 0 : : SET_TARGET_IARG(con, $$); : : ;
iargs: IARG reg64 iargs : 0 : : SET_TARGET_IARG(con, $$); : : ;

// Purpose: Rule to allow us to avoid loading the MB into a register if we
//          don't need to do any checkinit or nullCheck.
intrinsicMB: METHOD_BLOCK : 0 : : : : ;

// Purpose: Allows checkinits and nullChecks to be performed because they can
//          be attached to the MB using sequence nodes.  This is why the cost
//          need to be higher than that of the "intrinsicMB: ICONST_32" rule
//          above so that if we don't go through this rule if we only have an
//          MB and no sequence nodes.
intrinsicMB: regAddr : 10 : : : : {
        CVMRMResource *mbptr = popResource(con);
        /* Just throw the mbptr resource away because we don't really need
           it. */
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbptr);
    };

// Purpose: VINTRINSIC(parameters, methodBlock)
effect: VINTRINSIC parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
// Purpose: value32 = INTRINSIC32(parameters, methodBlock)
invoke32i_result: INTRINSIC32 parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
// Purpose: value64 = INTRINSIC64(parameters, methodBlock)
invoke64i_result: INTRINSIC64 parameters intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: VINTRINSIC(iargs, methodBlock)
effect: VINTRINSIC iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value32 = INTRINSIC32(iargs, methodBlock)
reg32: INTRINSIC32 iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: value64 = INTRINSIC64(iargs, methodBlock)
reg64: INTRINSIC64 iargs intrinsicMB : 90 :
    SET_AVOID_INTRINSIC_CALL(con, $$); : SET_TARGET_INTRINSIC_CALL(con, $$);
    : CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER : {
#ifdef CVMJIT_INTRINSICS
        FLUSH_GOAL_TOP(con);
        invokeIntrinsicMethod(con, $$);
#else
        /* Should not get here when intrinsics are not supported: */
        CVMassert(CVM_FALSE);
        CVMJITerror(con, CANNOT_COMPILE,
                    "CVMJIT: IR syntax error: Intrinsics not supported");
#endif /* CVMJIT_INTRINSICS */
    };

// Purpose: Stores a 32 return value into a register.
reg32:	invoke32i_result: 20 : : : : {
       forceJavaStackTopValueIntoRegister(con, CVMRM_INT_REGS(con), 
					  GET_REGISTER_GOALS);
};

// Purpose: Stores a 64 return value into a register pair.
reg64:  invoke64i_result: 20 : : : : {
   forceJavaStackTopValueIntoRegister(con, CVMRM_INT_REGS(con), 
				      GET_REGISTER_GOALS);
};

// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
param32: invoke32i_result : 0 : : : : {
	/* Free! Already on Stack  */
	CVMRMResource *operand = popResource(con);
        CVMRMconvertJavaStackTopValue2StackParam(con, operand);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

param32: reg32 : 10 : : : : {
	CVMRMResource *operand = popResource(con);
	CVMSMpushSingle(con, CVMRM_INT_REGS(con), operand);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
param64: invoke64i_result : 0 : : : : {
        /* Free! Already on Stack  */
        CVMRMResource *operand = popResource(con);
        CVMRMconvertJavaStackTopValue2StackParam(con, operand);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

param64: reg64 : 20 : : : : {
	CVMRMResource *operand = popResource(con);
	CVMSMpushDouble(con, CVMRM_INT_REGS(con), operand);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

// decrement reference count on the expression.
effect: reg32: 0 : : : : {
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };
effect: reg64: 0 : : : : {
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);
    };

// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
root: invoke32i_result: 0 : : : : {
	/* the 0 cost here is a fib, but must be < the cost of a deferred
	 * pop of invoke32i_result into a reg32, so that this instruction
	 * gets emitted
	 */
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);

	CVMSMpopSingle(con, NULL);
    };
// SVMC_JIT rr 2004-01-23: proper typing of  method invocations
root: invoke64i_result: 0 : : : : {
	/* the 0 cost here is a fib, but must be < the cost of a deferred
	 * pop of invoke64i_result into a reg64, so that this instruction
	 * gets emitted
	 */
	CVMRMResource* operand = popResource(con);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), operand);

	CVMSMpopDouble(con, NULL);
    };

root: BCOND_INT reg32 aluRhs : 20 : : : :
        compare32cc(con, $$, CVMCPU_CMP_OPCODE);
root: BCOND_LONG reg64 reg64 : 20 : : : :
        compare64cc(con, $$);
root: BCOND_FLOAT reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        fcomparecc(con, $$,
		   CVM_TRUE /* needBranch */, CVM_TRUE /* needSetcc */);
    };
root: BCOND_DOUBLE reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        dcomparecc(con, $$,
		   CVM_TRUE /* needBranch */, CVM_TRUE /* needSetcc */);
    };

// Purpose: value32{-1,0,1} = LCMP(valueLong1, valueLong2)
reg32: LCMP reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLCmp"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLCmp"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLCmp);
        longBinary2WordHelper(con, (void*)CVMCCMruntimeLCmp, $$, CVM_FALSE);
    };

// Purpose: value32{-1,0,1} = FCMPL(valueFloat1, valueFloat2) by helper
reg32: FCMPL reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        CVMRMResource *dest;
        fcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

// Purpose: value32{-1,0,1} = FCMPG(valueFloat1, valueFloat2) by helper
reg32: FCMPG reg32 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG2); : CVM_NEED_DO_FCMP_HELPER : {
        CVMRMResource *dest;
        fcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

// Purpose: value32{-1,0,1} = DCMPL(valueDouble1, valueDouble2)
reg32: DCMPL reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        CVMRMResource *dest;
        dcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

// Purpose: value32{-1,0,1} = DCMPG(valueDouble1, valueDouble2)
reg32: DCMPG reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : CVM_NEED_DO_DCMP_HELPER : {
        CVMRMResource *dest;
        dcomparecc(con, $$,
		   CVM_FALSE /* needBranch */, CVM_FALSE /* needSetcc */);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

iconst32Index: ICONST_32 : 0 : : : :
        pushIConst32(con, CVMJITirnodeGetConstant32($$)->j.i);

root:	JSR : 10 : : : : {
	CVMJITIRBlock* target = CVMJITirnodeGetBranchOp($$)->branchLabel;
	CVMRMsynchronizeJavaLocals(con);

	/* We need a major spill because unlike goto, we will be returning. */
	CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);

	CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
	jsrToBlock(con, target);
	CVMRMunpinAllIncomingLocals(con, target);

	/* We no longer do checkGC() here, because we don't have the proper
           local state for RET at this point. checkGC() is now done at 
           the beginning of JSR return targets. */
    };

regAddr:  JSR_RETURN_ADDRESS : 0 : : : : {
	CVMRMResource* dest;

	/* Get a resource to put the JSR return address in. */
	dest = CVMRMgetResourceStrict(CVMRM_INT_REGS(con),  
	    CVMCPU_JSR_RETURN_ADDRESS_SET & CVMRM_ANY_SET,
	    ~(CVMCPU_JSR_RETURN_ADDRESS_SET & CVMRM_ANY_SET), 1);
	/* force the return address into dest */
        CVMCPUemitLoadReturnAddress(con, CVMRMgetRegisterNumber(dest));
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

root:	RET regAddr : 10 :::: {
	CVMRMResource* src = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMCPUemitRegisterBranch(con, CVMRMgetRegisterNumber(src));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
    };

root:	GOTO	: 10 : : : : {
	CVMJITIRBlock* target = CVMJITirnodeGetBranchOp($$)->branchLabel;

	CVMRMsynchronizeJavaLocals(con);
	CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);

        branchToBlock(con, CVMCPU_COND_AL, target);

	CVMRMunpinAllIncomingLocals(con, target);

	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
    };

root:   RETURN	: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, NULL, 0);
    };

root:	IRETURN reg32: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, CVMRM_INT_REGS(con), 1);
    };

root:   LRETURN reg64: 10 : : : : {
        /* Emit the one-way ticket home: */
        emitReturn(con, CVMRM_INT_REGS(con), 2);
    };

regAddr: RESOLVE_REF : 90 : SET_AVOID_C_CALL($$); : : : {
        resolveConstant(con, $$, GET_REGISTER_GOALS);
    };

// Purpose: CHECKINIT(cb)
// Is only performed for its effect
// SVMC_JIT rr 2004-02-24: (CHECK_INIT cb value) -> (SEQUENCE (FOR_EFFECT (CHECK_INIT cb)) value)
// see doCheckInitOnCB() for details
regAddr: CHECKINIT CLASS_BLOCK regAddr : 90 : SET_AVOID_C_CALL($$); : : : {
      doCheckInit(con, $$); 
    };

effect: MAP_PC : 0 : : : : {
	CVMCompiledPcMapTable* pcMapTable = con->pcMapTable;
        CVMUint16 javaPc = CVMJITirnodeGetTypeNode($$)->mapPcNode.javaPcToMap;
	CVMUint16 compiledPc = CVMJITcbufGetLogicalPC(con);

	CVMCompiledPcMap* pcMap =
	   &(pcMapTable->maps[con->mapPcNodeCount++]);

	CVMassert(con->mapPcNodeCount <= pcMapTable->numEntries);

	pcMap->javaPc = javaPc;
	pcMap->compiledPc = compiledPc;

	CVMJITprintCodegenComment(("MAP_PC idepth=%d javaPc=%d compiledPc=%d\n",
	    con->inliningDepth, javaPc, compiledPc));
    };

root: OUTOFLINEINVOKE : 0 : : : : {
    printInliningInfo(con, $$, "Out-of-line invocation");
};

// Purpose: valueRef = NEW_OBJECT(classBlock)
regObj: NEW_OBJECT regAddr : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG2); : : {
        CVMRMResource *cbRes = popResource(con);
	CVMRMResource* dest;

#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        CVMBool   canPatch = CVM_TRUE;
        int       pcHelper;
#endif

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        pcHelper = CVMJITcbufGetLogicalPC( con );
#endif  /* CVMCPU_EMITTER_FILLS_DELAY_SLOTS */

        CVMJITprintCodegenComment(("Do new:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG2_REG);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        /* can only patch, if address loading didn't require more than 1 instruction */
        if ( CVMJITcbufGetLogicalPC( con ) != pcHelper + CVMCPU_INSTRUCTION_SIZE )
          canPatch = CVM_FALSE;
        else {
          /* we can move the address loading into the delay slot of the runtime call,
             therefore rewind the PC to overwrite the already generated code.
             we also must remove the code reference from the constant pool
          */
          CVMJITprintCodegenComment( ("Rewind (and delay the last instruction)" ) );
          CVMJITcbufRewind( con, CVMCPU_INSTRUCTION_SIZE );
          /* NOTE: since the class block is passed as RegAddr, we can rely that
                   its (constant) address is known by means of the resource.
                   should we assert anyway ?
          */
          CVMJITremoveConstantReference( con, cbRes->constant, pcHelper );
        }
#endif  /* CVMCPU_EMITTER_FILLS_DELAY_SLOTS */

        CVMRMmajorSpill(con, ARG2, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeNewGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeNewGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeNew);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeNewGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
    if ( canPatch == CVM_TRUE ) {
      pcHelper = CVMJITcbufGetLogicalPC( con );
      CVMJITcbufRewind( con, CVMCPU_INSTRUCTION_SIZE );
#ifdef CVM_DEBUG_ASSERTS
      CVMJITprintCodegenComment( ("Rewriting the delay slot" ) );
      /* make sure that we are really patching the 'nop' */
      CVMassert( * ( CVMUint32 * )
                   CVMJITcbufGetPhysicalPC( con ) == CVMCPU_NOP_INSTRUCTION );
#endif
      /* load the class block's address through pinning */
      cbRes = CVMRMpinResourceSpecific( CVMRM_INT_REGS( con ), cbRes, CVMCPU_ARG2_REG );
#ifdef CVM_DEBUG_ASSERTS
      /* make sure that only one instruction was rewritten */
      CVMassert( CVMJITcbufGetLogicalPC( con ) = pcHelper );
#endif
    }
#endif  /* CVMCPU_EMITTER_FILLS_DELAY_SLOTS */

	CVMJITcaptureStackmap(con, 0);
	/*
	 * Return value is RESULT1
	 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
	pushResource(con, dest);
    };

// Purpose: value64 = FETCH64(STATIC64(staticFieldSpec))
reg64: FETCH64 STATIC64 memSpec : 20 : : : : {
        CVMJITprintCodegenComment(("Do getstatic:"));
        CVMJITaddCodegenComment((con,
            "value{L} = getstatic(staticFieldAddr);"));
        getStaticField(con, CVMRM_INT_REGS(con),
		       $$, GET_REGISTER_GOALS, CVMCPU_LDR64_OPCODE, 2);
    };

// Purpose: value64 = FETCH64(STATIC64VOL(staticFieldSpec))
reg64: FETCH64 STATIC64VOL regAddr : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG1); : : {
        CVMJITprintCodegenComment(("Do volatile getstatic:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeGetstatic64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeGetstatic64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeGetstatic64Volatile);
        unaryHelper(con, (void*)CVMCCMruntimeGetstatic64Volatile, $$, ARG1, 2);
    };

// Purpose: valueRef = NEW_ARRAY_REF(elementClassBlock, dimension)
regObj: NEW_ARRAY_REF regAddr reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG3, ARG2); : : {
        CVMRMResource *dest;
        CVMRMResource *dimension = popResource(con);
        CVMRMResource *cbRes = popResource(con);

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        CVMJITprintCodegenComment(("Do anewarray:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG3_REG);
        dimension = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), dimension,
					     CVMCPU_ARG2_REG);

        CVMRMmajorSpill(con, ARG2|ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

        CVMJITaddCodegenComment((con, "call CVMCCMruntimeANewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeANewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeANewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeANewArrayGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 
			       /* SVMC_JIT d022609 (ML) 2004-04-21. 
				  unify risc/cisc interface */
			       0);
	CVMJITcaptureStackmap(con, 0);
	
	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimension);
	pushResource(con, dest);
    };

// Purpose: MONITOR_ENTER(object)
root: MONITOR_ENTER regObj : 90 : : SET_TARGET1($$, ARG3); : : {
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        CVMUint32       delayedInstruction;
        int             avoidThisReg = 0;
        int             originalRegisterFlag;
        CVMBool         canPatch = CVM_TRUE;
        CVMUint32       spillPC;
        CVMUint32       startPC;
#endif
	CVMRMResource* src  = popResource(con);

#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        avoidThisReg = src->regno; XXX -1? XXX
        startPC = CVMJITcbufGetLogicalPC( con );
#endif

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        /* patching is risky if ARG3 didn't get loaded here */
        if ( startPC == CVMJITcbufGetLogicalPC( con ) )
          canPatch == CVM_FALSE;
        else
          spillPC = CVMJITcbufGetLogicalPC( con );
#endif
	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        if ( canPatch == CVM_TRUE && spillPC == CVMJITcbufGetLogicalPC( con ) ) {
          /* before the glue is called, the monitored object is loaded in register ARG3.
             Therefore we can always delay the last instruction of the loading sequence.
          */
          delayedInstruction = delayInstruction( con );
          originalRegisterFlag = CVMRMoccupyRegister( CVMRM_INT_REGS( con ),
                                                      avoidThisReg           );
        }
        else
          /* patching is dangerous when registers have been spilled ! */
          canPatch = CVM_FALSE;
#endif
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMonitorEnterGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMonitorEnterGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMonitorEnter);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMonitorEnterGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        if ( canPatch == CVM_TRUE ) {
          CVMRMrestoreRegister( CVMRM_INT_REGS( con ),
                                avoidThisReg, originalRegisterFlag );
          rewriteDelaySlot( con, delayedInstruction );
        }
#endif
	CVMJITcaptureStackmap(con, 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

// Purpose: MONITOR_EXIT(object)
effect: MONITOR_EXIT regObj : 90 : : SET_TARGET1($$, ARG3); : : {
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        CVMUint32       delayedInstruction;
        int             avoidThisReg = 0;
        int             originalRegisterFlag;
        CVMBool         canPatch = CVM_TRUE;
        CVMUint32       spillPC;
        CVMUint32       startPC;
#endif
	CVMRMResource* src  = popResource(con);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        avoidThisReg = src->regno; XXX -1? XXX
        startPC = CVMJITcbufGetLogicalPC( con );
#endif
        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        /* patching is risky if ARG3 didn't get loaded here */
        if ( startPC == CVMJITcbufGetLogicalPC( con ) ) {
	    canPatch == CVM_FALSE;
	} else {
	    spillPC = CVMJITcbufGetLogicalPC( con );
	}
#endif
	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);

#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        if ( canPatch == CVM_TRUE && spillPC == CVMJITcbufGetLogicalPC( con ) ) {
           /* before the glue is called, the monitored object is loaded in register ARG3.
              Therefore we can always delay the last instruction of the loading sequence.
           */
           delayedInstruction = delayInstruction( con );
           originalRegisterFlag = CVMRMoccupyRegister( CVMRM_INT_REGS( con ),
                                                       avoidThisReg           );
         }
         else
           /* patching is dangerous when registers have been spilled ! */
           canPatch = CVM_FALSE;
#endif
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMonitorExitGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMonitorExitGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMonitorExit);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMonitorExitGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
#ifdef CVMCPU_EMITTER_FILLS_DELAY_SLOTS
        if ( canPatch == CVM_TRUE ) {
          CVMRMrestoreRegister( CVMRM_INT_REGS( con ),
                                avoidThisReg, originalRegisterFlag );
          rewriteDelaySlot( con, delayedInstruction );
        }
#endif
	CVMJITcaptureStackmap(con, 0);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

%{

/* Purpose: Emits code to do a NULL check of an object reference. */
static void doNullCheck(CVMJITCompilationContext *con,
			CVMJITIRNodePtr thisNode,
			CVMRMregset target, CVMRMregset avoid)
{
#ifdef CVMJIT_TRAP_BASED_NULL_CHECKS
    /* 
     * The NULL check is performed by doing a fake read using the pointer.
     * If null, we get a trap, which we catch and deal with.
     */
	CVMRMResource* objRes;
	CVMRMResource* scratch;

	CVMRMsynchronizeJavaLocals(con);

	/* Get the resource for the object to be NULL-checked */
	objRes = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), objRes, target, avoid);

	/* We are going to load into this register.
	   NOTE: We allocate the scratch after pinning the objRes because we
	         don't want to make sure that the objRes gets targetted to the
		 desired register first.
	*/
	scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				   CVMRM_ANY_SET, avoid, 1);
        scratch->flags |= CVMRMaddr;

	CVMJITaddCodegenComment((con,
				 trapCheckComments[CVMJITIR_NULL_POINTER]));
	
	/* LDR Rscratch, [obj] */
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDRADDR_OPCODE,
            CVMRMgetRegisterNumber(scratch), CVMRMgetRegisterNumber(objRes),
            0);

	/* We are done with the NULL check side effect. Pass the object on */
	if (thisNode != NULL) {
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), objRes, thisNode);
	} else {
	    CVMRMunpinResource(CVMRM_INT_REGS(con), objRes);
	}
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	pushResource(con, objRes);
#else
	CVMRMResource* objRes;

	CVMRMsynchronizeJavaLocals(con);

	/* Get the resource for the object to be NULL-checked */
	objRes = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), objRes, target, avoid);

	/* Compare to NULL */
        CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
	    CVMRMgetRegisterNumber(objRes), CVMCPUALURhsTokenConstZero);
	CVMJITaddCodegenComment((con,
	    trapCheckComments[CVMJITIR_NULL_POINTER]));
        CVMCPUemitAbsoluteCallConditional(con, 
            CVMCCMruntimeThrowNullPointerExceptionGlue,
            CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_EQ);

	/* We are done with the NULL check side effect. Pass the object on */
	if (thisNode != NULL) {
	    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), objRes, thisNode);
	} else {
	    CVMRMunpinResource(CVMRM_INT_REGS(con), objRes);
	}
	pushResource(con, objRes);
#endif
}

%}

// Purpose: Return object after null-checking
regObj: NULLCHECK regObj : 20 : : : : {
        doNullCheck(con, $$, GET_REGISTER_GOALS);
    };

//--------------- CISC and x86 specific ------------------------------------

//
//
// The goal for grammatical purposes will be called "root";
// The only root rules for now are assignment and control ops.
// The universal nonterminal here is reg32, i.e. a single processor register.

// Purpose: LOCAL32 = value32.
root:	ASSIGN LOCAL32 reg32 : 10 : : : : {
	CVMRMResource * rhs = popResource(con);
	CVMJITLocal   * lhs = CVMJITirnodeGetLocal(
	    CVMJITirnodeGetLeftSubtree($$));
	/* CVMBool isRef = CVMJITirnodeIsReferenceType($$); */

	/* If it is a constant, do not move it into a register at all */
	if (!CVMRMisConstant(rhs)) {
	  CVMRMpinResource(CVMRM_INT_REGS(con), rhs,
			   CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	}
	CVMRMstoreJavaLocal(CVMRM_INT_REGS(con), rhs, 1, CVM_FALSE, lhs->localNo);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    };

%{

/* Purpose: Sets a value to a static field of a class. */
static void setStaticField(CVMJITCompilationContext *con,
			   CVMJITRMContext* rc,
                           CVMInt32 opcode)
{
  int is_imm;

  /* NOTE: For a non-LVM build, the staticFieldSpec is the
     staticFieldAddress.  For an LVM build, the staticFieldSpec is the
     fieldblock of the static field. */
  
  /* store over static-field-ref over static field address */
  CVMRMResource *src = popResource(con); /* Right Hand Side first. */
  CVMCPUMemSpec *staticField = popMemSpec(con);

  CVMassert(staticField->type == CVMCPU_MEMSPEC_IMMEDIATE_OFFSET
	    || staticField->type == CVMCPU_MEMSPEC_REG_OFFSET );

  /* If the rhs, src, is a constant, do not move it into a register at all */
  is_imm = CVMRMisConstant(src);
  if (!is_imm) {
    CVMRMpinResource(rc, src, rc->anySet, CVMRM_EMPTY_SET);
  }
  
  switch (staticField->type) {
    case CVMCPU_MEMSPEC_IMMEDIATE_OFFSET:
	if (is_imm) {
	    CVMCPUemitMemoryReferenceImmAbsolute(con, opcode,
						 src->constant,
						 staticField->offsetValue);
	}
	else {
	    CVMCPUemitMemoryReferenceRegAbsolute(con, opcode,
						 CVMRMgetRegisterNumber(src),
						 staticField->offsetValue);
	}
      break;
    case CVMCPU_MEMSPEC_REG_OFFSET:
      CVMRMpinResource(CVMRM_INT_REGS(con), staticField->offsetReg, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
      if (is_imm)
      CVMCPUemitMemoryReferenceImmediateConst(con, opcode,
					 src->constant,
					 CVMRMgetRegisterNumber(staticField->offsetReg), 0);
      else
      CVMCPUemitMemoryReferenceImmediate(con, opcode,
					 CVMRMgetRegisterNumber(src),
					 CVMRMgetRegisterNumber(staticField->offsetReg), 0);
      break;
    default:
      CVMassert(0);
  }
  
  CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), staticField);
  CVMRMrelinquishResource(rc, src);
}

%}

%{

static void doIntShift(CVMJITCompilationContext *con, int shiftOp,
                       CVMJITIRNodePtr thisNode, CVMRMregset target, CVMRMregset avoid)
{
   CVMRMResource *dest;
   CVMInt32 destRegNum;
   CVMRMResource *lhs = popResource(con);
   CVMInt32 shiftOffset = 
      0x1f & CVMJITirnodeGetConstant32(CVMJITirnodeGetRightSubtree(thisNode))->j.i;
   CVMRMpinResource(CVMRM_INT_REGS(con), lhs, target, avoid);
   destRegNum = CVMRMgetRegisterNumber(lhs);
   CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
   dest = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << destRegNum), 
				 avoid, 1);
   CVMassert(destRegNum == CVMRMgetRegisterNumber(dest));
   CVMCPUemitShiftByConstant(con, shiftOp, destRegNum, destRegNum, shiftOffset);
   CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
   pushResource(con, dest);
}

%}

// Purpose: Converts a value32 into a memSpec.
memSpec: reg32 : 0 : : : : {
  CVMRMResource *operand = popResource(con);
  if (CVMRMisConstant(operand)) { 
    pushMemSpecImmediate(con, operand->constant);
  }
  else {
    pushMemSpecRegister(con, CVM_TRUE, operand);
  }
};

%{
static CVMRMResource *
InitLocalVarAddress(CVMJITCompilationContext* con,
		    CVMJITIRNodePtr thisNode,
		    int size,
		    CVMJITRMContext* rx,
		    CVMCPUAddress *addr,
		    CVMBool memopnd_on_rhs)  
{
    CVMRMResource* rp;
    CVMJITLocal* l;
    int frameOffset=0;
    int frameReg=0;
  
    if (memopnd_on_rhs) {
	l = CVMJITirnodeGetLocal( CVMJITirnodeGetRightSubtree(thisNode) );
    } else  {
	l = CVMJITirnodeGetLocal( CVMJITirnodeGetLeftSubtree(thisNode) );
    }

    CVMassert(!rx->compilationContext->inConditionalCode);  
    rp = rx->local2res[l->localNo];

#ifdef CVM_JIT_REGISTER_LOCALS
#else
    /* We assume that rp is always NULL, when execution reaches this program
       point.  If it is a CSE, an IDENT node should be used and not LOCAL32.
       Spilling does not seem to apply to LOCAL32; it applies to registers
       and the values accessed are of type CVMCPU_FRAME_TEMP and not
       CVMCPU_FRAME_LOCAL. */
    CVMassert(NULL == rp); 
#endif
  
    /* 
     * Taken from  reloadLocal(con, rx->loadOpcode[rp->size-1], rp):
     * Locals are located just before the frame ptr; this is a negative offset 
     */
    frameOffset = 
	-(int)((con->numberLocalWords - l->localNo) * sizeof(CVMJavaVal32));
    frameReg = CVMCPU_JFP_REG;

    /* Make sure we're within the addressing range: */
    if ((frameOffset > CVMCPU_MAX_LOADSTORE_OFFSET) ||
	(frameOffset < -(CVMCPU_MAX_LOADSTORE_OFFSET)))
    {
	CVMJITerror(con, CANNOT_COMPILE, "method has locals out of reach");
    }

    CVMJITaddCodegenComment((con, "Java local cell # %d", l->localNo));

    CVMCPUinit_Address_base_disp(addr, frameReg, frameOffset,
	CVMCPU_MEMSPEC_IMMEDIATE_OFFSET );

    return rp;
}

static void
wordRegRegOp(
    CVMJITCompilationContext* con,
    int opcode,
    CVMJITIRNodePtr thisNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
  CVMInt32 dstRegNum, rhsRegNum = -1;
  CVMCPUALURhs *rhs;
  CVMRMResource* lhs;
  CVMRMResource* dst;
  CVMassert(opcode == CVMCPU_ADD_OPCODE || opcode == CVMCPU_SUB_OPCODE ||
	    opcode == CVMCPU_AND_OPCODE || opcode == CVMCPU_OR_OPCODE  ||
	    opcode == CVMCPU_XOR_OPCODE || opcode == CVMCPU_MULL_OPCODE);

  rhs = popALURhs(con);
  lhs = popResource(con);
  CVMRMpinResource(CVMRM_INT_REGS(con), lhs, target, avoid);
  dstRegNum = CVMRMgetRegisterNumber(lhs);
  if (!CVMCPUalurhsIsConstant(rhs)) {
    CVMCPUalurhsPinResource(CVMRM_INT_REGS(con), opcode, rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMassert(CVMCPU_ALURHS_REGISTER == rhs->type);
    rhsRegNum = CVMRMgetRegisterNumber(rhs->r);
  }
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
  CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), rhs);
  dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegNum), avoid, 1);
  CVMassert(dstRegNum == CVMRMgetRegisterNumber(dst));  
  if (CVMCPUalurhsIsConstant(rhs)) {
    CVMCPUemitBinaryALUConstant(con, opcode, dstRegNum, dstRegNum, 
				CVMCPUalurhsGetConstantValue(rhs), CVM_FALSE);
  }
  else {
    CVMCPUemitBinaryALU(con, opcode, dstRegNum, dstRegNum,
			CVMCPUalurhsEncodeRegisterToken(con, rhsRegNum),
			CVM_FALSE);
  }
  CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, thisNode);
  pushResource(con, dst);
}

static void
wordRegStaticOp(CVMJITCompilationContext* con,
		int opcode,
		CVMJITIRNodePtr thisNode,
		CVMRMregset target,
		CVMRMregset avoid,
		CVMBool memopnd_on_rhs)
{
  CVMRMResource *dst;
  CVMInt32 dstRegID;
  CVMCPUAddress addr;
  CVMCPUMemSpec *staticField;
  CVMRMResource *regsrc;
  CVMassert(opcode == CVMCPU_ADD_OPCODE || opcode == CVMCPU_SUB_OPCODE ||
	    opcode == CVMCPU_AND_OPCODE || opcode == CVMCPU_OR_OPCODE  ||
	    opcode == CVMCPU_XOR_OPCODE || opcode == CVMCPU_MULL_OPCODE);
  
  if (memopnd_on_rhs == CVM_TRUE) {
    staticField = popMemSpec(con);
    regsrc = popResource(con);
  }
  else {
    CVMassert(opcode != CVMCPU_SUB_OPCODE);
    regsrc = popResource(con);
    staticField = popMemSpec(con);
  }

  CVMassert(staticField->type == CVMCPU_MEMSPEC_IMMEDIATE_OFFSET
	    || staticField->type == CVMCPU_MEMSPEC_REG_OFFSET );

  CVMRMpinResource(CVMRM_INT_REGS(con), regsrc, target, avoid);
  dstRegID = CVMRMgetRegisterNumber(regsrc);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), regsrc);
  dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID), avoid, 1);
  CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));

  switch (staticField->type) {
    case CVMCPU_MEMSPEC_IMMEDIATE_OFFSET:
      CVMCPUinit_Address_disp(&addr, staticField->offsetValue, CVMCPU_MEMSPEC_ABSOLUTE);
      break;
    case CVMCPU_MEMSPEC_REG_OFFSET:
      CVMRMpinResource(CVMRM_INT_REGS(con), staticField->offsetReg, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
      CVMCPUinit_Address_base_disp(&addr, CVMRMgetRegisterNumber(staticField->offsetReg),
				   0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      break;
    default:
      CVMassert(0);
  }

  CVMCPUemitBinaryALUMemory(con, opcode, dstRegID, dstRegID, &(addr));

  CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), staticField);

  CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, thisNode);
  pushResource(con, dst);
}

static void
wordRegFieldrefOp(CVMJITCompilationContext* con,
		  int opcode,
		  CVMJITIRNodePtr thisNode,
		  CVMRMregset target,
		  CVMRMregset avoid,
		  CVMBool memopnd_on_rhs)
{
  CVMCPUAddress addr;
  CVMInt32 dstRegID;
  CVMRMResource *dst;
  CVMCPUMemSpec *fieldOffset;
  CVMRMResource *objPtr;
  CVMRMResource *regsrc;
  CVMassert(opcode == CVMCPU_ADD_OPCODE || opcode == CVMCPU_SUB_OPCODE ||
	    opcode == CVMCPU_AND_OPCODE || opcode == CVMCPU_OR_OPCODE  ||
	    opcode == CVMCPU_XOR_OPCODE || opcode == CVMCPU_MULL_OPCODE);
  if (memopnd_on_rhs == CVM_TRUE) {
    fieldOffset = popMemSpec(con);
    objPtr = popResource(con);
    regsrc = popResource(con);
  }
  else {
    CVMassert(opcode != CVMCPU_SUB_OPCODE);    
    regsrc = popResource(con);
    fieldOffset = popMemSpec(con);
    objPtr = popResource(con);
  }
  CVMRMpinResource(CVMRM_INT_REGS(con), objPtr, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
  CVMJITaddCodegenComment((con, "= getfield(obj, fieldIdx);"));
  CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
  CVMRMpinResource(CVMRM_INT_REGS(con), regsrc, target, avoid);
  dstRegID = CVMRMgetRegisterNumber(regsrc);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), regsrc);
  dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID), avoid, 1);
  CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));
  CVMCPUinit_Address_base_memspec(&addr, CVMRMgetRegisterNumber(objPtr), fieldOffset);
  CVMCPUemitBinaryALUMemory(con, opcode, dstRegID, dstRegID, &(addr));
  CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
  CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, thisNode);
  pushResource(con, dst);
}

static void
wordRegLocalOp(CVMJITCompilationContext* con,
	       int opcode,
	       CVMJITIRNodePtr thisNode,
	       CVMRMregset target,
	       CVMRMregset avoid,
	       CVMBool memopnd_on_rhs)  
{
    CVMRMResource *dst;
    CVMRMResource *regsrc;
    CVMRMResource *l;
    int dstRegID;
    CVMCPUAddress addr;
    CVMJITRMContext* rx = CVMRM_INT_REGS(con);

    l = InitLocalVarAddress(con, thisNode, 1, rx, &addr, memopnd_on_rhs);
#if defined(CVM_JIT_REGISTER_LOCALS)
#if defined(X86_EXPERIMENTAL_CODE)
    if (l != NULL && CVMRMgetRegisterNumberUnpinned(l) != -1) {
	if (memopnd_on_rhs) {
	    pushALURhsResource(con, l);
	} else {
	    regsrc = popResource(con);
	    pushResource(con, l);
	    pushALURhsResource(con, regsrc);
	}
	wordRegRegOp(con, opcode, thisNode, target, avoid);
	return;
    }
#else
    (void) l;
#endif
#else
    CVMassert(l == NULL);
    (void) l;
#endif

    regsrc = popResource(con);
    CVMRMpinResource(rx, regsrc, target, avoid);
    dstRegID = CVMRMgetRegisterNumber(regsrc);
    CVMRMrelinquishResource(rx, regsrc);
    dst = CVMRMgetResourceStrict(rx, (1U << dstRegID), avoid, 1);
    CVMassert(dstRegID == CVMRMgetRegisterNumber(dst)); 
    CVMCPUemitBinaryALUMemory(con, opcode, dstRegID, dstRegID, &(addr));
    CVMRMoccupyAndUnpinResource(rx, dst, thisNode);
    pushResource(con, dst);
}

static void
longBinaryOp(CVMJITCompilationContext* con,
	      int opcode,
	      CVMJITIRNodePtr thisNode,
	      CVMRMregset target,
	      CVMRMregset avoid)
{
  CVMRMResource* rhs = popResource(con);
  CVMRMResource* lhs = popResource(con);
  CVMRMResource* dst;
  CVMInt32 lhsRegID = -1;
  CVMInt32 rhsRegID = -1;

  CVMRMpinResource(CVMRM_INT_REGS(con), rhs, target, avoid);
  CVMRMpinResource(CVMRM_INT_REGS(con), lhs, target, avoid);
  lhsRegID = CVMRMgetRegisterNumber(lhs);
  rhsRegID = CVMRMgetRegisterNumber(rhs);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
  /* clobber lhs register */
  dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << lhsRegID), CVMRM_EMPTY_SET, 2);
  CVMassert(lhsRegID == CVMRMgetRegisterNumber(dst));
  CVMCPUemitBinaryALU64(con, opcode, lhsRegID, lhsRegID, rhsRegID);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
  CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, thisNode);
  pushResource(con, dst);
}

static void doLongShiftConst(CVMJITCompilationContext *con,
			  int opcode, 
			  CVMJITIRNodePtr thisNode,
			  CVMRMregset target, 
			  CVMRMregset avoid)
{
  CVMRMResource *dst;
  CVMInt32 dstRegID;
  CVMRMResource *src; 
  CVMInt32 shiftOffset 
     = CVMJITirnodeGetConstant32(CVMJITirnodeGetRightSubtree(thisNode))->j.i;
  shiftOffset &= 0x3f;  /* mask higher bits per vm spec */
  if (shiftOffset == 0) {
    /* no code to emit in this case */
    passLastEvaluated(con, CVMRM_INT_REGS(con), thisNode);
  } else {
    src = popResource(con);
    CVMRMpinResource(CVMRM_INT_REGS(con), src, target, avoid);
    dstRegID = CVMRMgetRegisterNumber(src);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID), avoid, 2);
    CVMCPUemitLongShiftByConstant(con, opcode, dstRegID, dstRegID, shiftOffset);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, thisNode);
    pushResource(con, dst);
  }
}

/* Purpose: Emits a call to a Binary CCM helper. */
static void
binaryHelper(
    CVMJITCompilationContext *con,
    void* helperAddress,
    CVMJITIRNodePtr thisNode,
    CVMBool checkZero,
    int argSize, /* total size in words of all arguments */
    int resultSize)
{
    const char *symbolName;
    CVMCodegenComment *comment;
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dest;
    int lhsReg, rhsReg;
    CVMRMregset outSet;

    CVMJITpopSymbolName(con, symbolName);
    CVMJITpopCodegenComment(con, comment);
    lhsReg = CVMCPU_ARG1_REG;
    if (argSize == 2) {
	rhsReg = CVMCPU_ARG2_REG;
	outSet = ARG1|ARG2;
    } else if (argSize == 3){
	rhsReg = CVMCPU_ARG3_REG; 
	outSet = ARG1|ARG2|ARG3;
    } else {
	CVMassert(argSize == 4);
	rhsReg = CVMCPU_ARG3_REG; /* */
	outSet = ARG1|ARG2|ARG3|ARG4;
    }
    /* Pin the input to the first two arguments because the helper expects it
       there: */
    lhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), lhs, lhsReg);
    rhs = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), rhs, rhsReg);

    if (checkZero) {
	if (CVMRMdirtyJavaLocalsCount(con) == 0) {
	    if (resultSize == 2) {
		CVMRMResource *scratch =
		    CVMRMgetResource(CVMRM_INT_REGS(con),
				     CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
		int destReg = CVMRMgetRegisterNumber(scratch);
                CVMCPUemitBinaryALURegister(con, CVMCPU_OR_OPCODE,
					    destReg, rhsReg, rhsReg+1,
					    CVMJIT_SETCC);
#ifndef CVMCPU_HAS_ALU_SETCC
		CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
				  destReg, CVMCPUALURhsTokenConstZero);
					
#endif
		CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	    } else {
                CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
				  rhsReg, CVMCPUALURhsTokenConstZero);
	    }
	    CVMJITaddCodegenComment((
                con, trapCheckComments[CVMJITIR_DIVIDE_BY_ZERO]));
            CVMCPUemitAbsoluteCallConditional(con,
		(void*)CVMCCMruntimeThrowDivideByZeroGlue,
                CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_EQ);
	} else {
	    /* Dirty locals are not supported yet */
	    CVMassert(CVM_FALSE);
	}
    }

    /* Spill the outgoing registers if necessary: */
    CVMRMminorSpill(con, outSet);

#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (argSize >= 3) {
            /* argSize >= 3 means the lhs argument is doubleword */
            CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG1_REG, 
                                          CVMCPU_ARG1_REG);
            if (argSize == 4) {
                /* argSize = 4 means both lhs and rhs arguments 
                   are doubleword */
                CVMCPUemitMoveTo64BitRegister(con, CVMCPU_ARG2_REG,
                                              CVMCPU_ARG3_REG);
            } else {
                /* argSize is 3, and rhs is a 32-bit type. We need to move
                 * CVMCPU_ARG3_REG to CVMCPU_ARG2_REG to set up the second
                 * argument.
                 */
                CVMassert(argSize == 3);
                CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE,
                                       CVMCPU_ARG2_REG, CVMCPU_ARG3_REG,
                                       CVMJIT_NOSETCC);
            }
        }
    }
#endif /* CVMCPU_HAS_64BIT_REGISTERS */

    /* Emit the call to the helper to compute the result: */
    CVMJITpushCodegenComment(con, comment);
    CVMJITpushSymbolName(con, symbolName);
    CVMCPUemitAbsoluteCall(con, helperAddress,
			   CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK,
			   argSize);
    
#ifdef CVMCPU_HAS_64BIT_REGISTERS
    {
        if (resultSize == 2) {
            /* resultSize = 2 means the result is a doubleword type */
            CVMCPUemitMoveFrom64BitRegister(con, CVMCPU_RESULT1_REG,
                                            CVMCPU_RESULT1_REG);
        }
    }
#endif

    /* Release resources and publish the result: */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMCPU_RESULT1_REG,
				    resultSize);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

%}

reg32:	LOCAL32 : 10 : : : : {
	CVMJITLocal*   l = CVMJITirnodeGetLocal( $$ );
        CVMRMResource* dest =
            CVMRMbindResourceForLocal(CVMRM_INT_REGS(con), 1,
                                      CVM_FALSE, l->localNo);
        CVMassert(!CVMJITirnodeIsReferenceType($$));
        CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
                                            dest, GET_REGISTER_GOALS);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };


reg64:	LOCAL64 : 20 : : : : {
	CVMJITLocal*   l = CVMJITirnodeGetLocal( $$ );
        CVMRMResource* dest =
            CVMRMbindResourceForLocal(CVMRM_INT_REGS(con), 2,
                                      CVM_FALSE, l->localNo);
        CVMassert(!CVMJITirnodeIsReferenceType($$));
        CVMRMpinResourceEagerlyIfDesireable(CVMRM_INT_REGS(con),
                                            dest, GET_REGISTER_GOALS);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

%{

/* Purpose: Gets a value from a static field of an object. */
static void getStaticField(CVMJITCompilationContext *con,
			   CVMJITRMContext* rc,
                           CVMJITIRNodePtr thisNode,
                           CVMRMregset target, CVMRMregset avoid,
                           CVMInt32 opcode, int fieldSize)
{
    /* NOTE: For a non-LVM build, the staticFieldSpec is the
       staticFieldAddress.  For an LVM build, the staticFieldSpec is the
       fieldblock of the static field. */
    /* FIXME(rt) WANT TO allow floating target registers */

    /* fetch over static-field-ref over fb constant */
    CVMCPUMemSpec *staticField = popMemSpec(con);
    CVMRMResource *dest = CVMRMgetResource(rc, target, avoid, fieldSize);

    CVMassert(staticField->type == CVMCPU_MEMSPEC_IMMEDIATE_OFFSET
	      || staticField->type == CVMCPU_MEMSPEC_REG_OFFSET );

    switch (staticField->type) {
	case CVMCPU_MEMSPEC_IMMEDIATE_OFFSET:
	  CVMCPUemitMemoryReferenceRegAbsolute(con, opcode,
					       CVMRMgetRegisterNumber(dest),
					       staticField->offsetValue);
	  break;
	case CVMCPU_MEMSPEC_REG_OFFSET:
	  CVMRMpinResource(CVMRM_INT_REGS(con), staticField->offsetReg, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	  CVMCPUemitMemoryReferenceImmediate(con, opcode,
					     CVMRMgetRegisterNumber(dest),
					     CVMRMgetRegisterNumber(staticField->offsetReg), 0);
	  break;
	default:
	  CVMassert(0);
    }

    CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), staticField);
    CVMRMoccupyAndUnpinResource(rc, dest, thisNode);
    pushResource(con, dest);
}

%}

// Purpose: value32 = FETCH32(STATIC32(staticFieldSpec))
reg32: FETCH32 STATIC32 memSpec : 10 : : : : {
        CVMJITprintCodegenComment(("Do getstatic:"));
        CVMJITaddCodegenComment((con,
            "value{I|F|O} = getstatic(staticFieldAddr);"));
        getStaticField(con, CVMRM_INT_REGS(con),
		       $$, GET_REGISTER_GOALS, CVMCPU_LDR32_OPCODE, 1);
    };

// Purpose: valueRef = MULTI_NEW_ARRAY_REF(basicElementClassBlock, dimensions)
regObj: MULTI_NEW_ARRAY_REF regAddr parameters : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2_0($$, ARG3); : : {
	CVMRMResource* dest;
	CVMJITIRNode* paramnode;
	int nDimensions;
        CVMRMResource *cbRes = popResource(con);
        CVMRMResource *dimensionsDepth;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        CVMJITprintCodegenComment(("Do multianewarray:"));
        cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
					 CVMCPU_ARG3_REG);

	/* FIXME(rt) float reg spill */
        CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);

	/*
	 * number of dimensions is simply the number of PARAMETER nodes
	 * stacked under this one.
	 */
	paramnode = CVMJITirnodeGetRightSubtree($$);
        nDimensions = CVMJITirnodeGetBinaryOp($$)->data;

        /* Load the number of dimensions into ARG2: */
        CVMJITaddCodegenComment((con, "number of dimensions"));
        dimensionsDepth = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
						   CVMCPU_ARG2_REG, 1);
        CVMCPUemitLoadConstant(con, CVMCPU_ARG2_REG, nDimensions);

        /* Address of the first of them is JSP-4*nDimensions: */
        CVMJITaddCodegenComment((con, "&dimensions"));
        /* arg4 = jsp - (dimensionsDepth << 2): */
        CVMCPUemitComputeAddressOfArrayElement(con, CVMCPU_SUB_OPCODE,
            CVMCPU_ARG4_REG, CVMCPU_JSP_REG,
            CVMRMgetRegisterNumber(dimensionsDepth), CVMCPU_SLL_OPCODE, 2);
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeMultiANewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeMultiANewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeMultiANewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeMultiANewArrayGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	CVMJITcaptureStackmap(con, 0);

	/* pop dimensions from operand stack */
        CVMJITaddCodegenComment((con, "pop dimensions off the stack"));
        CVMCPUemitBinaryALUConstant(con, CVMCPU_SUB_OPCODE,
            CVMCPU_JSP_REG, CVMCPU_JSP_REG, 4*nDimensions, CVMJIT_NOSETCC);
        /* Tell stackman to pop the dimensions: */
        CVMSMpopParameters(con, nDimensions);

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimensionsDepth);
	pushResource(con, dest);
    };

// Purpose: valueRef = NEW_ARRAY_BASIC(dimension)
regObj: NEW_ARRAY_BASIC reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET1($$, ARG2); : : {
	CVMRMResource* dest;
	CVMRMResource* dimension = popResource(con);
        CVMBasicType typeCode;
	CVMClassBlock* arrCB;
	CVMUint32 elementSize;
	
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        /* Make JSP point just past the last argument currently on the stack */
        CVMSMadjustJSP(con);
#endif

        /* Map the IR node opcode to the type code: */
        typeCode = (CVMJITgetOpcode($$) >> CVMJIT_SHIFT_OPCODE)
                    - CVMJIT_NEW_ARRAY_BOOLEAN + CVM_T_BOOLEAN;

	/* This is known at compile time for arrays of basic types */
	arrCB = (CVMClassBlock*)CVMbasicTypeArrayClassblocks[typeCode];
	elementSize = CVMbasicTypeSizes[typeCode];

        CVMJITprintCodegenComment(("Do newarray:"));

	dimension = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), dimension,
					     CVMCPU_ARG2_REG);
	CVMRMmajorSpill(con, ARG2, CVMRM_SAFE_SET);

	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMCPUemitLoadAddrConstant(con, CVMCPU_ARG3_REG, (CVMAddr)arrCB);
        CVMCPUemitLoadConstant(con, CVMCPU_ARG1_REG, elementSize);

#ifdef CVM_JIT_INLINE_NEWARRAY
	/* SVMC_JIT c5041613 (dk) 12.02.2004 */
	/* Currently deactivated since inlining leads to performance loss.
	   Maybe it gets better if the majorSpill is removed from the
	   common path. However this requires it to be shifted into a
	   conditional branch. */
	inlineNewArray(con);
#else  /*  CVM_JIT_INLINE_NEWARRAY */
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeNewArrayGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeNewArrayGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeNewArray);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeNewArrayGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	CVMJITcaptureStackmap(con, 0);
#endif /*  CVM_JIT_INLINE_NEWARRAY */

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), dimension);
	pushResource(con, dest);
    };

regObj: STRING_ICELL_CONST : 20 : : : : {
	CVMRMResource* stringICellResource;
	CVMUint32      stringICellReg;
	CVMRMResource* stringObjectResource =
	    CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);
	CVMUint32      stringObjectReg =
	    CVMRMgetRegisterNumber(stringObjectResource);

	CVMStringICell* stringICell =	    
            CVMJITirnodeGetConstantAddr($$)->stringICell;
        CVMJITsetSymbolName((con, "StringICell"));
	stringICellResource =
	    CVMRMgetResourceForConstantAddr(CVMRM_INT_REGS(con),
					  CVMRM_ANY_SET, CVMRM_EMPTY_SET,
                                          (CVMAddr)stringICell);
	stringICellReg = CVMRMgetRegisterNumber(stringICellResource);
        CVMJITaddCodegenComment((con, "StringObject from StringICell"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDRADDR_OPCODE,
            stringObjectReg, stringICellReg, 0);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con),
					stringObjectResource, $$);
	pushResource(con, stringObjectResource);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), stringICellResource);
    };

regAddr: CLASS_BLOCK : 20 : : : : {
        CVMRMResource *dest;	    
        CVMClassBlock *cb = CVMJITirnodeGetConstantAddr($$)->cb;
        CVMJITsetSymbolName((con, "cb %C", cb));
        dest = CVMRMbindResourceForConstantAddr(CVMRM_INT_REGS(con),
						(CVMAddr)cb);      
        /* Need this in case this constant is a CSE */
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
        pushResource(con, dest);
    };

//  Purpose: value32 <- (0 - value32).
reg32: INEG32 reg32 : 10 : : : : {
   CVMRMResource *dst;
   CVMInt32 dstRegID;
   CVMRMResource *src = popResource(con);
   CVMRMpinResource(CVMRM_INT_REGS(con), src, GET_REGISTER_GOALS);
   dstRegID = CVMRMgetRegisterNumber(src);
   CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
   dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID),
				GET_REGISTER_AVOID_GOALS, 1);
   CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));
   CVMCPUemitUnaryALU(con, CVMCPU_NEG_OPCODE, dstRegID, dstRegID, CVM_TRUE);
   CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, $$);
   pushResource(con, dst);
};

// Purpose: value32 <- (value32 == 0)?1:0.
reg32: NOT32 reg32 : 10 : : SET_TARGET1($$, (1U << CVMX86_ECX)) : : {
    CVMRMResource *dst;
    CVMInt32 dstRegID;
    CVMRMResource *src = popResource(con);
    CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src, CVMX86_ECX);

    dstRegID = CVMRMgetRegisterNumber(src);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID),
                                 GET_REGISTER_AVOID_GOALS, 1);
    CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));
    CVMCPUemitUnaryALU(con, CVMCPU_NOT_OPCODE, dstRegID, dstRegID, CVM_TRUE);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, $$);
    pushResource(con, dst);
};

// Purpose: value32 <- (value32 != 0)?1:0.
reg32: INT2BIT32 reg32 : 10 : : SET_TARGET1($$, (1U << CVMX86_ECX)) : : {
    CVMRMResource *dst;
    CVMInt32 dstRegID;
    CVMRMResource *src = popResource(con);
    CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src, CVMX86_ECX);

    dstRegID = CVMRMgetRegisterNumber(src);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID),
                                 GET_REGISTER_AVOID_GOALS, 1);
    CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));
    CVMCPUemitUnaryALU(con, CVMCPU_INT2BIT_OPCODE, dstRegID, dstRegID,
                       CVM_TRUE);

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, $$);
    pushResource(con, dst);
};

reg32: IADD32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: IADD32 reg32 staticRef32FromMem : 15 : : : : {
    wordRegStaticOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: IADD32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: IADD32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};

reg32: IADD32 staticRef32FromMem reg32 : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: IADD32 fieldref32fromMem reg32 : 16 : : : : {
  wordRegFieldrefOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE); 
};
reg32: IADD32 LOCAL32 reg32 : 16 : : : : {
  wordRegLocalOp(con, CVMCPU_ADD_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};

staticRef32FromMem: FETCH32 STATIC32 memSpec : 0 : : : : {
  CVMJITprintCodegenComment(("Do getstatic-32-CISC:"));
};

// Currently unused.  Retained for future reference:
//staticRef64FromMem: FETCH64 STATIC64 memSpec : 0 : : : : {
//  CVMJITprintCodegenComment(("Do getstatic-64-CISC:"));
//};

// This is just a hook to get register usage propagation right
// (GETFIELD_SYNTHESIS, GETFIELD_INHERITANCE). 
fieldref32fromMem: FETCH32 FIELDREF32 regObj memSpec : 0 :
  GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
  CVMJITprintCodegenComment(("Do getfield-32-CISC:"));
};

// Currently unused.  Retained for future reference:
//// This is just a hook to get register usage propagation right
//// (GETFIELD_SYNTHESIS, GETFIELD_INHERITANCE). 
//fieldref64fromMem: FETCH64 FIELDREF64 regObj memSpec : 0 :
//  GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
//  CVMJITprintCodegenComment(("Do getfield-64-CISC:"));
//};

reg32: ISUB32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_SUB_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: ISUB32 reg32 staticRef32FromMem : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_SUB_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: ISUB32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_SUB_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: ISUB32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_SUB_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};

reg32: AND32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: AND32 reg32 staticRef32FromMem : 15 : : : : {
  wordRegStaticOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: AND32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: AND32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: AND32 staticRef32FromMem reg32 : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: AND32 fieldref32fromMem reg32 : 16 : : : : {
  wordRegFieldrefOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: AND32 LOCAL32 reg32 : 16 : : : : {
  wordRegLocalOp(con, CVMCPU_AND_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};

reg32: OR32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: OR32 reg32 staticRef32FromMem : 15 : : : : {
  wordRegStaticOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: OR32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: OR32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: OR32 staticRef32FromMem reg32 : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE); 
};
reg32: OR32 fieldref32fromMem reg32 : 16 : : : : {
  wordRegFieldrefOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: OR32 LOCAL32 reg32 : 16 : : : : {
  wordRegLocalOp(con, CVMCPU_OR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};

reg32: XOR32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: XOR32 reg32 staticRef32FromMem : 15 : : : : {
  wordRegStaticOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: XOR32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: XOR32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: XOR32 staticRef32FromMem reg32 : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: XOR32 fieldref32fromMem reg32 : 16 : : : : {
  wordRegFieldrefOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: XOR32 LOCAL32 reg32 : 16 : : : : {
  wordRegLocalOp(con, CVMCPU_XOR_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};

reg32: IMUL32 reg32 aluRhs : 10 : : : : {
  wordRegRegOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS);
};
reg32: IMUL32 reg32 staticRef32FromMem : 15 : : : : {
  wordRegStaticOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: IMUL32 reg32 fieldref32fromMem : 15 : : : : {
  wordRegFieldrefOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: IMUL32 reg32 LOCAL32 : 15 : : : : {
  wordRegLocalOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_TRUE);
};
reg32: IMUL32 staticRef32FromMem reg32 : 16 : : : : {
  wordRegStaticOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: IMUL32 fieldref32fromMem reg32 : 16 : : : : {
  wordRegFieldrefOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};
reg32: IMUL32 LOCAL32 reg32 : 16 : : : : {
  wordRegLocalOp(con, CVMCPU_MULL_OPCODE, $$, GET_REGISTER_GOALS, CVM_FALSE);
};

// Purpose: value32 = ALENGTH(arrayObj)
reg32: ALENGTH regObj : 100 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 1);
	CVMRMResource* src = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMJITaddCodegenComment((con, "arraylength"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(src),
            ARRAY_LENGTH_OFFSET);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	pushResource(con, dest);
    };

// Purpose: value64 = -value64.
reg64: INEG64 reg64 : 20 : : : : {
  CVMRMResource *dst;
  CVMInt32 dstRegID;
  CVMRMResource *src = popResource(con);
  CVMRMpinResource(CVMRM_INT_REGS(con), src, GET_REGISTER_GOALS);
  dstRegID = CVMRMgetRegisterNumber(src);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
  dst = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << dstRegID), GET_REGISTER_AVOID_GOALS, 2);
  CVMassert(dstRegID == CVMRMgetRegisterNumber(dst));
  CVMCPUemitUnaryALU64(con, CVMCPU_NEG64_OPCODE, dstRegID, dstRegID);
  CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, $$);
  pushResource(con, dst);
};

// Purpose: value64 = value64 / value64.
reg64: IDIV64  reg64 reg64 : 90 : SET_AVOID_C_CALL($$); : SET_TARGET2($$, ARG1, ARG3); : : {
  CVMJITaddCodegenComment((con, "call CVMCCMruntimeLDiv"));
  CVMJITsetSymbolName((con, "CVMCCMruntimeLDiv"));
  CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLDiv);
  longBinaryHelper(con, (void*)CVMCCMruntimeLDiv, $$, CVM_TRUE);
};

// Purpose: value64 = value64 % value64.
reg64: IREM64  reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :  SET_TARGET2($$, ARG1, ARG3); : : {
  CVMJITaddCodegenComment((con, "call CVMCCMruntimeLRem"));
  CVMJITsetSymbolName((con, "CVMCCMruntimeLRem"));
  CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLRem);
  longBinaryHelper(con, (void*)CVMCCMruntimeLRem, $$, CVM_TRUE);
};

// Purpose: value64 = value64 << (const32 & 0x3f).
reg64: SLL64 reg64 ICONST_32 : 40 : : : : {
  doLongShiftConst(con, CVMCPU_SLL64_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value64 = value64 >>> (const32 & 0x3f).
reg64: SRL64 reg64 ICONST_32 : 40 : : : : {
  doLongShiftConst(con, CVMCPU_SRL64_OPCODE, $$, GET_REGISTER_GOALS);
};

// Purpose: value64 = value64 >> (const32 & 0x3f).
reg64: SRA64 reg64 ICONST_32 : 40 : : : : {
  doLongShiftConst(con, CVMCPU_SRA64_OPCODE, $$, GET_REGISTER_GOALS);
};

// ***************************************************************************

// Purpose: value64 = value64 << value32.
reg64: SLL64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLShl"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLShl"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLShl);
        longBinaryHelper2(con, (void*)CVMCCMruntimeLShl, $$, CVM_FALSE);
    };

// Purpose: value64 = value64 >> value32.
reg64: SRA64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLShr"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLShr"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLShr);
        longBinaryHelper2(con, (void*)CVMCCMruntimeLShr, $$, CVM_FALSE);
    };

// Purpose: value64 = value64 >>> value32.
reg64: SRL64  reg64 reg32 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeLUshr"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLUshr"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeLUshr);
         longBinaryHelper2(con, (void*)CVMCCMruntimeLUshr, $$, CVM_FALSE);
    };


// ***************************************************************************


reg64: ICONST_64 : 20 : : : : {
	CVMRMResource* dest = CVMRMgetResource(CVMRM_INT_REGS(con),
					       GET_REGISTER_GOALS, 2);
	int destregno = CVMRMgetRegisterNumber(dest);
	CVMJavaVal64 v64;
	CVMmemCopy64(v64.v, CVMJITirnodeGetConstant64($$)->j.v);
        CVMCPUemitLoadLongConstant(con, destregno, v64.l);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: valueDouble = valueDouble % valueDouble.
reg64: DREM reg64 reg64 : 90 : SET_AVOID_C_CALL($$); :
    SET_TARGET2($$, ARG1, ARG3); : : {
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeDRem"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeDRem"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeDRem);
        longBinaryHelper(con, (void*)CVMCCMruntimeDRem, $$, CVM_FALSE);
    };

%{

/*
 * The descriptive table for each kind of array access.
 * This table is indexed by typeid. So the order of CVM_TYPEID constants
 * matters.
 *
 * For each entry, we have an index shift amount, whether this is a
 * ref entry, the appropriate load opcode, and the appropriate store
 * opcode for an array element of this type.
 */
typedef struct ArrayElemInfo ArrayElemInfo;
struct ArrayElemInfo {
    int      shiftAmount;  /* 2<<shiftAmount == elemSize */
    int      size;         /* Resultant size in words */
    CVMBool  isRef;
    int      loadOpcode;
    int      storeOpcode;
#ifdef CVM_JIT_USE_FP_HARDWARE
    int      floatLoadOpcode;
    int      floatStoreOpcode;
#endif
};

#define CVM_ILLEGAL_OPCODE -1

#ifdef CVM_JIT_USE_FP_HARDWARE
#define CVM_NONE	, CVM_ILLEGAL_OPCODE, CVM_ILLEGAL_OPCODE
#define CVM_FLDST_NONE	, CVM_ILLEGAL_OPCODE, CVM_ILLEGAL_OPCODE
#define CVM_FLDST32	, CVMCPU_FLDR32_OPCODE, CVMCPU_FSTR32_OPCODE
#define CVM_FLDST64	, CVMCPU_FLDR64_OPCODE, CVMCPU_FSTR64_OPCODE
#else
#define CVM_NONE
#define CVM_FLDST_NONE
#define CVM_FLDST32
#define CVM_FLDST64
#endif

typedef struct ScaledIndexInfo ScaledIndexInfo;
struct ScaledIndexInfo {
    /*
     * The data for the "super-class"
     */
    CVMJITIdentityDecoration  dec;

    CVMBool hasConstIndex;
    CVMInt32 index;
    CVMRMResource* indexReg;
    CVMRMResource* arrayBaseReg;
    int shiftAmount;
    const ArrayElemInfo* elemInfo;

    int baseRegID;
    CVMRMResource *slotAddrReg;
    int slotAddrOffset;
};

const ArrayElemInfo typeidToArrayElemInfo[] = {
    /* CVM_TYPEID_NONE */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_ENDFUNC */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_VOID */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVM_TYPEID_INT */
    {2, 1, CVM_FALSE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_SHORT */
    {1, 1, CVM_FALSE, CVMCPU_LDR16_OPCODE, CVMCPU_STR16_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_CHAR */
    {1, 1, CVM_FALSE, CVMCPU_LDR16U_OPCODE, CVMCPU_STR16_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_LONG */
    {3, 2, CVM_FALSE, CVMCPU_LDR64_OPCODE, CVMCPU_STR64_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_BYTE */
    {0, 1, CVM_FALSE, CVMCPU_LDR8_OPCODE, CVMCPU_STR8_OPCODE CVM_FLDST_NONE},
    /* CVM_TYPEID_FLOAT */
    {2, 1, CVM_FALSE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST32},
    /* CVM_TYPEID_DOUBLE */
    {3, 2, CVM_FALSE, CVMCPU_LDR64_OPCODE, CVMCPU_STR64_OPCODE CVM_FLDST64},
    /* CVM_TYPEID_BOOLEAN  This will look like a byte array */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},
    /* CVM_TYPEID_OBJ */
    {2, 1, CVM_TRUE, CVMCPU_LDR32_OPCODE, CVMCPU_STR32_OPCODE CVM_FLDST_NONE},
    /* CVMJIT_TYPEID_32BITS */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_64BITS */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_ADDRESS */
    {-1, -1, CVM_FALSE, 0, 0 CVM_FLDST_NONE},   /* No arrays of this type */
    /* CVMJIT_TYPEID_UBYTE */
    {0, 1, CVM_FALSE, CVMCPU_LDR8U_OPCODE, CVMCPU_STR8_OPCODE CVM_FLDST_NONE},
};


/* Purpose: Instantiates a ScaledIndexInfo data structure. */
static ScaledIndexInfo*
newScaledIndexInfo(CVMJITCompilationContext *con, CVMRMResource *indexReg,
                   CVMInt32 index, CVMBool isConstIndex)
{
    ScaledIndexInfo* sinfo = CVMJITmemNew(con, JIT_ALLOC_CGEN_OTHER,
                                          sizeof(ScaledIndexInfo));
    CVMJITidentityInitDecoration(con, &sinfo->dec,
				 CVMJIT_IDENTITY_DECORATION_SCALEDINDEX);
    sinfo->hasConstIndex = isConstIndex;
    sinfo->index = index;
    sinfo->indexReg = indexReg;
    if (indexReg != NULL) {
	/* incorporated into sinfo. Increment ref count */
	CVMRMincRefCount(con, indexReg);
    }
    sinfo->shiftAmount = -1;
#ifdef CVM_DEBUG
    sinfo->baseRegID = CVMCPU_INVALID_REG;
    sinfo->slotAddrReg = NULL;
    sinfo->slotAddrOffset = 0;
#endif
    return sinfo;
}

/* Purpose: Pushes a register index type ScaledIndexInfo on to the codegen
            semantic stack. */
static void
pushScaledIndexInfoReg(CVMJITCompilationContext *con, CVMRMResource *indexReg)
{
    ScaledIndexInfo* sinfo = newScaledIndexInfo(con, indexReg, 0, CVM_FALSE); 
    pushIConstAddr(con, (CVMAddr)sinfo); 
}

/* Purpose: Pushes an immediate index type ScaledIndexInfo on to the codegen
            semantic stack. */
static void
pushScaledIndexInfoImmediate(CVMJITCompilationContext *con, CVMInt32 index)
{
    ScaledIndexInfo* sinfo = newScaledIndexInfo(con, NULL, index, CVM_TRUE);  
    pushIConstAddr(con, (CVMAddr)sinfo);    
}

/* Purpose: Pops a ScaledIndexInfo off of the codegen semantic stack. */
CVM_INLINE static ScaledIndexInfo*
popScaledIndexInfo(CVMJITCompilationContext *con)
{   
    return (ScaledIndexInfo*)popIConstAddr(con);  
}

/* Purpose: Pops a ScaledIndexInfo off of the codegen semantic stack. */
CVM_INLINE static void
pushScaledIndexInfo(CVMJITCompilationContext *con, ScaledIndexInfo* sinfo)
{ 
    pushIConstAddr(con, (CVMAddr)sinfo);  
}

/* Purpose: Does setup for doing a scaled index operation.  This may entail:
            1. Allocating and pinning any scratch resources needed.
            2. Emitting some setup code to produce intermediate values to be
               used in a memory reference later to do the actual Java array
               element access.

   computeSlotAddr tells setupScaledIndex() that the array slot address
   has to be computed unconditionally as a register-immediate pair.

   isRef tells setupScaledIndex() that a card table routine is going to
   be called on the array access, so the slot address has to be computed
   precisely.

*/
static void
setupScaledIndex(CVMJITCompilationContext *con, int opcode,
                 CVMRMResource *array, ScaledIndexInfo *sinfo,
		 CVMBool isRefStore)
{
  int arrayRegID = CVMRMgetRegisterNumber(array);
  
  sinfo->slotAddrReg = NULL;
  
  if (sinfo->hasConstIndex) {
    CVMUint32 offset;
    
    /* Fold all constants into a single offset: */
    offset = (sinfo->index << sinfo->shiftAmount) + ARRAY_DATA_OFFSET;
    
    if (isRefStore) {
      CVMRMResource *scratch;
      int scratchRegID;
      /* If we get here, we're dealing with a obj ref store.  Hence,
	 we will need to fully evaluate the effective address into a
	 register and use a register offset type of memspec.  The fully
	 evaluated effective address will be used later by GC card
	 table marking code.
      */
      scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				 CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
      scratch->flags |= CVMRMaddr;
      sinfo->slotAddrReg = scratch;
      
      scratchRegID = CVMRMgetRegisterNumber(scratch);
      CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE, scratchRegID,
				  arrayRegID, offset, CVMJIT_NOSETCC);
      sinfo->baseRegID = scratchRegID;
      sinfo->slotAddrOffset = 0;
    } else { /* isRefStore == CVM_FALSE */
      CVMassert(CVMCPUmemspecIsEncodableAsOpcodeSpecificImmediate(opcode, offset));
      sinfo->slotAddrOffset = offset;
      sinfo->baseRegID = arrayRegID;
      sinfo->slotAddrReg = array;
      CVMRMincRefCount(con, array);
    }
  } else {
    /* The index is not a constant but is in a register */
    CVMRMResource *scratch;
    int indexRegID;
    int scratchRegID;
    
    /* If we get here, then the index is not constant.  We will first
       compute an arrayIndex:
       subscript = base + (index << shiftAmount)
       
       Then we use an immediate offset type memspec to add the array
       header offset to the subscript to compute the actual effective
       address we want.
    */
    scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
			       CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
    scratch->flags |= CVMRMaddr;
    sinfo->slotAddrReg = scratch;
    CVMRMincRefCount(con, scratch);
    
    scratchRegID = CVMRMgetRegisterNumber(scratch);
    CVMRMpinResource(CVMRM_INT_REGS(con), sinfo->indexReg,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    indexRegID = CVMRMgetRegisterNumber(sinfo->indexReg);
    
    /* scratchReg = baseReg + (indexReg << #shiftAmount): */
    CVMCPUemitComputeAddressOfArrayElement(con, CVMCPU_ADD_OPCODE,
					   scratchRegID, arrayRegID, indexRegID, CVMCPU_SLL_OPCODE,
					   sinfo->shiftAmount);
    
    if (isRefStore) {
      /* If we get here, we're dealing with a obj ref store.  Hence,
	 we will need to fully evaluate the effective address into a
	 register and use a register offset type of memspec.  The fully
	 evaluated effective address will be used later by GC card
	 table marking code.
      */
      CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
				  scratchRegID, scratchRegID, ARRAY_DATA_OFFSET, CVMJIT_NOSETCC);
      sinfo->slotAddrOffset = 0;
    } else {
      CVMassert(CVMCPUmemspecIsEncodableAsOpcodeSpecificImmediate(opcode, ARRAY_DATA_OFFSET));
      sinfo->slotAddrOffset = ARRAY_DATA_OFFSET;
    }
    sinfo->baseRegID = scratchRegID;
  }
}

/* Purpose: Gets the baseRegID that was setup by setupScaledIndex(). */
#define getScaledIndexBaseRegID(sinfo)      ((sinfo)->baseRegID)

/* Purpose: unpin resource components of a ScaledIndexInfo */
static void
unpinScaledIndex(CVMJITCompilationContext *con, ScaledIndexInfo *sinfo)
{
    /* We are done with this ScaledIndexInfo. Get rid of it and
       its associated componenets */
    if (!sinfo->hasConstIndex) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->indexReg);
    }
    if (sinfo->slotAddrReg != NULL) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg);
    }
    if (sinfo->arrayBaseReg != NULL) {
	CVMRMunpinResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg);
    }
}

/* Purpose: unpin resource components of a ScaledIndexInfo */
static void
persistAndUnpinScaledIndex(CVMJITCompilationContext *con,
			   ScaledIndexInfo *sinfo)
{
    /* We are done with this ScaledIndexInfo. Get rid of it and
       its associated componenets */
    if (!sinfo->hasConstIndex) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->indexReg, NULL);
    }
    if (sinfo->slotAddrReg != NULL) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg,
				    NULL);
    }
    if (sinfo->arrayBaseReg != NULL) {
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg,
				    NULL);
    }
}

/* Purpose: Releases any scratch that may have been pinned by
            setupScaledIndex(). */
static void
relinquishScaledIndex(CVMJITCompilationContext *con, ScaledIndexInfo *sinfo)
{
    /* We should never decrement below 0. */
    CVMassert(CVMJITidentityGetDecorationRefCount(con, &sinfo->dec) > 0);
    CVMJITidentityDecrementDecorationRefCount(con, &sinfo->dec);
    if (CVMJITidentityGetDecorationRefCount(con, &sinfo->dec) <= 0) {
	/* We are done with this ScaledIndexInfo. Get rid of it and
	   its associated componenets */
	if (!sinfo->hasConstIndex) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->indexReg);
	}
	if (sinfo->slotAddrReg != NULL) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->slotAddrReg);
	}
	if (sinfo->arrayBaseReg != NULL) {
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), sinfo->arrayBaseReg);
	}
    } else {
	/* Merely unpin everything */
	unpinScaledIndex(con, sinfo);
    }
}

/* Purpose: Emits code to do a load of a Java array element. */
static void
indexedLoad(
    CVMJITCompilationContext* con,
    CVMJITRMContext* rc,
    CVMJITIRNodePtr fetchNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    CVMRMResource *dest;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arrayObj = popResource(con);
    CVMUint16 typeId;
    CVMJITIRNode* indexNode;
    const ArrayElemInfo* elemInfo;
    int size;
    int shiftAmount;
    int opcode;

    CVMassert(CVMJITirnodeIsFetch(fetchNode));
    indexNode = CVMJITirnodeGetLeftSubtree(fetchNode);

    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));

    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];

    size = elemInfo->size;
    shiftAmount = elemInfo->shiftAmount;
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->loadOpcode
				    : elemInfo->floatLoadOpcode;
#else
    opcode = elemInfo->loadOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment(( 
        "Do load(arrayObj, index) (elem type=%d,%c):", typeId,
	typeId == CVMJIT_TYPEID_UBYTE ?
	   'b' : CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));

    dest = CVMRMgetResource(rc, target, avoid, size);
    CVMRMpinResource(CVMRM_INT_REGS(con), arrayObj,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    setupScaledIndex(con, opcode, arrayObj, sinfo, CVM_FALSE);
    CVMCPUemitMemoryReferenceImmediate(con, opcode,
				       CVMRMgetRegisterNumber(dest),
				       getScaledIndexBaseRegID(sinfo),
				       sinfo->slotAddrOffset);

    /* We should not have set arrayObj in this sinfo instance. arrayObj
       is relinquished separately below */
    CVMassert(sinfo->arrayBaseReg == NULL);

    relinquishScaledIndex(con, sinfo);

    CVMRMoccupyAndUnpinResource(rc, dest, fetchNode);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrayObj);
    pushResource(con, dest);

}

/* Purpose: Emits code to do a load of a Java array element. */
static void
indexedAddr(
    CVMJITCompilationContext* con,
    CVMJITIRNodePtr indexNode,
    CVMRMregset target,
    CVMRMregset avoid)
{
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arrayObj = popResource(con);
    CVMUint16 typeId;
    const ArrayElemInfo* elemInfo;
    int shiftAmount;
    int opcode;
    CVMBool canBeUsedInStore;

    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));

    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];

    shiftAmount = elemInfo->shiftAmount;
    opcode = elemInfo->loadOpcode;

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment(( 
        "Compute slot &arr[index] (elem type=%c):",
	CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));
    CVMRMpinResource(CVMRM_INT_REGS(con), arrayObj,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    canBeUsedInStore = CVMJITirnodeBinaryNodeIs(indexNode, WRITE);

    /*
     * When we are setting up the scaled index, we don't know if it's
     * for a load or a store. So we conservatively set isRef, even though
     * we might not need the extra computation for a precise slot address
     * in case of a reference read (and not a write)
     */
    setupScaledIndex(con, opcode, arrayObj, sinfo,
		     canBeUsedInStore && elemInfo->isRef /* isRefStore */);

    /*
     * Remember some more in the sinfo
     */
    sinfo->arrayBaseReg = arrayObj;
    CVMRMincRefCount(con, arrayObj);

    sinfo->elemInfo = elemInfo;

    /*
     * Make sure the sinfo resource components persist across rules
     */
    persistAndUnpinScaledIndex(con, sinfo);

    /*
     * And propagate the scaledIndex part further. sinfo->slotAddrReg
     * contains the slot address.
     */
    pushScaledIndexInfo(con, sinfo);

}

#define ARRAY_LOAD_SYNTHESIS(con, p)   L_BINARY_SYNTHESIS((con), (p))
#define ARRAY_LOAD_INHERITANCE(con, p) L_BINARY_INHERITANCE((con), (p))

/* Purpose: reads a slot from an array */
static void
fetchArraySlot(CVMJITCompilationContext *con,
	       CVMJITRMContext* rc,
	       CVMJITIRNodePtr fetchNode,
	       CVMRMregset target, CVMRMregset avoid)
{
    CVMRMResource *dest;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arraySlot = sinfo->slotAddrReg;
    const ArrayElemInfo* elemInfo = sinfo->elemInfo;
    int opcode;

    CVMassert(CVMJITirnodeIsFetch(fetchNode));

#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->loadOpcode
				    : elemInfo->floatLoadOpcode;
#else
    opcode = elemInfo->loadOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    dest = CVMRMgetResource(rc, target, avoid, elemInfo->size);
    CVMRMpinResource(CVMRM_INT_REGS(con), arraySlot,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    CVMCPUemitMemoryReferenceImmediate(con, opcode,
				       CVMRMgetRegisterNumber(dest),
				       CVMRMgetRegisterNumber(arraySlot),
				       sinfo->slotAddrOffset);

    CVMRMoccupyAndUnpinResource(rc, dest, fetchNode);
    relinquishScaledIndex(con, sinfo);
    pushResource(con, dest);
}

%}

// Purpose: value = FETCH32(FIELDREF32(obj,fieldOffset))
reg32: FETCH32 FIELDREF32 regObj memSpec : 10 : 
    GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do getfield:"));
        CVMJITaddCodegenComment((con, "value{I|F}"));
        fetchField(con, CVMRM_INT_REGS(con), $$,
		   GET_REGISTER_GOALS, CVMCPU_LDR32_OPCODE, 1);
    };

// Purpose: value = FETCH64(FIELDREF64(obj,fieldOffset))
reg64: FETCH64 FIELDREF64 regObj memSpec : 10 :
    GETFIELD_SYNTHESIS(con, $$); : GETFIELD_INHERITANCE(con, $$); : : {
        CVMJITprintCodegenComment(("Do getfield:"));
        CVMJITaddCodegenComment((con, "value{L|D}"));
        fetchField(con, CVMRM_INT_REGS(con), $$,
		   GET_REGISTER_GOALS, CVMCPU_LDR64_OPCODE, 2);
    };

// Purpose: value = FETCH64(FIELDREF64VOL(obj,fieldOffset))
reg64: FETCH64 FIELDREF64VOL regObj reg32 : 90 :
    SET_AVOID_C_CALL($$); : SET_TARGET2($$, ARG1, ARG2); : : {
	CVMRMResource *fieldOffset;

	/* Note: we need to do the NULL check for this object reference because
	   the CCM helper can't do it: */
	fieldOffset = popResource(con); /* Leave the objRes on the stack. */
	doNullCheck(con, NULL, ARG1, ARG2);
	pushResource(con, fieldOffset); /* Put back the fieldOffset. */

	/* Now do the get field: */
        CVMJITprintCodegenComment(("Do volatile getfield:"));
        CVMJITaddCodegenComment((con, "call CVMCCMruntimeGetfield64Volatile"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeGetfield64Volatile"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeGetfield64Volatile);
        binaryHelper(con, (void*)CVMCCMruntimeGetfield64Volatile, $$,
		     CVM_FALSE, 2, 2);
    };

%{

#if (CVM_GCCHOICE == CVM_GC_GENERATIONAL)
/*
 * Emit barrier for write of srcReg into objReg+dataOffset=destAddr
 */
static void
emitMarkCardTable(CVMJITCompilationContext *con,
		  int objAddrReg, int destAddrReg)
{
    /* Need three registers here. One to hold zero, the other
     * to hold the card table virtual base, and a third
     * to hold the slot address being written into:
     */

    CVMCodegenComment *comment;
    CVMRMResource* cardtableReg;
    CVMRMResource* t0;
#ifdef CVM_SEGMENTED_HEAP
    CVMRMResource* t1;
    int fixupPC; /* To patch the conditional barrier branch */
    CVMCPUAddress lowerBoundAddr;
    CVMUint32 lowerBound  = (CVMUint32)CVMglobals.youngGenLowerBound;
    CVMUint32 higherBound = (CVMUint32)CVMglobals.youngGenUpperBound;
    int dstRegID;
#endif

    CVMJITpopCodegenComment(con, comment);
    t0 = CVMRMgetResource(CVMRM_INT_REGS(con), CVMRM_ANY_SET,
			  CVMRM_EMPTY_SET, 1);
#ifdef CVM_SEGMENTED_HEAP
    CVMJITprintCodegenComment(("Check if barrier required:"));

    t1 = CVMRMgetResource(CVMRM_INT_REGS(con), CVMRM_ANY_SET,
			  CVMRM_EMPTY_SET, 1);
    dstRegID = CVMRMgetRegisterNumber(t1);
    /* t1 = objRef - #lowerBound */
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, dstRegID, objAddrReg, CVM_FALSE);

    /* lower Bound can be compiled into the code */
    CVMCPUemitBinaryALUConstant(con, CVMCPU_SUB_OPCODE, dstRegID, dstRegID, lowerBound, 
				CVMJIT_NOSETCC);

    /* Compare(t1, #higherBound - #lowerBound) */
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LO, dstRegID,
			      higherBound - lowerBound);
    fixupPC = CVMJITcbufGetLogicalPC(con);
    CVMJITaddCodegenComment((con, "br skipBarrier if less than (hi-lo)"));
    /* Branch to skipBarrier if unsigned less than */
    CVMCPUemitBranch(con, 0, CVMCPU_COND_LO);
    con->inConditionalCode = CVM_TRUE;
    CVMJITprintCodegenComment(("Will do barrier:"));
    CVMJITprintCodegenComment(("Compute segment addr = BIC(obj, 0xffff)"));

    /* t1 = BIC(objAddr, 0xffff) to compute the segment address */
    /*
     * WARNING: don't let CVMCPUemitBinaryALUConstant() load a
     * large constant into a register. This will change the
     * regman state, which is a no-no in conditionally executed
     * code. Instead, manually load the large constant here
     * using CVMCPUemitLoadConstant(), which won't affect the
     * regman state.
     */
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, dstRegID, objAddrReg, CVM_FALSE);
    /* SVMC: RS, 2003-11-05:
     * Use SEGMENT_ALIGNMENT - 1 instead of 0xffff to make segment
     * size of segmented heap adjustable.
     */
    CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE, dstRegID, dstRegID,
				SEGMENT_ALIGNMENT - 1, CVMJIT_NOSETCC);
	
    CVMJITaddCodegenComment((con, "seg->cardTableVirtualBase"));
    /* And from the segment address, the barrier address: */
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
       dstRegID, dstRegID, GC_SEGMENT_CTV_OFFSET);
    cardtableReg = t1;
#else /* !CVM_SEGMENTED_HEAP case below */

    /* Easy to compute for the non-segmented case */
    CVMJITsetSymbolName((con, "cardTableVirtualBase"));
    cardtableReg =
        CVMRMgetResourceForConstant32(CVMRM_INT_REGS(con),
	    CVMRM_ANY_SET, CVMRM_EMPTY_SET,
            (CVMInt32)CVMglobals.gc.cardTableVirtualBase);
    /* Make sure we get these constants right: */
    CVMassert(CVMRMgetRegisterNumber(cardtableReg) != -1);
#endif /* CVM_SEGMENTED_HEAP */
    /* Now we are ready to write into card table, as well as
       perform the actual write: */

    /* Do card table write: */
    CVMJITaddCodegenComment((con, "mark card table"));
    CVMCPUemitArrayElementStore(con, CVMCPU_STR8_OPCODE,
        CARD_DIRTY_BYTE, CVMRMgetRegisterNumber(cardtableReg),
        destAddrReg, CVMCPU_SRL_OPCODE, CVM_GENGC_CARD_SHIFT,
        CVM_TRUE, CVMRMgetRegisterNumber(t0));

#ifdef CVM_SEGMENTED_HEAP
    CVMtraceJITCodegen(("skipBarrier:"));
    CVMJITfixupAddress(con, fixupPC, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    con->inConditionalCode = CVM_FALSE;

#endif

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), t0);

    /* this is actually t1 */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), cardtableReg);

    CVMJITpushCodegenComment(con, comment);
}

/*
 * Given an evaluated FIELDREF's components on the stack, return the
 * object address portion
 */
static CVMRMResource*
extractObjectAddrFromFieldRef(CVMJITCompilationContext* con)
{
    CVMCPUALURhs* rhs = popALURhs(con);
    CVMRMResource* lhs = popResource(con);
    pushResource(con, lhs);
    pushALURhs(con, rhs);
    return lhs;
}

#else
#define emitMarkCardTable(con, destAddrReg)

/* Currently, the JIT code assumes that our GC choice is generational,
   since we have to emit a card table write barrier. So enforce this
   using an cpp error if the GC choice is not as expected. */
#error The dynamic compiler only works with generational GC
#endif

#define PUTFIELD_SYNTHESIS(con, thisNode) \
    L_BINARY_R_UNARY_SYNTHESIS((con), (thisNode))
#define PUTFIELD_INHERITANCE(con, thisNode) \
    L_BINARY_R_UNARY_INHERITANCE((con), (thisNode))

%}

// Purpose: ASSIGN(FIELDREFOBJ(obj,fieldOffset), valueRef)
root: ASSIGN FIELDREFOBJ regObj aluRhs regObj : 10 :
    PUTFIELD_SYNTHESIS(con, $$); : PUTFIELD_INHERITANCE(con, $$); : : {
        CVMRMResource *src = popResource(con);
        CVMRMResource *fieldAddr;
	CVMRMResource *objAddr;
        int srcRegID, fieldAddrRegID, objAddrRegID;

        CVMJITprintCodegenComment(("Do putfield:"));

        /* Compute the address of the field: */
        CVMJITaddCodegenComment((con, "fieldAddr = obj + fieldOffset;"));
	objAddr = extractObjectAddrFromFieldRef(con);
	CVMRMincRefCount(con, objAddr); /* One more use for the barrier */
        wordRegRegOp(con, CVMCPU_ADD_OPCODE, CVMJITirnodeGetLeftSubtree($$),
			CVMRM_ANY_SET, CVMRM_EMPTY_SET);

        /* Emit write along with barrier for reference writes: */
        fieldAddr = popResource(con);
        CVMRMpinResource(CVMRM_INT_REGS(con), src,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), fieldAddr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        CVMRMpinResource(CVMRM_INT_REGS(con), objAddr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        srcRegID = CVMRMgetRegisterNumber(src);
        fieldAddrRegID = CVMRMgetRegisterNumber(fieldAddr);
        objAddrRegID = CVMRMgetRegisterNumber(objAddr);

        CVMJITaddCodegenComment((con, "putfield(fieldAddr, valueObj);"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE, srcRegID,
                                           fieldAddrRegID, 0);
        emitMarkCardTable(con, objAddrRegID, fieldAddrRegID);

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), objAddr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), fieldAddr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
    };

%{

/* Purpose: Emits code to do */
static void
emitArrayAssignabilityCheck(CVMJITCompilationContext* con,
                            CVMRMResource *aref, CVMRMResource *src)
{
    CVMRMResource* arrClass; /* Scratch register */
    CVMRMResource* rhsClass; /* Scratch register */
    int srcRegID;
    int arefRegID;
    CVMCPUCondCode condCode;
#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    int fixupPC;
#endif

    /* This is an 'aastore'. Do the assignability check */
    CVMJITaddCodegenComment((con, "aastore assignability check"));
    arrClass = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
				        CVMCPU_ARG3_REG, 1);
    arrClass->flags |= CVMRMaddr;
    rhsClass = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_ARG4_REG, 1);
    rhsClass->flags |= CVMRMaddr;

#ifdef CVM_JIT_HAVE_CALLEE_SAVED_REGISTER
    /* Get the rhs of the assignment and the array reference
       into registers. Do not clobber ARG1-ARG4 */
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), src,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);
    srcRegID = CVMRMgetRegisterNumber(src);
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), aref,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);
    /*
     * We might eventually do a call. Assume we will
     */
    CVMRMminorSpill(con, ARG3|ARG4);
#else
    CVMRMpinResource(CVMRM_INT_REGS(con), src,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), aref,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    srcRegID  = CVMRMgetRegisterNumber(src);
    arefRegID = CVMRMgetRegisterNumber(aref);
#endif
    /* OK, first off, check for the rhs being NULL. NULL is assignable
       to any array of references. */
    /* Compare to NULL, and skip assignability check if NULL */
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
		      srcRegID, CVMCPUALURhsTokenConstZero);

    /*
     * Call the helper after loading the cb of the array class and
     * rhs class into ARG3 and ARG4 (with the lower bits still set).
     * This is all done conditionally based on whether or not
     * the object is NULL.
     */

#ifndef CVM_JIT_HAVE_CALLEE_SAVED_REGISTER
    /* we use the assigned regisers arefRegID/srcRegID after unpinning!!! 
     * NOTE: This is not the normal way to exercise unpinning of registers and
     * usage of reg IDs.  This particular instance is a special case due
     * a special need.  This pattern of unpinning and usage is dangerous and is
     * used here only after careful consideration to make sure that it does not
     * break anything.  This pattern should not be used elsewhere if avoidable,
     * and usage should be subject to careful review by experts of this system.
     */
    CVMRMunpinResource(CVMRM_INT_REGS(con), src);
    CVMRMunpinResource(CVMRM_INT_REGS(con), aref);
    /*
     * We might eventually do a call. Assume we will
     */
    CVMRMminorSpill(con, ARG3|ARG4);
#endif

#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    condCode = CVMCPU_COND_AL;
    fixupPC = CVMJITcbufGetLogicalPC(con);
    CVMJITaddCodegenComment((con, "br .skip"));
    CVMCPUemitBranch(con, 0, CVMCPU_COND_EQ);
    con->inConditionalCode = CVM_TRUE;
#else
    condCode = CVMCPU_COND_NE;
#endif

    /* LDRNE ARG3, [robj] */
    CVMJITaddCodegenComment((con, "Get the array class"));
    CVMCPUemitMemoryReferenceImmediateConditional(con, CVMCPU_LDR32_OPCODE,
        CVMCPU_ARG3_REG, arefRegID, OBJECT_CB_OFFSET,
        condCode);

    /* LDRNE ARG4, [src] */
    CVMJITaddCodegenComment((con, "Get the rhs class"));
    CVMCPUemitMemoryReferenceImmediateConditional(con, CVMCPU_LDR32_OPCODE,
        CVMCPU_ARG4_REG, srcRegID, OBJECT_CB_OFFSET, condCode);

    /* do helper call */
    CVMJITaddCodegenComment((con, "call %s",
                             "CVMCCMruntimeCheckArrayAssignableGlue"));
    CVMJITsetSymbolName((con,"CVMCCMruntimeCheckArrayAssignableGlue"));
    CVMJITstatsRecordInc(con,
        CVMJIT_STATS_CVMCCMruntimeCheckArrayAssignable);

    CVMCPUemitAbsoluteCallConditional(con,
        (void*)CVMCCMruntimeCheckArrayAssignableGlue,
        CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, condCode);
    CVMJITcaptureStackmap(con, 0);

#if !defined(CVMCPU_HAS_CONDITIONAL_LOADSTORE_INSTRUCTIONS) || !defined(CVMCPU_HAS_CONDITIONAL_CALL_INSTRUCTIONS)
    CVMtraceJITCodegen((".skip"));
    CVMJITfixupAddress(con, fixupPC, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
    con->inConditionalCode = CVM_FALSE;
#endif

    /* And we are at done */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrClass);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhsClass);

    CVMJITprintCodegenComment(("aastore assignment check done"));
    CVMRMunpinResource(CVMRM_INT_REGS(con), aref);
    CVMRMunpinResource(CVMRM_INT_REGS(con), src);
}

/* Purpose: Emits code to do a store of a Java array element. */
static void
indexedStore(
    CVMJITCompilationContext* con,
    CVMJITRMContext* rc,
    CVMJITIRNodePtr thisNode)
{
    CVMBool isRefStore;
    CVMRMResource *src  = popResource(con);
    int srcRegID, arrayRegID;
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *aref = popResource(con);

    CVMUint16 typeId;
    CVMJITIRNode* indexNode;
    const ArrayElemInfo* elemInfo;
    int size;
    int shiftAmount;
    int opcode;

    indexNode = CVMJITirnodeGetLeftSubtree(thisNode);

    /* Make sure we are getting the correct tree shape */
    CVMassert(CVMJITirnodeIsIndex(indexNode));

    typeId = CVMJITirnodeGetBinaryOp(indexNode)->data;
    elemInfo = &typeidToArrayElemInfo[typeId];

    size = elemInfo->size;
    shiftAmount = elemInfo->shiftAmount;
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->storeOpcode
				    : elemInfo->floatStoreOpcode;
#else
    opcode = elemInfo->storeOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMassert(sinfo->shiftAmount == -1);
    sinfo->shiftAmount = shiftAmount;
    
    CVMJITprintCodegenComment(( 
        "Do store(arrayObj, index) (elem type=%c):",
	CVMbasicTypeSignatures[CVMterseTypeBasicTypes[typeId]]));

    CVMassert(elemInfo->isRef == CVMJITirnodeIsReferenceType(thisNode));
    isRefStore = CVMJITirnodeIsReferenceType(thisNode);
    if (isRefStore) {
        emitArrayAssignabilityCheck(con, aref, src);
    }

    CVMRMpinResourceStrict(rc, src, CVMCPU_ARGREGS, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), aref, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    srcRegID = CVMRMgetRegisterNumber(src);
    arrayRegID = CVMRMgetRegisterNumber(aref);

    {
        int baseRegID;
        setupScaledIndex(con, opcode, aref, sinfo, isRefStore);
        baseRegID = getScaledIndexBaseRegID(sinfo);
        CVMCPUemitMemoryReferenceImmediate(con, opcode, srcRegID, baseRegID,
					   sinfo->slotAddrOffset);
	CVMRMrelinquishResource(rc, src); 

        if (isRefStore) {
            CVMassert(sinfo->slotAddrOffset == 0);
            emitMarkCardTable(con, arrayRegID, baseRegID);
        }
	/* We should not have set arrayObj in this sinfo instance. arrayObj
	   is relinquished separately below */
        CVMassert(sinfo->arrayBaseReg == NULL);

        relinquishScaledIndex(con, sinfo);
    }

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), aref);
}

/* Purpose: reads a slot from an array */
static void
storeArraySlot(CVMJITCompilationContext *con,
	       CVMJITRMContext* rc,
	       CVMJITIRNodePtr thisNode)
{
    CVMRMResource* rhs = popResource(con);
    ScaledIndexInfo *sinfo = popScaledIndexInfo(con);
    CVMRMResource *arraySlot = sinfo->slotAddrReg;
    CVMRMResource *arrayRef;
    int opcode;
    const ArrayElemInfo* elemInfo = sinfo->elemInfo;

    arrayRef = sinfo->arrayBaseReg;
    if (elemInfo->isRef) {
	emitArrayAssignabilityCheck(con, arrayRef, rhs);
    }
#ifdef CVM_JIT_USE_FP_HARDWARE
    opcode = (rc == CVMRM_INT_REGS(con)) ? elemInfo->storeOpcode
				    : elemInfo->floatStoreOpcode;
#else
    opcode = elemInfo->storeOpcode;
#endif
    CVMassert(opcode != CVM_ILLEGAL_OPCODE);

    CVMRMpinResource(CVMRM_INT_REGS(con), arraySlot,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    /* In case of byte arrays we must get a register which is byte
       accessible here. */
    CVMRMpinResourceStrict(rc, rhs, CVMCPU_ARGREGS, CVMRM_EMPTY_SET);
    CVMCPUemitMemoryReferenceImmediate(con, opcode,
				       CVMRMgetRegisterNumber(rhs),
				       CVMRMgetRegisterNumber(arraySlot),
				       sinfo->slotAddrOffset);
    if (elemInfo->isRef) {
	CVMassert(sinfo->slotAddrOffset == 0);
	CVMRMpinResource(CVMRM_INT_REGS(con), arrayRef,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	emitMarkCardTable(con,
			  CVMRMgetRegisterNumber(arrayRef),
			  CVMRMgetRegisterNumber(arraySlot));
	CVMRMunpinResource(CVMRM_INT_REGS(con), arrayRef);
    }

    relinquishScaledIndex(con, sinfo);
    CVMRMrelinquishResource(rc, rhs);
}

#define ARRAY_STORE_SYNTHESIS(con, thisNode) \
    L_BINARY_R_UNARY_SYNTHESIS((con), (thisNode))
#define ARRAY_STORE_INHERITANCE(con, thisNode) \
    L_BINARY_R_UNARY_INHERITANCE((con), (thisNode))

%}

%{

static CVMRMResource*
invokeMethod(CVMJITCompilationContext *con, CVMJITIRNodePtr invokeNode)
{
    /*
     * Safe set is the empty set.
     * To fix, we would need to save and restore registers to
     * the Java stack (caller or callee frame) on transitions.
     */
    CVMRMregset safeSet = CVMRM_EMPTY_SET;
    CVMRMResource *mbptr = popResource(con);
    CVMUint16 numberOfArgs = CVMJITirnodeGetBinaryOp(invokeNode)->data;

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument */
    CVMSMadjustJSP(con);
#endif


    /* mov  a1, mb */
    mbptr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), mbptr,
				     CVMCPU_ARG1_REG);

    CVMRMmajorSpill(con, ARG1, safeSet);

    /*
     * emit indirect call to caller
     */
    CVMCPUemitInvokeMethod(con);

    CVMJITcaptureStackmap(con, numberOfArgs);
    CVMSMpopParameters(con, numberOfArgs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbptr);

    /* Dump the constant pool here after a method invocation if we need to: */
    CVMX86emitConstantPoolDumpWithBranchAroundIfNeeded(con);

    /* Bind the result resource to the invokeNode: */
    return CVMSMinvocation(con, invokeNode);
}

%}



//
// Intrinsic Method invocation.

%{

#ifdef CVMJIT_INTRINSICS
#ifdef CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER
static CVMRMregset
pinIntrinsicArgs(CVMJITCompilationContext *con,
                 CVMCPUCallContext *callContext,
                 CVMJITIRNodePtr intrinsicNode,
                 CVMJITIntrinsic *irec)
{
    CVMRMregset outgoingRegs = CVMRM_EMPTY_SET;
    int numberOfArgs = irec->numberOfArgs;

    /* Pin the args to respective registers: */
    if (numberOfArgs != 0) {
        struct CVMJITStackElement* sp;
        int i,j;
        con->cgsp -= numberOfArgs;
        sp = con->cgsp + 1;
        /* Pin the high args first because they may need to be written to
           the stack and use scratch registers while they are at it.  The
           low args will get pinned into arg registers: */
        for (i = numberOfArgs - 1; i >= 0; i--) {
            CVMJITIRNode *iargNode = CVMJITirnodeGetLeftSubtree(intrinsicNode);
            CVMRMResource *arg = sp[i].u.r;
            for (j = 0; j < i; j++) {
                iargNode = CVMJITirnodeGetRightSubtree(iargNode);
            }
            arg = CVMCPUCCALLpinArg(con, callContext, arg,
                        CVMJITgetTypeTag(iargNode),
                        CVMJIT_IARG_ARG_NUMBER(iargNode),
                        CVMJIT_IARG_WORD_INDEX(iargNode),
                        &outgoingRegs);
            sp[i].u.r = arg;
        }
/*        CVMassert(iargNode->tag == CVMJIT_ENCODE_NULL_IARG); */
    }
    return outgoingRegs;
}

static void
relinquishIntrinsicArgs(CVMJITCompilationContext *con,
                        CVMCPUCallContext *callContext,
                        CVMJITIRNodePtr intrinsicNode,
                        CVMJITIntrinsic *irec)
{
    int numberOfArgs = irec->numberOfArgs;

    /* Pin the args to respective registers: */
    if (numberOfArgs != 0) {
        CVMJITIRNode *iargNode = CVMJITirnodeGetLeftSubtree(intrinsicNode);
        struct CVMJITStackElement* sp;
        int i;
        sp = con->cgsp + 1;
        for (i = 0; i < numberOfArgs; i++) {
            CVMRMResource *arg = sp[i].u.r;
            CVMCPUCCALLrelinquishArg(con, callContext, arg,
                CVMJITgetTypeTag(iargNode), CVMJIT_IARG_ARG_NUMBER(iargNode),
                CVMJIT_IARG_WORD_INDEX(iargNode));
            iargNode = CVMJITirnodeGetRightSubtree(iargNode);
        }
        CVMassert(iargNode->tag == CVMJIT_ENCODE_NULL_IARG);
    }
}

static void
extendSmallTypeTo32Bits(CVMJITCompilationContext *con, int reg, char sig);

static void
invokeIntrinsicMethod(CVMJITCompilationContext *con,
                      CVMJITIRNodePtr intrinsicNode)
{
    CVMUint16 intrinsicID = CVMJITirnodeGetBinaryOp(intrinsicNode)->data;
    CVMJITIntrinsic *irec = &CVMglobals.jit.intrinsics[intrinsicID - 1];
    const CVMJITIntrinsicConfig *config = irec->config;
    CVMUint16 properties = config->properties;

    CVMJITprintCodegenComment(("Invoke INTRINSIC %C.%M:",
                                CVMmbClassBlock(irec->mb), irec->mb));
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument */
    CVMSMadjustJSP(con);
#endif

    if ((properties & CVMJITINTRINSIC_OPERATOR_ARGS) != 0) {
        const CVMJITIntrinsicEmitterVtbl *emitter;
        /* NOTE: The emitter is responsible for popping arguments of the
           stack, and pushing any result back on the stack as well: */
        emitter = (const CVMJITIntrinsicEmitterVtbl *)
                      config->emitterOrCCMRuntimeHelper;
        emitter->emitOperator(con, intrinsicNode);

    } else {
        int rtnType = CVMJITgetTypeTag(intrinsicNode);
        CVMBool okToDumpCP, okToBranchAroundCP;
        CVMRMResource *dest = NULL;
        CVMBool useJavaStack;
        CVMRMregset outgoingRegSet = 0;
        CVMCPUCallContext callContext;
        CVMRMResource *cceeRes = NULL;

        useJavaStack = ((properties & CVMJITINTRINSIC_JAVA_ARGS) != 0);

        /* Flush the Java frame pointer to the Java stack if neccesary.  Do
           this before we pin any arguments because they can adjust the stack
           frame and hence make it difficult to compute the value of the ccee
           which may be needed for this flush operation: */
        if ((properties & CVMJITINTRINSIC_FLUSH_JAVA_STACK_FRAME) != 0) {
	    int eeReg;
#ifndef CVMCPU_EE_REG
	    CVMRMResource *eeRes =
		CVMRMgetResource(CVMRM_INT_REGS(con),
				 CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	    eeReg = CVMRMgetRegisterNumber(eeRes);
	    /* Get the ee: */
	    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
		eeReg, CVMoffsetof(CVMCCExecEnv, eeX));
#else
	    eeReg = CVMCPU_EE_REG;
#endif
	    /* Store the JFP into the stack: */
	    CVMJITaddCodegenComment((con, "flush JFP to stack"));
	    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
                CVMCPU_JFP_REG, eeReg, offsetof(CVMExecEnv, interpreterStack) +
				       offsetof(CVMStack, currentFrame));
#ifndef CVMCPU_EE_REG
	    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
        }

        /* Load the ccee arg if necessary.  Do this before we pin any other
           arguments because they can adjust the stack frame and hence make
           it difficult to compute the value of the ccee: */
        if ((properties & CVMJITINTRINSIC_ADD_CCEE_ARG) != 0) {
            cceeRes =
                CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					 CVMCPU_ARG1_REG, 1);
            CVMCPUemitLoadCCEE(con, CVMCPU_ARG1_REG);
            outgoingRegSet |= ARG1;
        }

        /* Prepare the outgoing arguments: */
        if (useJavaStack) {
            outgoingRegSet |= CVMRM_EMPTY_SET;
        } else {
            CVMCPUCCALLinitArgs(con, &callContext, irec, CVM_FALSE);
            outgoingRegSet |= pinIntrinsicArgs(con, &callContext,
                                               intrinsicNode, irec);
        }

        /* Do spills if necessary: */
        if ((properties & CVMJITINTRINSIC_NEED_MINOR_SPILL) != 0) {
            CVMRMminorSpill(con, outgoingRegSet);
        } else if ((properties & CVMJITINTRINSIC_NEED_MAJOR_SPILL) != 0) {
            CVMRMmajorSpill(con, outgoingRegSet, CVMRM_SAFE_SET);
        }

        okToDumpCP = ((properties & CVMJITINTRINSIC_CP_DUMP_OK) != 0);
        okToBranchAroundCP = okToDumpCP &&
            ((properties & CVMJITINTRINSIC_NEED_STACKMAP) == 0);

        CVMJITaddCodegenComment((con,
            "%C.%M helper", CVMmbClassBlock(irec->mb), irec->mb));
        if ((properties & CVMJITINTRINSIC_FLUSH_JAVA_STACK_FRAME) != 0) {
            /* Emit call to intrinsic helper: */
            CVMCPUemitFlushJavaStackFrameAndAbsoluteCall_CCode(con,
                config->emitterOrCCMRuntimeHelper,
                okToDumpCP, okToBranchAroundCP, irec->numberOfArgsWords);
        } else {
            /* Emit call to intrinsic helper: */

	  CVMCPUemitAbsoluteCall(con, config->emitterOrCCMRuntimeHelper, okToDumpCP, 
				 okToBranchAroundCP, irec->numberOfArgsWords);
        }

        /* Capture stackmap if necessary: */
        if ((properties & CVMJITINTRINSIC_NEED_STACKMAP) != 0) {
            /* NOTE: Intrinsic helpers are always frameless.  Hence, the
               argSize we pass to CVMJITcaptureStackmap() is always 0 i.e.
               the caller is always responsible for scanning the args. */
            CVMJITcaptureStackmap(con, 0);
        }

        /* Release the ccee arg if necessary: */
        if ((properties & CVMJITINTRINSIC_ADD_CCEE_ARG) != 0) {
            CVMRMrelinquishResource(CVMRM_INT_REGS(con), cceeRes);
        }

        /* Setup the result resource if appropriate: */
        if (useJavaStack) {
            /* Pop the arguments of the Java stack: */
            CVMSMpopParameters(con, CVMmbArgsSize(irec->mb));
            /* Bind the result resource to the intrinsicNode: */
            dest = CVMSMinvocation(con, intrinsicNode);
            if (rtnType != CVM_TYPEID_VOID) {
                pushResource(con, dest);
            } else {
                /* No resource will be bound for a void return type: */
                CVMassert(dest == NULL);
            }
        } else {
            if (rtnType != CVM_TYPEID_VOID) {
                int resultWords = ((rtnType == CVM_TYPEID_LONG) ||
                                   (rtnType == CVM_TYPEID_DOUBLE)) ? 2 : 1;
                                   

		  /* SVMC_JIT 15062004 d022185(HS)
                   * Currently intrinsic fcts only work with int registers,
		   * but FP results are stored in %f0, not %o0 on sparcv9.
                   * We have to move the value from the used place to the estimated one.
		   * The correct solution needs major changes in the intrinsic handling,
                   * which sun should do itself, hopefully.
		   */
#if defined(CVM_JIT_USE_FP_HARDWARE)
                if(rtnType == CVM_TYPEID_DOUBLE || rtnType == CVM_TYPEID_FLOAT) {
                  CVMRMResource* dest1 = CVMRMgetResourceSpecific(CVMRM_FP_REGS(con),
						CVMCPU_FPRESULT1_REG,
                                                resultWords);
                  CVMRMoccupyAndUnpinResource(CVMRM_FP_REGS(con), dest1, intrinsicNode); 
                  dest = CVMRMcloneResource(CVMRM_FP_REGS(con),dest1,CVMRM_INT_REGS(con),1U<<CVMCPU_RESULT1_REG,CVMRM_EMPTY_SET);
		  CVMRMrelinquishResource(CVMRM_FP_REGS(con),dest1);
                } else
#endif
                {
                dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
						CVMCPU_RESULT1_REG,
                                                resultWords);
                /* This code used to be just emitted for MS compiler. 
                 * However, newer gcc's also can put 8/16 bit return 
                 * values into 8/16 bit register so we need to  
                 * extend correctly. 
                 */ 
                /* The microsoft compiler only uses as little register
                 * space (al, ax, eax, edx:eax) as necessary to store
                 * the return value.  So we have to pad types less
                 * than 32 bits.
                 */
		extendSmallTypeTo32Bits(con, CVMRMgetRegisterNumber(dest), 
					config->methodSignature[strlen(config->methodSignature)-1]);
                }
                CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con),
					    dest, intrinsicNode);
            }
            relinquishIntrinsicArgs(con, &callContext, intrinsicNode, irec);
            CVMCPUCCALLdestroyArgs(con, &callContext, irec, CVM_FALSE);
            /* NOTE: We cannot must not push the dest resource until after
                     we relinquish the iargs.  This is because we're relying
                     on the stack to still hold the pointers to the iarg
                     resources.  Pushing the dest resource before we
                     relinquish the iargs would cause the pointer to the first
                     iarg resource to be trashed.
            */
            if (rtnType != CVM_TYPEID_VOID) {
                pushResource(con, dest);
            }
        }
    }
}
#endif /* CVM_NEED_INVOKE_INTRINSIC_METHOD_HELPER */
#endif /* CVMJIT_INTRINSICS */

%}

// Purpose: mb = FETCH_MB_FROM_VTABLE(GET_VTBL(object), methodOffset)
regAddr: FETCH_MB_FROM_VTABLE GET_VTBL regObj memSpec : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
        CVMCPUMemSpec *vtblIndex = popMemSpec(con);
	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* vtblBase =
	   CVMRMgetResource(CVMRM_INT_REGS(con),
			    CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	CVMRMResource* dest;

        CVMJITprintCodegenComment(("Fetch mb from vtable:"));
	CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);

	/* LDR rv, [robj] */
        CVMJITaddCodegenComment((con, "Get object.cb"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(vtblBase), CVMRMgetRegisterNumber(objPtr),
            OBJECT_CB_OFFSET);

	/* Mask off special bits from the class pointer */
	/* BIC rv, rv, #3 */
        CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
				    CVMRMgetRegisterNumber(vtblBase),
				    CVMRMgetRegisterNumber(vtblBase), 0x3,
				    CVMJIT_NOSETCC);

	/* Now find the vtable pointer in the classblock */
	/* LDR rv, [rv + CB_VTBL_OFF] */
        CVMJITaddCodegenComment((con, "Get cb.vtbl"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(vtblBase),
            CVMRMgetRegisterNumber(vtblBase), CB_VTBL_OFFSET);

	/* Now vtblBase holds the vtable pointer. Do the rest. */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
	dest = CVMRMgetResource(CVMRM_INT_REGS(con), GET_REGISTER_GOALS, 1);

        /* LDR rdest, [rv + vtblIndex*4] */
        CVMJITaddCodegenComment((con, "method = cb.vtbl[methodIdx]"));
        CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), vtblIndex, CVMRM_ANY_SET,
				 CVMRM_EMPTY_SET);
        CVMCPUemitMemoryReference(con, CVMCPU_LDR32_OPCODE,
            CVMRMgetRegisterNumber(dest), CVMRMgetRegisterNumber(vtblBase),
            CVMCPUmemspecGetToken(con, vtblIndex));
        CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), vtblIndex);

	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), vtblBase,
		CVMJITirnodeGetLeftSubtree($$));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), vtblBase);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

// Purpose: MB_TEST_OUTOFLINE(objMB, candidateMB)
root: MB_TEST_OUTOFLINE regAddr aluRhs : 30 : : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    CVMCPUALURhs* candidateMB = popALURhs(con);
    CVMRMResource* mb = popResource(con);

    CVMRMpinResource(CVMRM_INT_REGS(con), mb,
		     CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUalurhsPinResource(CVMRM_INT_REGS(con),
                            CVMCPU_CMP_OPCODE, candidateMB,
                            CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    /* cmp mb, candidateMB */
    CVMCPUemitCompare(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_NE,
                      CVMRMgetRegisterNumber(mb),
                      CVMCPUalurhsGetToken(con, candidateMB));

    /* branch back to the inlined method if the MB test pass */
    CVMCPUemitBranch(con,
        con->currentCompilationBlock->oolReturnAddress, CVMCPU_COND_EQ);

    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), candidateMB);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), mb);

    /* No longer need to restrict register usage within the block
     * after this point.
     */
    CVMRMremoveRegSandboxRestriction(CVMRM_INT_REGS(con),
                                     con->currentCompilationBlock);

    CVMJITcsBeginBlock(con);	
#endif
    };

// Purpose: cb = FETCH_VCB(object)
regAddr: FETCH_VCB regObj : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
  	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* dest;

        CVMJITprintCodegenComment(("Fetch vcb:"));
        CVMRMpinResource(CVMRM_INT_REGS(con), objPtr,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	dest =	CVMRMgetResource(CVMRM_INT_REGS(con),
                                 GET_REGISTER_GOALS, 1);

	/* LDR rv, [robj] */
        CVMJITaddCodegenComment((con, "Get object.cb"));
        CVMJITcsSetExceptionInstruction(con);
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
                                           CVMRMgetRegisterNumber(dest),
                                           CVMRMgetRegisterNumber(objPtr),
                                           OBJECT_CB_OFFSET);

 	/* Mask off special bits from the class pointer */
	/* BIC rv, rv, #3 */
        CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE,
				    CVMRMgetRegisterNumber(dest),
				    CVMRMgetRegisterNumber(dest), 0x3,
				    CVMJIT_NOSETCC);

	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
#endif
    };

// FETCH_MB_FROM_VTABLE_OUTOFLINE(cb, methodOffset)
regAddr: FETCH_MB_FROM_VTABLE_OUTOFLINE regAddr memSpec : 30 :
    SET_AVOID_METHOD_CALL($$); : : : {
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
        CVMCPUMemSpec *vtblIndex = popMemSpec(con);
        CVMRMResource* cb;
        CVMRMResource* dest;
#ifdef VOFFMEMSPEC
#ifdef CVM_DEBUG_ASSERTS	
        CVMAddr fixupAddress  = popAddress(con);
#else
        popAddress(con);
#endif
#endif
        /* The incoming CB should not be trashed because
         * it comes from phi, which is not part of the reserved
         * registers.
         */
        cb = popResource(con);

        CVMJITprintCodegenComment(("Fetch mb from vtable for outofline:"));
        CVMRMpinResource(CVMRM_INT_REGS(con), cb,
                         CVMRM_ANY_SET, CVMRM_EMPTY_SET);
        dest = CVMRMgetResource(CVMRM_INT_REGS(con),
                                CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);

	/* Now find the vtable pointer in the classblock */
	/* LDR rVTBL, [rCB + CB_VTBL_OFF] */
        CVMJITaddCodegenComment((con, "Get cb.vtbl"));
        CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
                                           CVMRMgetRegisterNumber(dest),
                                           CVMRMgetRegisterNumber(cb),
                                           CB_VTBL_OFFSET);

        /* LDR rDest, [rVTBL + vtblIndex*4] */
        CVMJITaddCodegenComment((con, "method = cb.vtbl[methodIdx]"));
        CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), vtblIndex, 
			         CVMRM_ANY_SET, CVMRM_EMPTY_SET);

        CVMCPUemitMemoryReference(con, CVMCPU_LDR32_OPCODE,
                                  CVMRMgetRegisterNumber(dest), 
                                  CVMRMgetRegisterNumber(dest),
                                  CVMCPUmemspecGetToken(con, vtblIndex));

        CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), vtblIndex);
#ifdef VOFFMEMSPEC
	CVMassert(fixupAddress == 0);
#endif

        CVMRMrelinquishResource(CVMRM_INT_REGS(con), cb);
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
#endif 
    };

// Purpose: mb = FETCH_MB_FROM_ITABLE(GET_ITBL(object), methodBlock)
regAddr: FETCH_MB_FROM_ITABLE GET_ITBL regObj regAddr : 30 :
    SET_AVOID_C_CALL($$); : SET_TARGET2($$, ARG2, ARG3); : : {
        CVMRMResource* mbPtr = popResource(con);
	CVMRMResource* objPtr = popResource(con);
	CVMRMResource* dest;

        objPtr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), objPtr,
					  CVMCPU_ARG2_REG);
        mbPtr = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), mbPtr,
					 CVMCPU_ARG3_REG);
        CVMRMmajorSpill(con, ARG2|ARG3, CVMRM_SAFE_SET);
	dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMJITprintCodegenComment(("Fetch mb from itable:"));

	/* call the helper glue */
        CVMJITaddCodegenComment((con,
				 "call CVMCCMruntimeLookupInterfaceMBGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeLookupInterfaceMBGlue"));
        CVMJITstatsRecordInc(con,
            CVMJIT_STATS_CVMCCMruntimeLookupInterfaceMB);
        CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeLookupInterfaceMBGlue,
                               CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);

	/* reserve a word for the guess method */
        CVMJITaddCodegenComment((con, "interface lookup guess"));
        CVMJITemitWord(con, 0);
	CVMJITcaptureStackmap(con, 0);

	/* Return value is in RESULT1 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
        CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbPtr);
	pushResource(con, dest);
    };

parameters: NULL_PARAMETER : 0 : : : : ;
parameters: PARAMETER32 param32 parameters : 0 : : : : ;
parameters: PARAMETER64 param64 parameters : 0 : : : : ;

// SVMC_JIT sw 2004-02-27: empty effect used for eliminating checkinits
effect: ICONST_32: 0 : : : : {
    };

//
// a comparison does two things:
// generates code to set the condition codes, and also emits
// the conditional branch instruction if requested.
//

%{
#define compareAddrcc compare32cc

static void
compare32cc(CVMJITCompilationContext *con,
	    CVMJITIRNodePtr thisNode, int opcode)
{
    CVMCPUALURhs* rhs = popALURhs(con);
    CVMRMResource* lhs = popResource(con);
    CVMJITConditionalBranch* branch = CVMJITirnodeGetCondBranchOp(thisNode);
    CVMJITIRBlock* target = branch->target;
    CVMCPUCondCode condCode = mapCondCode(branch->condition);
#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    CVMRMregSandboxResources* sandboxRes = NULL;
#endif

#ifndef CVMCPU_HAS_COMPARE
    /* pin before calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMCPUalurhsPinResource(CVMRM_INT_REGS(con), opcode, rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    switch(rhs->type)
    {
    case CVMCPU_ALURHS_CONSTANT:
      CVMCPUemitCompareConstant(con, opcode, condCode, CVMRMgetRegisterNumber(lhs), rhs->constValue);
      break;
    case CVMCPU_ALURHS_REGISTER:
      CVMCPUalurhsPinResource(CVMRM_INT_REGS(con), opcode, rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
      CVMCPUemitCompareRegister(con, opcode, condCode, CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs->r));
      break;
    default: 
      CVMpanic("unknown ALURhs type");
    }
    CVMRMsynchronizeJavaLocals(con);
#ifdef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), rhs);
    /* pin after calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif

#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    if(target->mtIndex != CVMJIT_IAI_VIRTUAL_INLINE_CB_TEST_DEFAULT) {
        /* reserve registers for outofline MB test */
        sandboxRes = CVMRMgetRegSandboxResources(
            CVMRM_INT_REGS(con), target,
            CVMRM_ANY_SET, CVMRM_EMPTY_SET,
            CVMCPU_VIRTUAL_INLINE_OUTOFLINE_MB_TEST_REG_NUM);
    }
#endif

    branchToBlock(con, condCode, branch->target);

#ifdef IAI_VIRTUAL_INLINE_CB_TEST
    if(target->mtIndex != CVMJIT_IAI_VIRTUAL_INLINE_CB_TEST_DEFAULT) {
        CVMJITcsBeginBlock(con);
	target->oolReturnAddress = CVMJITcbufGetLogicalPC(con);
        /* relinquish the sandbox resources */
        CVMRMrelinquishRegSandboxResources(CVMRM_INT_REGS(con),
                                           sandboxRes);
    }
#endif

#ifndef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMCPUalurhsRelinquishResource(CVMRM_INT_REGS(con), rhs);
#endif
    CVMRMunpinAllIncomingLocals(con, target);
}

static void 
compare32_reg_fieldref(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode, int opcode, CVMBool memopnd_on_rhs)
{
  CVMCPUAddress addr;
  CVMCPUMemSpec *fieldOffset;
  CVMRMResource *objPtr;
  CVMRMResource* regopnd;
  CVMJITConditionalBranch* branch;
  CVMCPUCondCode condCode;

  if (memopnd_on_rhs == CVM_TRUE) {
    fieldOffset = popMemSpec(con);
    objPtr = popResource(con);
    regopnd = popResource(con);
  }
  else {
    regopnd = popResource(con);
    fieldOffset = popMemSpec(con);
    objPtr = popResource(con);
  }
  branch = CVMJITirnodeGetCondBranchOp(thisNode);
  condCode = mapCondCode(branch->condition);

  CVMRMpinResource(CVMRM_INT_REGS (con), objPtr, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
  CVMJITaddCodegenComment((con, "= getfield(obj, fieldIdx);"));
  CVMCPUmemspecPinResource(CVMRM_INT_REGS(con), fieldOffset, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
  CVMRMpinResource(CVMRM_INT_REGS(con), regopnd, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
  CVMCPUinit_Address_base_memspec(&addr, CVMRMgetRegisterNumber(objPtr), fieldOffset);
  if (memopnd_on_rhs == CVM_TRUE) {
    CVMCPUemitCompareRegisterMemory(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ/* dummy */, 
				    CVMRMgetRegisterNumber(regopnd), &addr);
  }
  else {
    CVMCPUemitCompareMemoryRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ/* dummy */, 
				    &addr, CVMRMgetRegisterNumber(regopnd));
  }
  CVMRMsynchronizeJavaLocals(con);
  CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
  branchToBlock(con, condCode, branch->target);
  CVMRMunpinAllIncomingLocals(con, branch->target);

  CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), fieldOffset);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), objPtr);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), regopnd);
}

static void
compare32_reg_static(CVMJITCompilationContext *con,  CVMJITIRNodePtr thisNode, int opcode,
		     CVMBool memopnd_on_rhs)
{
  CVMCPUAddress addr;
  CVMCPUMemSpec *staticField;
  CVMRMResource *regopnd;
  CVMJITConditionalBranch* branch;
  CVMCPUCondCode condCode;

  if (memopnd_on_rhs == CVM_TRUE) {
    staticField = popMemSpec(con);
    regopnd = popResource(con);
  } else {
    regopnd = popResource(con);
    staticField = popMemSpec(con);
  }

  CVMassert(staticField->type == CVMCPU_MEMSPEC_IMMEDIATE_OFFSET
	    || staticField->type == CVMCPU_MEMSPEC_REG_OFFSET );

  branch = CVMJITirnodeGetCondBranchOp(thisNode);
  condCode = mapCondCode(branch->condition);

  CVMRMpinResource(CVMRM_INT_REGS(con), regopnd, CVMRM_ANY_SET, CVMRM_EMPTY_SET);

  switch (staticField->type) {
    case CVMCPU_MEMSPEC_IMMEDIATE_OFFSET:
      CVMCPUinit_Address_disp(&addr, staticField->offsetValue, CVMCPU_MEMSPEC_ABSOLUTE);
      break;
    case CVMCPU_MEMSPEC_REG_OFFSET:
      CVMRMpinResource(CVMRM_INT_REGS(con), staticField->offsetReg, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
      CVMCPUinit_Address_base_disp(&addr, CVMRMgetRegisterNumber(staticField->offsetReg),
				   0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      break;
    default:
      CVMassert(0);
  }

  if (memopnd_on_rhs == CVM_TRUE) {
     CVMCPUemitCompareRegisterMemory(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ/* dummy */, 
				     CVMRMgetRegisterNumber(regopnd), &addr);
  }
  else {
     CVMCPUemitCompareMemoryRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ/* dummy */, 
				     &addr, CVMRMgetRegisterNumber(regopnd));
  }
  CVMRMsynchronizeJavaLocals(con);
  CVMRMpinAllIncomingLocals(con, branch->target, CVM_FALSE);
  branchToBlock(con, condCode, branch->target);
  CVMRMunpinAllIncomingLocals(con, branch->target);

  CVMCPUmemspecRelinquishResource(CVMRM_INT_REGS(con), staticField);

  CVMRMrelinquishResource(CVMRM_INT_REGS(con), regopnd);
}

static void
compare64cc(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode)
{
    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMJITConditionalBranch* branch = CVMJITirnodeGetCondBranchOp(thisNode);
    CVMJITIRBlock* target = branch->target;
    CVMCPUCondCode condCode = mapCondCode(branch->condition);

    CVMJITaddCodegenComment((con, "begin BCOND_LONG."));
#ifndef CVMCPU_HAS_COMPARE
    /* pin before calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif
    CVMRMpinResource(CVMRM_INT_REGS(con), lhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    CVMRMpinResource(CVMRM_INT_REGS(con), rhs, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    condCode = CVMCPUemitCompare64(con, CVMCPU_CMP64_OPCODE, condCode,
                    CVMRMgetRegisterNumber(lhs), CVMRMgetRegisterNumber(rhs));

    CVMRMsynchronizeJavaLocals(con);
#ifdef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    /* pin after calling CVMCPUemitCompare() */
    CVMRMpinAllIncomingLocals(con, target, CVM_FALSE);
#endif

    branchToBlock(con, condCode, target);

    CVMJITprintCodegenComment(("end BCOND_LONG."));

#ifndef CVMCPU_HAS_COMPARE
    /* no longer need resource used in CVMCPUemitCompare() */
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
#endif
    CVMRMunpinAllIncomingLocals(con, target);
}

%}

root: BCOND_INT reg32 staticRef32FromMem : 25 : : : :
        compare32_reg_static(con, $$, CVMCPU_CMP_OPCODE, CVM_TRUE);
root: BCOND_INT staticRef32FromMem reg32 : 26 : : : :
        compare32_reg_static(con, $$, CVMCPU_CMP_OPCODE, CVM_FALSE);
root: BCOND_INT reg32 fieldref32fromMem : 25 : : : :
        compare32_reg_fieldref(con, $$, CVMCPU_CMP_OPCODE, CVM_TRUE);
root: BCOND_INT fieldref32fromMem reg32 : 26 : : : :
        compare32_reg_fieldref(con, $$, CVMCPU_CMP_OPCODE, CVM_FALSE);

%{

#ifdef MIN
#undef MIN
#endif
#define MIN(x,y) ((x) < (y) ? (x) : (y))

#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
void
patchInstruction(CVMJITCompilationContext* con)
{
    CVMCPUInstruction* patchedInstructions = 
    	CVMCompiledGCCheckPCs_patchedInstructions(con->gcCheckPcs, 
						  con->gcCheckPcsSize);
    CVMUint32      pcOffset;

    CVMassert(con->gcCheckPcsIndex < con->gcCheckPcsSize);

    patchedInstructions[con->gcCheckPcsIndex] =
        *(CVMCPUInstruction*)CVMJITcbufGetPhysicalPC(con);
#if CVM_JIT_DEBUG_EXTRA
{
    int i;
    CVMUint8 *ptr = (CVMUint8 *)CVMJITcbufGetPhysicalPC(con);
    CVMconsolePrintf("Address of context = 0x%x\n", con);
    CVMconsolePrintf("Number of GC PCs = 0x%x\n", con->gcCheckPcsSize);
    CVMconsolePrintf("Address of GC PCs size = 0x%x\n", &con->gcCheckPcsSize);
    CVMconsolePrintf("Address of patchedInstruction = 0x%x\n", 
			&patchedInstructions[con->gcCheckPcsIndex]);
    CVMconsolePrintf("patchedInstruction = "); 
    for(i = 0; i < sizeof(CVMCPUInstruction); i++)
	CVMconsolePrintf("%x ",*ptr++); 
    CVMconsolePrintf("\n"); 
}
#endif
    
    /* save away this offset in the gcCheckPcs table */
    pcOffset = CVMJITcbufGetLogicalPC(con) +
        (con->codeEntry - con->codeBufAddr);
    CVMassert(pcOffset <= 0xffff); /* Make sure it fits */
    con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = pcOffset;
}
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */

static void
loadIncomingLocals(CVMJITCompilationContext* con,
		   CVMJITIRBlock *b, CVMBool spilledPhis)
{
#ifdef CVM_JIT_REGISTER_LOCALS
    /*
     * Backwards branch targets need to explicitly load all locals so OSR
     * will work. However, normally we skip this by setting up
     * b->logicalAddress to point after the loading of the locals.
     *
     * Note that we also need to load incoming locals after doing a gc to
     * reload ref locals.
     */
    if (b->incomingLocalsCount > 0) {
	CVMassert(spilledPhis == 0);
	/* This is the address we will OSR to */
	b->loadLocalsLogicalAddress = CVMJITcbufGetLogicalPC(con);
	CVMtraceJITCodegen((
            "\tL%d:\t%d:\t@ entry point when locals need to be loaded\n",
             b->blockID, CVMJITcbufGetLogicalPC(con)));
	CVMRMpinAllIncomingLocals(con, b, CVM_TRUE);
	CVMRMunpinAllIncomingLocals(con, b);
    }
#endif /* CVM_JIT_REGISTER_LOCALS */
}

/*

  A Rendevous with the Garbage Collector (GC)
  ===========================================
  
  Backward branches are typical GC safe points - places where Java
  threads can take a break and let the GC do it's work. Instead of
  polling the GC, the JITC runtime is notified when garbage collection
  is necessary.
  
  On such a notification the compiler runtime patches active compiled
  methods to synchronize with the GC.
  
  --- < Example (compiled loop) > ---
  
  
  gc_label:	  nop             # patchable GC Check point
  loop:		  ...
  		  ...
  		  ...
  		  cmp ...
  		  jcc loop        # patchable branch
  
  On GC notification the runtime patches the loop to do a rendezvous
  with the GC:
  
  gc_label:	  call CVMCCMruntimeGCRendezvousGlue # patchable GC Check point
  loop:		  ...
  		  ...
  		  ...
  		  cmp ...
  		  jcc gc_label    # patchable branch
  
  --- </ Example (compiled loop) > ---
  
  How it works:
  
    o A basic block is known to be the target of a backward branch
    o Thus, a call to CVMCCMruntimeGCRendezvousGlue is placed at the
      beginning of the block, the calls instruction code is stored into
      CVMJITCompilationContext.gcCheckPcs.patchedInstructions and then the
      instruction is overwritten again with nops (see checkGC() in
      jitgrammarrules.jcs)
    o For backwards branches the procedure is similar. First the branch
      is emitted with the rendezvous call as target, its encoding is saved
      away and the the branch is reemitted with the loop entry as
      target. As soon as GC wants to perform a collection, the branch is
      patched with the saved branch. (see branchToBlock() in
      jitgrammarrules.jcs)
    o GC calls CVMJITcsPatchRendezvousCalls() initiate patching
  
  How it works on CISC:
  
    o Instruction are of variable length
    o We set an upper limit for the length of patchable instructions,
      and make data structures wide enough to hold these instructions
    o Holes between instruction end and instruction limit are filed with
      nops

*/

#ifdef CVM_JIT_GENERATE_GC_RENDEZVOUS
   /* SVMC_JIT d022609 (ML) 2004-06-30. TODO: this code does not yet
    * reflect Sun's new strategy for gc patching. the new pattern
    * proceeds as follows.  before emitting a gc call, code for
    * spilling phi registers is emitted.  this prevents patching the
    * gc call plus the spilling code atomically. hence a branch is
    * emitted before the spilling code, branching around the spilling
    * and gc calling code. only the branch site will be patched in
    * case of gc request. since on the one hand in SVMC we do not
    * emit these gc calls anyway and on the other hand our postpass
    * optimizer needs to know about the structure of these gc call
    * sites (in the non-SVMC case), we cannot afford to get this
    * code updated on x86 at present.
    */
/*
 * At block entry, if we know this is the target of a backwards branch,
 * check for GC rendezvous request.
 */
void
CVMJITcheckGC(CVMJITCompilationContext* con, CVMJITIRBlock* b)
{
/* 
 * At block entry, if we know this is a target of a backwards branch, check
 * for GC rendezvous request.
 */
#if !defined(CVMJIT_TRAP_BASED_GC_CHECKS) && \
    !defined(CVMJIT_PATCH_BASED_GC_CHECKS)

    /*
     * Generate code to do a GC rendezvous inline if one has been requested.
     */
    CVMBool spilledPhis;
    CVMRMResource *scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
                 		      CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
    int scratchReg = CVMRMgetRegisterNumber(scratch);
    int cvmGlobalsReg;

    /* This is where branches to the block will branch to. */
    CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
                        b->blockID, CVMJITcbufGetLogicalPC(con)));

    /* This is where we branch to when a gc is needed (do_gc label) */
    b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

    /* spill phis */
    spilledPhis = CVMRMspillPhis(con, CVMRM_SAFE_SET);

    CVMJITprintCodegenComment(("Do GC Check:"));

#ifdef CVMCPU_CVMGLOBALS_REG
    cvmGlobalsReg = CVMCPU_CVMGLOBALS_REG;
#else
    /* load CVMglobals */
    cvmGlobalsReg = scratchReg;
    CVMJITaddCodegenComment((con, "CVMglobals"));
    CVMJITsetSymbolName((con, "CVMglobals"));
    CVMCPUemitLoadConstant(con, scratchReg, (CVMInt32)&CVMglobals);
#endif

    /* load CVMglobals.cstate[CVM_GC_SAFE].request */
    CVMJITaddCodegenComment((con, "CVMglobals.cstate[CVM_GC_SAFE].request;"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        scratchReg, cvmGlobalsReg,
        offsetof(CVMGlobalState, cstate[CVM_GC_SAFE].request));

    /* check if gc requested */
    CVMJITaddCodegenComment((con, "If GC is requested,"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_NE,
                              scratchReg, 0);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);

    /* gc if requested */
    CVMJITaddCodegenComment((con, "CVMCCMruntimeGCRendezvousGlue"));
    CVMCPUemitAbsoluteCallConditional(con, CVMCCMruntimeGCRendezvousGlue,
                                      CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK,
                                      CVMCPU_COND_NE);

    /* Get a stackmap. This is a GC point */
    CVMJITcaptureStackmap(con, 0);

#if 0
    /* reload phis */
    if (spilledPhis) {
	CVMRMreloadPhis(con, CVMRM_SAFE_SET);
    }
#endif

    /* load incoming locals if necessary */
    loadIncomingLocals(con, b, spilledPhis);

    b->logicalAddress = CVMJITcbufGetLogicalPC(con);

    return;
 
#endif /* !CVMJIT_TRAP_BASED_GC_CHECKS && !CVMJIT_PATCH_BASED_GC_CHECKS */

#ifdef CVMJIT_TRAP_BASED_GC_CHECKS

    {
        /*
          Just generate an instruction that will cause a trap (crash) when
          a gc is requested. There a 3 cases to worry about:

          (1) Trap-based Incoming Locals:
          ---------------------------
          bk->loadLocalsLogicalAddress:
          gc_return:
            <stackmap>
            load incoming locals
          bk->gcLogicalAddress:
            ldr  rGC,offset(gc_return)(rGC)
          bk->logicalAddress:
      
          -Trap redirects execution to CVMCCMruntimeGCRendezvousGlue,
           and sets LR to gc_return. The offset to gc_return is encoded
           in the ldr trap instruction.
      
          (2) Trap-based Incoming Phis:
          -------------------------
          do_gc:
            spill phis
            CVMCCMruntimeGCRendezvousGlue
            <stackmap>
            load phis
          bk->gcLogicalAddress:
            ldr  rGC,-offset(do_gc)(rGC)
          bk->logicalAddress:
    
          -Trap redirects execution to do_gc based on offset embedded in the
           trap instruction. Note that the offset is negative to indicate
           that execution needs to resume at the offset rather than just
           setting LR to the offset.
     
          (3) Trap-based Normal:
          ------------------
            <stackmap>
          bk->gcLogicalAddress:
            ldr  rGC,0(rGC)
          bk->logicalAddress:

          -Trap redirects execution to CVMCCMruntimeGCRendezvousGlue, 
           and sets LR to the trap instruction. The offset of 0 is 
           encoded the ldr trap instruction, so the trap handler knows 
           to set LR to be the same as the trap instruction.
        */

        CVMBool spilledPhis;
        CVMInt32 gcLogicalAddress;

        /*this is where we branch to when a gc is needed (do_gc label)*/
        gcLogicalAddress = CVMJITcbufGetLogicalPC(con);
        CVMJITcsSetEmitInPlace(con);

        /* spill phis */
        spilledPhis = CVMRMspillPhis(con, CVMRM_SAFE_SET);

        /* We only need an inlined unconditional gcRendezvous if there are
         * incoming phis.
         */
        if (spilledPhis) {
  	    /* unconditional gc  */
	    CVMJITaddCodegenComment((con, "CVMCCMruntimeGCRendezvousGlue"));
	    CVMCPUemitAbsoluteCall(con, CVMCCMruntimeGCRendezvousGlue,
			           CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH, 0);
        }

        if (CVMJITcbufGetLogicalPC(con) == 
	    CVMJITgetPreviousStackmapLogicalPC(con)) {
	    /* 6314307: If the previous block ended with a method call, then
             * this current pc already has a stackmap that includes the
	     * arguments to that call. When we generate a 2nd stackmap for
	     * this pc below, it won't end up getting used. If this
	     * gcCheck results in a gc, then non-existent stack items will
	     * be scanned. We need to force an extra instruction before
	     * the gcCheck instruction so the 2nd stackmap will be found.
	     */
	    CVMCPUemitNop(con);
	    /* we don't want GC to return to the nop or it will use the
	       wrong stackmap */
	    gcLogicalAddress = CVMJITcbufGetLogicalPC(con);
        }

        /* Get a stackmap. This is a GC point */
        CVMJITcaptureStackmap(con, 0);

        /* reload phis */
        if (spilledPhis) {
	    CVMRMreloadPhis(con, CVMRM_SAFE_SET);
        }
    
        /* load incoming locals if necessary */
        loadIncomingLocals(con, b, spilledPhis);

#ifdef CVMCPU_HAS_VOLATILE_GC_REG
        /* rGC is not setup in an NV register. Get it from the ccee. Note
         * that it is normally setup before the backwards branch, which will
         * branch after this code. This one is in case we GC, in which case
         * rGC will be trashed before we re-enter the block.
         */
        CVMJITaddCodegenComment((con, "rGC = ccee->gcTrapAddr"));
        CVMCPUemitCCEEReferenceImmediate(con,
				     CVMCPU_LDR32_OPCODE, CVMCPU_GC_REG,
				     offsetof(CVMCCExecEnv, gcTrapAddr));
#endif

        /* This is where branches to the block will branch to. Note that 
         * we branch after the rGC setup above. In order to reduce load 
         * latency issues, we require that rGC be setup before doing the 
         * branch.
         */
        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
			b->blockID, CVMJITcbufGetLogicalPC(con)));
        b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

        {
	    /* Emit the instruction that will trap when a gc is
	     * requested. Note that the offset is the distance to the
	     * instructions above to handle the actual gcRendezvous. The SEGV
	     * handler will use this value to redirect execution to the above
	     * code.
	     */
	    int gcOffset = b->gcLogicalAddress - gcLogicalAddress;
	    CVMassert(gcOffset <=
		      sizeof(void*) * CVMJIT_MAX_GCTRAPADDR_WORD_OFFSET);
	    if (spilledPhis) {
                /* encode that there are incoming phis */
	        gcOffset = -gcOffset;
	    }
	    CVMJITaddCodegenComment((con, "gc trap instruction"));
#ifdef CVM_DEBUG_ASSERTS
	{
	    CVMUint8 *ptr = (CVMUint8 *)CVMJITcbufGetPhysicalPC(con);
#endif
	    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
					   CVMCPU_GC_REG, CVMCPU_GC_REG,
					   gcOffset);
#ifdef CVM_DEBUG_ASSERTS
            CVMassert((*((CVMCPUInstruction*)ptr) &
                CVMCPU_GCTRAP_INSTRUCTION_MASK) == CVMCPU_GCTRAP_INSTRUCTION);
	}
#endif
        }

        b->logicalAddress = CVMJITcbufGetLogicalPC(con);

        CVMJITcsClearEmitInPlace(con);
        return;
    }
#endif /* CVMJIT_TRAP_BASED_GC_CHECKS */

#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
    {
        /*
         * Generate a call to do the rendezvous, but also patch over the call
         * so normally it is not made until a gc is requested. This way there
         * is no overhead involved at the gc checkpoint.
         */
        int spilled;

        CVMJITcsSetEmitInPlace(con);
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
        CVMassert(con->SMjspOffset == 0);
#endif

        /*
          There are 3 forms of backwards branch targets we worry about:

          (1) Patch-based Incoming Locals:
          -------------------------------
          do_gc:
              gcRendezvous    <-- patched until a GC is needed
          bk->loadLocalsLogicalAddress:
          load incoming locals
          bk->logicalAddress:
      
          (2) Patch-based Incoming Phis:
          ------------------------------
          do_gc:
              spill phis
              gcRendezvous    <-- patched until a GC is needed
              load phis
          bk->logicalAddress:
      
          (3) Patch-based Normal:
          -----------------------
          do_gc:
              gcRendezvous    <-- patched until a GC is needed
          bk->logicalAddress:
      
          When patching gcRendezvous, we do 1 of two things:
          -If there is no delay slot, gcRendezvous is patched by rewinding
           and overwriting it with whatever instruction would naturally
           come next.
          -If there is delay slot, gcRendezvous is patched with a nop.
      
          -All branches to the block are to bk->logicalAddress, thus avoiding
           any gcRendezvous overhead.
          -When doing a fallthrough, an explicit branch to bk->logicalAddress
           is used, unless there is no phis or incoming locals, in which case
           a fallthrough is allowed.
      
          -If there are incoming locals, the first MAP_PC node in the block
           is remapped to bk->loadLocalsLogicalAddress for OSR. There is no 
           OSR when there are incoming phis, and no remapping is needed in 
           the normal case.
         */

        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ Patchable GC Check point\n",
            CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));

        /* this is where we branch to when a gc is needed (do_gc label) */
        b->gcLogicalAddress = CVMJITcbufGetLogicalPC(con);

        spilled = CVMRMspillPhisCount(con, CVMRM_SAFE_SET);

        /*
         * Need to spill phis before doing a gc and reload them after the gc
         * in case either they are ref phis or are in volatile registers.
         * See case (2) above.
         */
        if (spilled > 0) {
	    CVMJITprintCodegenComment(("******* PHI spill %C.%M\n",
				       CVMmbClassBlock(con->mb), con->mb));
	    CVMRMspillPhis(con, CVMRM_SAFE_SET);
        }

        if (CVMJITcbufGetLogicalPC(con) == 
	    CVMJITgetPreviousStackmapLogicalPC(con)) {
	    /* If the previous block ended with a method call, then this
	     * current pc already has a stackmap that includes the
	     * arguments to that call. When we generate a 2nd stackmap for
	     * this pc below, it won't end up getting used. If this
	     * gcCheck results in a gc, then non-existent stack items will
	     * be scanned. We need to force an extra instruction before
	     * the gcCheck instruction so the 2nd stackmap will be found.
	     */
   	    /*
	     * NOTE: We could do better here. Currently we don't always
	     * flush JSP when making method calls, so the stackmap is our
	     * only indicator of how much we need to scan. If we always
	     * flush JSP, then the scanner can just use the previous
	     * stackmap and stop scanning when topOfStack is reached. This
	     * is a fix that was made in Orion, and also allows us to
	     * remove another fix involving exception handling when
	     * TOS isn't flushed. See bug #4732902.  
	    */
	    CVMCPUemitNop(con);
        }

        /* 1. Emit the gc rendezvous instruction. Tell the emitter that
         *    we are going to rewind the code buffer. */
        CVMCPUemitGcCheck(con, CVM_TRUE);
        /* 2. rewind to just before the rendezvous instruction */
        CVMJITcbufRewind(con, sizeof(CVMCPUInstruction));
        /* 3. save away the gc rendezvous instruction. */
        patchInstruction(con);

#ifdef CVMCPU_NUM_NOPS_FOR_GC_PATCH
        {
            int i;
            /* We only patch one instruction, which is the branch or call to
             * CVMCCMruntimeGCRendezvousGlue. For platforms like Sparc that
             * have a delay slot after a call(or branch), we put nop's at
             * the patch location and also one instruction after patch, so
             * when GC is patched we can guarantee that the delay slot does 
             * not contain any unexpected instruction. On platforms that 
             * require multiple instructions to do the call (like powerpc 
             * when we don't copy to the code cache) we also need to 
             * generate a nop so we don't attempt to return to the call 
             * instruction, but instead return to the instruction after it.
             * Otherwise if it gets repatched before we return, we executed
             * the call instruction without executing the code to setup the
             * call properly.
             */
            for (i = 0; i < CVMCPU_NUM_NOPS_FOR_GC_PATCH; i++) {
                CVMCPUemitNop(con);
            }
        }
#endif

        /* Get a stackmap. This is a GC point */
        CVMJITcaptureStackmap(con, 0);

        /* reload the spilled phis. See case (2) above. */
        if (spilled > 0) {
	    CVMRMreloadPhis(con, CVMRM_SAFE_SET);
        }

        /* load incoming locals if necessary */
        loadIncomingLocals(con, b, spilled > 0);

        /*
         * This is where branches to the block will branch to. Note that
         * we are smart here and make branches to this block branch after
         * the loading of locals, since the locals will already be loaded
         */
        CVMtraceJITCodegen(("\tL%d:\t%d:\t@ entry point for branches\n",
            CVMJITirblockGetBlockID(b), CVMJITcbufGetLogicalPC(con)));
        b->logicalAddress = CVMJITcbufGetLogicalPC(con);
        CVMJITcsClearEmitInPlace(con);
    }
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */ 
}
#endif /* CVM_JIT_GENERATE_GC_RENDEZVOUS */

static void
branchToBlockX(
    CVMJITCompilationContext* con,
    CVMCPUCondCode condcode,
    CVMJITIRBlock* target,
    CVMBool isPatchableBranch)
{

    int logicalAddress = target->logicalAddress;
    CVMJITFixupElement** fixupList = NULL;

    /*
     * If we are branching to a block that we haven't compiled yet,
     * then add the instruction to the list that must be fixed up
     * once the target block is compiled.
     */
    if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
	if (condcode == CVMCPU_COND_AL) {
	    fixupList = &(target->branchFixupList);
	} else {
	    fixupList = &(target->condBranchFixupList);
	}
    }

#ifndef CVMJIT_PATCH_BASED_GC_CHECKS
     	if (CVMJITirblockIsBackwardBranchTarget(target)) {
           logicalAddress = target->gcLogicalAddress;
        }
    
    CVMJITaddCodegenComment((con, "branch to block L%d", target->blockID));
    CVMCPUemitBranchNeedFixup(con, logicalAddress, condcode, fixupList);

#else /* CVMJIT_PATCH_BASED_GC_CHECKS */ 
    /* 
       SVMC_JIT (rr). was: `#ifdef CVMCPU_HAS_DELAY_SLOT'.  do this
       only if we generate GC points (SVMC does not) and if these GC
       points should be optimized by patching (sharing code might
       disallow this).
       NOTE: This code actually applies when the gc rendezvous
       code requires more than one instruction.  A delay slot means
       it will always be more than one instruction, but for other
       platforms, the code sequence could depend on the distance of
       the jump. PowerPC and ARM both fall into this category, but
       only if CVM_JIT_COPY_CCMCODE_TO_CODECACHE is not #defined,
       which by default it is because this produces the fastest code.
       For this reason, we don't worry about doing the following
       optimization for platforms that don't have a delay slot.
       However, if we were eager to do this, we should actually check
       CVMCPU_NUM_NOPS_FOR_GC_PATCH and not CVMCPU_HAS_DELAY_SLOT.
    */
    if (CVMJITirblockIsBackwardBranchTarget(target)) {
        CVMCPUInstruction* patchedInstructions =
	  CVMCompiledGCCheckPCs_patchedInstructions(con->gcCheckPcs, con->gcCheckPcsSize);

        CVMassert(con->gcCheckPcsIndex < con->gcCheckPcsSize);
        CVMJITaddCodegenComment((con, "branch to block L%d",
				 CVMJITirblockGetBlockID(target)));

        if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
            /* This must be a forward branch. */
            if (isPatchableBranch) {
                CVMCPUemitPatchableBranchNeedFixup(
                            con, logicalAddress, condcode, fixupList);
            } else {
                CVMCPUemitBranchNeedFixup(con, logicalAddress, condcode, fixupList);
            }
            patchedInstructions[con->gcCheckPcsIndex] = 0;
            con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = 0;
        } else {
            int rewindOffset = 0;
	    int branchPC = CVMJITcbufGetLogicalPC(con);

            CVMassert(target->gcLogicalAddress != 0);
            CVMassert(target->gcLogicalAddress <= logicalAddress);

	    /* 
             * No need to patch the backwards branch if
             * (targetAddress = target->gcLogicalAddress)
	     */
	    if (logicalAddress != target->gcLogicalAddress) {
            	CVMJITprintCodegenComment(("Patchable backwards branch:"));
	    	CVMJITaddCodegenComment((con,
				     "branch to GC rendezvous instruction"));
	    	/* First emit branch to instruction that will GC */	
            	CVMCPUemitPatchableBranch(con, target->gcLogicalAddress, condcode);

            	/* Space is reserved for the GC check patching instruction
             	 * at the beginning of a backward branch target (see
             	 * checkGC()).
             	 *
             	 * Normally, the reserved space is skipped when branching to
             	 * the target for performance reason. When GC is requested,
             	 * the branch instruction will be patched to the beginning
             	 * of the target, so that GCRendezvous can be invoked.
             	 */
            	rewindOffset = CVMJITcbufGetLogicalPC(con) - branchPC;
            	CVMJITcbufRewind(con, rewindOffset);
            	/* save away the branch instruction. */
	    	patchInstruction(con);
		/* reemit branch */
		CVMJITaddCodegenComment((con,
                         	     "overwriting with branch to loop start"));
		/* SVMC_JIT (rr). Do the optimization only if the
 		 * backward branch is shorter than 8 bytes. Otherwise it
 		 * cannot be patched atomically. */
		CVMCPUemitPatchableBranch(con, logicalAddress, condcode);
            } else {
		/* no patch for this backwards branch. */
		patchedInstructions[con->gcCheckPcsIndex] = 0;
		con->gcCheckPcs->pcEntries[con->gcCheckPcsIndex++] = 0;
                CVMJITaddCodegenComment((con, "branch to block L%d, skip GC",
                                        CVMJITirblockGetBlockID(target)));
                if (isPatchableBranch) {
		    CVMCPUemitPatchableBranch(con, logicalAddress, condcode);
                } else {
	            CVMCPUemitBranch(con, logicalAddress, condcode);
                }
            }		
        }
    } else {
        CVMJITaddCodegenComment((con, "branch to block L%d",
                                CVMJITirblockGetBlockID(target)));
        if (isPatchableBranch) {
	    CVMCPUemitPatchableBranchNeedFixup(con, logicalAddress,
					       condcode, fixupList);
        } else {
            CVMCPUemitBranchNeedFixup(con, logicalAddress, condcode, fixupList);
        }
    }
#endif /* CVMJIT_PATCH_BASED_GC_CHECKS */
}

static void
branchToBlock(
    CVMJITCompilationContext* con,
    CVMCPUCondCode condcode,
    CVMJITIRBlock* target)
{
    branchToBlockX(con, condcode, target, CVM_FALSE);
}

static void
jsrToBlock(
    CVMJITCompilationContext* con,
    CVMJITIRBlock* target)
{
    if (!(target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
        CVMCPUemitBranchLinkNeedFixup(con, target->logicalAddress,
                                      &(target->branchFixupList));
    } else {
        CVMCPUemitBranchLink(con, target->logicalAddress);
    }
}

%}

root:	TABLESWITCH reg32 : 40 : : : : {
  CVMJITTableSwitch* ts = CVMJITirnodeGetTableSwitchOp($$);
  int i;
  int low = ts->low;
  int high = ts->high;
  CVMRMResource* key = popResource(con);
  int keyRegNo;
  CVMBool patchableBranch = CVM_FALSE;
  
  CVMJITprintCodegenComment(("tableswitch"));
  CVMRMsynchronizeJavaLocals(con);
  
#ifdef CVMCPU_HAS_ZERO_REG
        if (key == (CVMRM_INT_REGS(con))->preallocatedResourceForConstantZero){
            /* We know the key is a constant 0. So this can be simplified. */
            if (low <= 0 && high >= 0) {
                CVMJITIRBlock *target = ts->tableList[0-low];
                branchToBlock(con, CVMCPU_COND_AL, target);
            } else {
                branchToBlock(con, CVMCPU_COND_AL, ts->defaultTarget);
            }
        } else
#endif
    {
      /*
       * After pinning the key resource, we are going to modify
       * the register it gets pinned to. This causes problems because
       * the resource may be associated with a local or constant
       * that will be used later. For this reason, we must first
       * relinquish the resource after pinning it, and then allocate
       * a new resource for the register it was pinned to.
       * FIXME: What if the key register is across blocks?
       */
      CVMRMpinResource(CVMRM_INT_REGS(con), key,
		       CVMRM_ANY_SET, CVMRM_EMPTY_SET);
      keyRegNo = CVMRMgetRegisterNumber(key);
      CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
      key = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), keyRegNo, 1);
      
      /* subtract "low" from "key" since branch table is 0-based */
      if (low != 0) {
	int opcode;    /* add or sub */
	int absLow;    /* absolute value of low */
	if (low > 0) {
	  opcode = CVMCPU_SUB_OPCODE;
	  absLow = low;
	} else {
	  opcode = CVMCPU_ADD_OPCODE;
	  absLow = -low;
	}
	CVMCPUemitBinaryALUConstant(con, opcode, keyRegNo, keyRegNo,
				    absLow, CVMJIT_NOSETCC);
      }
      
      /* compare (key - low) to (high - low) */
      {
	CVMUint32 highLow = high - low;
	CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
				  CVMCPU_COND_HI, keyRegNo, highLow);
      }
      
      /* emit branch for the default target */
      branchToBlock(con, CVMCPU_COND_HI, ts->defaultTarget);
     
#ifdef CVMJIT_PATCH_BASED_GC_CHECKS
      for (i = 0; i < ts->nElements; i++) {
          CVMJITIRBlock* target = ts->tableList[i];
          if (CVMJITirblockIsBackwardBranchTarget(target) &&
              (target->flags & CVMJITIRBLOCK_ADDRESS_FIXED)) {
              patchableBranch = CVM_TRUE;
              break;
          }
      }
#endif 
      CVMCPUemitTableSwitchBranch(con, keyRegNo, patchableBranch);
      CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);

      /* emit branches for each table element. Each of these branches
         takes 6 bytes. */
      for (i = 0; i < ts->nElements; i++) {
	CVMJITIRBlock* target = ts->tableList[i];
	branchToBlockX(con, CVMCPU_COND_AL, target, patchableBranch);
      }
    }
};

root:	LOOKUPSWITCH reg32 : 40 : : : : {
	/*
	 * There are really 4 ways to do a lookup switch based on
	 * whether you not you want to do a binary search and whether or not
	 * you want a table driven loop, or an unrolled loop.
	 *
	 *   1. binary search with loop
	 *   2. binary search with unrolled loop
	 *   3. linear search with loop
	 *   4. linear search with unrolled loop
	 *
	 * Here are advantages and disadvantages of all:
	 *
	 *   -Unrolled loops are always faster then their loop counterparts,
	 *    but for large tables they are almost 50% bigger. The breakeven
	 *    point for size seems to be around 10 entries.
	 *   -Unrolled loops require little if any loads from memory, whereas
	 *    loops require 1 or 2 loads, depending on how the branch is
	 *     handled.
	 *   -Unrolled loop only require a register for the key and possibly
	 *    for a large temporary constant. loops require more registers,
	 *    espcially when doing a binary search.
	 *   -(1) is faster than (3) for large data sets, but slower for small.
	 *   -(2) is almost always faster than (4). It's only disadvantage
	 *    to (4) is that it may cause more constant values to be loaded
	 *    into a register  rather than used immediate value, but this
	 *    is rare.
	 *
	 * In summary, a binary search with an unrolled loop is fastest and
	 * the most register efficient and is worth the extra space overhead
         * for large tables, espcially since most lookup tables tend to be on
	 * the smaller size, so this is what we chose.
	 *
	 * To generate the code we need to traverse the sorted lookupswitch
	 * table in a manner similar to the way a binary search would. The
	 * difference is that we aren't looking for anything, but instead
	 *  have to visit each node and generate code for it.
	 *
	 * For each node visited we do the following (it helps to visualize the
	 * table organized as a balanced binary tree):
	 *  1. generate compare code to see if this node is a match.
	 *  2. If there is a "left subtree", push a context for it so we can
	 *     generate code for it later and generate a "blt" to it.
	 *  3. Generate a "beq" to branch to the correct target for a match
	 *  4. If there is a "right subtree", make it the next node to visit.
	 *  5. If there is not a "right subtree", pop the topmost node off of
	 *     the stack and make it the next node to visit. If there is
	 *     nothing on the stack then we are done.
	 *
	 * There's one more thing that makes the code a bit tricky. We don't
	 * use cmp to compare the key to the matchValue. We use sub instead
	 * (or add if the value is negative). The advantage of this is that if
	 * the table has a bunch of values in a narrow range, but all are
	 * > 255, then we can avoid having to do an ldr for each one of them.
	 * Instead, we just ldr the first one we need to check, and the rest
	 * of the time we can just base the "compare" values on the difference
	 * between the matchValue of the current node and the matchValue of
	 * the previous node we visited.
	 */
	CVMJITLookupSwitch* ls = CVMJITirnodeGetLookupSwitchOp($$);
        CVMRMResource* key = popResource(con);
	int keyreg;
        CVMRMResource* scratch;
	int scratchreg;
     	CVMUint16 low  = 0;	
    	CVMUint16 high = ls->nPairs - 1;
    	CVMUint16 index = (low + high) / 2;
    	CVMLookupSwitchStackItem stack[16]; /* enough for nPairs == 64k */
    	CVMUint8  tos = 0;
	CVMBool useCmp; /* use cmp rather than adds and subs */
        int branchToDefaultPC = -1;
#ifdef CVMCPU_HAS_ALU_SETCC
	CVMInt32  prevMatchValue = 0;
#endif

        CVMJITprintCodegenComment(("lookupswitch"));
	CVMRMsynchronizeJavaLocals(con);

	/* if there are no pairs, then just branch to the target. */
	if (ls->nPairs == 0) {
            branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
	    goto done;
	}

#ifndef CVMCPU_HAS_ALU_SETCC
	useCmp = CVM_TRUE;
#else
	{
	    /* if (hi - lo < 0), then that means it is actually greater than
	     * MININT, so we need to use compares rather than adds/subs.
	     */
	    int lo = ls->lookupList[0].matchValue;
	    int hi = ls->lookupList[ls->nPairs - 1].matchValue;
	    useCmp = (lo < 0) && (hi > 0) && (hi - lo < 0);
	}
#endif

#ifdef CVMCPU_HAS_ZERO_REG
        if (key == (CVMRM_INT_REGS(con))->preallocatedResourceForConstantZero){
            int i;
            for (i = 0; i < ls->nPairs; i++) {
                if (ls->lookupList[i].matchValue == 0) {
                    /* If we found a match, then branch to its handler. */
                    branchToBlock(con, CVMCPU_COND_AL, ls->lookupList[i].dest);
                    goto done;
                }
            }
            branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
            goto done;
        }
#endif
	/*
	 * After pinning the key resource, we are going to modify
	 * the register it gets pinned to. This causes problems because
	 * the resource may be associated with a local or constant
	 * that will be used later. For this reason, we must first
	 * relinquish the resource after pinning it, and then allocate
	 * a new resource for the register it was pinned to.
	 * FIXME: What if the key register is across blocks?
	 */
	CVMRMpinResource(CVMRM_INT_REGS(con), key,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	keyreg = CVMRMgetRegisterNumber(key);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
	key = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), keyreg, 1);
	
	/*
	 * We may need a scratch register for loading large constant
	 * values into. Get it now (and allow for any necessary spills).
	 * because it will be to late once we start generating code (we
	 * don't want the regman state to change)
	 */
	scratch = CVMRMgetResource(CVMRM_INT_REGS(con),
				   CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	scratchreg = CVMRMgetRegisterNumber(scratch);
	
	
	/*
	 * On each iteration, look at the lookupswitch node at index,
	 * generate the appropriate code, push state for a "lower" node
	 * to be generated if necessary, and setup state for next "higher"
	 * node to be generated if necessary. We are all done what there
	 * are no nodes on the stack and no "higher" nodes left.
	 */
	con->inConditionalCode = CVM_TRUE;
	while (1) {
	    /*
	     * diff is the absolute value of the difference between this
	     * nodes matchValue and the previous nodes matchValue. It needs
	     * to be an absolute value and an uint because it may be larger
	     * than MAXINT.
	     */
#ifdef CVMCPU_HAS_ALU_SETCC
	    CVMUint16      prevIndex;
#endif
	    CVMInt32       matchValue = ls->lookupList[index].matchValue;
	
	    CVMtraceJITCodegen((".match%d (low=%d index=%d high=%d)\n",
				index, low, index, high));
#ifdef CVMCPU_HAS_ALU_SETCC
	    if (!useCmp) {
		CVMUint32      diff;
		int            opcode; /* do we add or subtract diff */
		/* compute diff */
		if (prevMatchValue > matchValue) {
		    opcode = CVMCPU_ADD_OPCODE;
		    diff = prevMatchValue - matchValue;
		} else {
		    opcode = CVMCPU_SUB_OPCODE;
		    diff = matchValue - prevMatchValue;
		}
		
		/* add or subtract diff from key and setcc.
		 * WARNING: don't let CVMCPUemitBinaryALUConstant() load a
		 * large constant into a register. This will change the
		 * regman state, which is a no-no in conditionally executed
		 * code. Instead, manually load the large constant here
		 * using CVMCPUemitLoadConstant(), which won't affect the
		 * regman state.
		 */
		if (CVMCPUalurhsIsEncodableAsImmediate(diff)) {
		    CVMCPUemitBinaryALUConstant(con, opcode,
		        keyreg, keyreg, diff, CVMJIT_SETCC);
		} else {
		    CVMCPUemitLoadConstant(con, scratchreg, diff);
		    CVMCPUemitBinaryALURegister(con, opcode,
			keyreg, keyreg, scratchreg, CVMJIT_SETCC);
		}
	    }
#endif
	
	    /*
	     * There are 3 states we can be in.
	     * 1. low < index < high, which means that if we don't have
	     *    a match, we need to deal both with the possiblity of
	     *    index being too big or too large.
	     * 2. low == index < high, which means that if we don't have
	     *    a match, we only need to deal with the possibility of
	     *    index being too small.
	     * 3. low == index == high, which means either we find a
	     *    a match this time or the lookup failed.
	     * NOTE: low < index == high is not possible.
	     */
	    if (low != high) {          /* (1) and (2) */
		if (index != low) {     /* (1) only */
		    if (useCmp) {
			/* WARNING: don't let regman know about constant */
			if (CVMCPUalurhsIsEncodableAsImmediate(matchValue)) {
			    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
				CVMCPU_COND_LT, keyreg, matchValue);
			} else {
			    CVMCPUemitLoadConstant(con, scratchreg, matchValue);
			    CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
			        CVMCPU_COND_LT, keyreg, scratchreg);
			}
		    }
		    /* If key is too small, branch to node that will handle
		     * a smaller match, but first push a context so we can
		     * generate code for that node later and also backpatch
		     * this reference to it.
		     */
		    pushLookupNode(stack, &tos, low, index - 1, index,
				   CVMJITcbufGetLogicalPC(con));
		    CVMCPUemitBranch(con, 0, CVMCPU_COND_LT);
		}
		if (useCmp) {
		    /* WARNING: don't let regman know about constant */
		    if (CVMCPUalurhsIsEncodableAsImmediate(matchValue)) {
			CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
			    CVMCPU_COND_EQ, keyreg, matchValue);
		    } else {
			CVMCPUemitLoadConstant(con, scratchreg, matchValue);
			CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, scratchreg);
		    }
		}
		/* If we found a match, then branch to its handler. */
		branchToBlock(con, CVMCPU_COND_EQ,
			      ls->lookupList[index].dest);
		/* Setup for node to handle the next higher range. */
#ifdef CVMCPU_HAS_ALU_SETCC
		prevIndex = index;
#endif
		low = index + 1;
	    } else {                     /* (3) low == index == high */
		CVMLookupSwitchStackItem* item;
		if (useCmp) {
		    /* WARNING: don't let regman know about constant */
		    if (CVMCPUalurhsIsEncodableAsImmediate(matchValue)) {
			CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, matchValue);
		    } else {
			CVMCPUemitLoadConstant(con, scratchreg, matchValue);
			CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE,
		            CVMCPU_COND_EQ, keyreg, scratchreg);
		    }
		}
		
		/* If EQ we have a match */
		branchToBlock(con, CVMCPU_COND_EQ, ls->lookupList[index].dest);

                /* Else we have a lookup failure and need to branch to the
                   default target. Note we are only allowed one backward branch
                   to the default target in this lookupswitch, due to how
                   gcCheckPcsSize is computed by the front end. Therefore
                   if we've already generated a backward branch to the default,
                   target we'll just branch to this branch instead.
                */
                if (branchToDefaultPC == -1) {
                    if (CVMJITirblockIsBackwardBranchTarget(ls->defaultTarget))
                    {
                        branchToDefaultPC = CVMJITcbufGetLogicalPC(con);
                    }
                    branchToBlock(con, CVMCPU_COND_AL, ls->defaultTarget);
                } else {
                    CVMJITaddCodegenComment((con,
                        "branch to default target branch"));
		    CVMCPUemitBranch(con, branchToDefaultPC, CVMCPU_COND_AL);
                }
		
		/* See if there are still some more nodes to generate. */
		item = popLookupNode(stack, &tos);
		if (item == NULL) {
		    break;
		}
		
		/* Setup for the handling of the popped node */
		low = item->low;
		high = item->high;
#ifdef CVMCPU_HAS_ALU_SETCC
		prevIndex = item->prevIndex;
#endif
		
		/* backpatch the branch in the node that created this node. */
		CVMJITfixupAddress(con,
				   item->logicalAddress,
				   CVMJITcbufGetLogicalPC(con),
				   CVMJIT_COND_BRANCH_ADDRESS_MODE);
	    }
	
	    /* setup index and prevMatchValue for the next node */
	    index = (low + high) / 2;
#ifdef CVMCPU_HAS_ALU_SETCC
	    prevMatchValue = ls->lookupList[prevIndex].matchValue;
#endif
	}
	con->inConditionalCode = CVM_FALSE;
	
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), scratch);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), key);
 done:;

    };

%{

static void
emitBoundsCheck(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
		CVMBool isConstIndex, CVMInt32 constIndex, CVMBool length_in_mem) 
{
    CVMRMResource* arrayLength = popResource(con);
    CVMRMResource* arrayIndex = NULL;

    /* We always pin the arrayLength operand. If length_in_mem == TRUE, then it does
       not represent the arrayLength, but the object address. The length then has to
       be explictly accessed in the compare. */
    CVMRMpinResource(CVMRM_INT_REGS(con), arrayLength, CVMRM_ANY_SET, CVMRM_EMPTY_SET);

    if (CVM_FALSE == length_in_mem) {

      /* compare arrayLength to the index */
      if (isConstIndex) {
	/* index is the constant passed in */
	CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS,
				  CVMRMgetRegisterNumber(arrayLength),
				  constIndex);
      } else {
	/* array index is the resource on the stack */
	arrayIndex = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), arrayIndex,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS,
				  CVMRMgetRegisterNumber(arrayLength),
				  CVMRMgetRegisterNumber(arrayIndex));
      }
    }
    else {
      if (isConstIndex) {
	CVMCPUAddress addr;
	CVMCPUinit_Address_base_disp(&addr, CVMRMgetRegisterNumber(arrayLength), ARRAY_LENGTH_OFFSET, 
				     CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
	CVMCPUemitCompareMemoryImmediate(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS, &addr, constIndex);
	CVMJITaddCodegenComment((con, "arraylength"));
	CVMJITdumpCodegenComments(con);
      }
      else {
	CVMCPUAddress addr;

	/* array index is the resource on the stack */
	arrayIndex = popResource(con);
	CVMRMpinResource(CVMRM_INT_REGS(con), arrayIndex, CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMCPUinit_Address_base_disp(&addr, CVMRMgetRegisterNumber(arrayLength), ARRAY_LENGTH_OFFSET, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
	CVMCPUemitCompareMemoryRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_LS,
					&addr, CVMRMgetRegisterNumber(arrayIndex));
      }
    }
    
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), arrayLength);

    /* trap if array bounds check failed */
    CVMRMsynchronizeJavaLocals(con);
    CVMJITaddCodegenComment((con,trapCheckComments[CVMJITIR_ARRAY_INDEX_OOB]));
    CVMCPUemitAbsoluteCallConditional(con,
        (void*)CVMCCMruntimeThrowArrayIndexOutOfBoundsExceptionGlue,
        CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK, CVMCPU_COND_LS);

    /* push the arrayIndex resource on the stack */
    if (isConstIndex) {
        pushIConst32(con, constIndex);
    } else {
	pushResource(con, arrayIndex); 
	CVMRMunpinResource(CVMRM_INT_REGS(con), arrayIndex);
    }
}
%}

// Purpose: BOUNDS_CHECK(index, arrayLength)
reg32: BOUNDS_CHECK reg32 reg32 : 20: : : : {
        emitBoundsCheck(con, $$, CVM_FALSE, 0, CVM_FALSE);
    };

reg32: BOUNDS_CHECK reg32 ALENGTH regObj : 20: : : : {
        emitBoundsCheck(con, $$, CVM_FALSE, 0, CVM_TRUE);
    };

// Purpose: BOUNDS_CHECK(index, arrayLength)
iconst32Index: BOUNDS_CHECK ICONST_32 reg32 : 20: : : : {
        CVMInt32 arrayIndex =
	    CVMJITirnodeGetConstant32(CVMJITirnodeGetLeftSubtree($$))->j.i;
        emitBoundsCheck(con, $$, CVM_TRUE, arrayIndex, CVM_FALSE);
    };

iconst32Index: BOUNDS_CHECK ICONST_32 ALENGTH regObj : 20: : : : {
        CVMInt32 arrayIndex =
	    CVMJITirnodeGetConstant32(CVMJITirnodeGetLeftSubtree($$))->j.i;
        emitBoundsCheck(con, $$, CVM_TRUE, arrayIndex, CVM_TRUE);
    };

%{
void
emitReturn(CVMJITCompilationContext* con, CVMJITRMContext* rp, int size) {
        void* helper;
        CVMRMResource *src;

	if (size == 0) {
	    src = NULL;
	} else {
	    src  = popResource(con);
	    CVMassert(size == src->size);
	    if (!CVMRMisConstant(src))
	      CVMRMpinResource(rp, src, rp->anySet, CVMRM_EMPTY_SET);
	    CVMRMstoreReturnValue(rp, src);
	}
	CVMCPUemitPopFrame(con, size);
	if (CVMmbIs(con->mb, SYNCHRONIZED)) {
	    CVMJITaddCodegenComment((con, "goto CVMCCMreturnFromSyncMethod"));
	    CVMJITsetSymbolName((con, "CVMCCMreturnFromSyncMethod"));
	    helper = (void*)CVMCCMreturnFromSyncMethod;
	} else {
	    CVMJITaddCodegenComment((con, "goto CVMCCMreturnFromMethod"));
	    CVMJITsetSymbolName((con, "CVMCCMreturnFromMethod"));
	    helper = (void*)CVMCCMreturnFromMethod;
	}
        /* Emit the one-way ticket home: */
	if (src != NULL) {
	    CVMRMrelinquishResource(rp, src);
	}
	
#ifdef CVM_JIT_USE_FP_HARDWARE
	/* set all floating-point registers to empty. */
	CVMCPUfree_float_regs(con);
#endif

        CVMCPUemitReturn(con, helper);
	CVMJITresetSymbolName(con);

	CVMJITdumpRuntimeConstantPool(con, CVM_FALSE);
}
%}

root:	ATHROW regObj: 10 : : SET_TARGET1($$, ARG3); : : {
	CVMRMResource* src  = popResource(con);
	CVMRMResource* target = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), 
					 CVMRM_ANY_SET, ARG3, 1);
	CVMRMsynchronizeJavaLocals(con);
        src = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src,
				       CVMCPU_ARG3_REG);
	/* A "major" spill is not needed here.  We are leaving,
	  so we only care about the locals */
#if 0
	CVMRMmajorSpill(con, ARG3, CVMRM_SAFE_SET);
#endif
        CVMJITaddCodegenComment((con, "goto CVMCCMruntimeThrowObjectGlue"));
        CVMJITsetSymbolName((con, "CVMCCMruntimeThrowObjectGlue"));
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeThrowObject);
	CVMCPUemitIndirectCall_GlueCode(con, (void*)CVMCCMruntimeThrowObjectGlue,
					CVMRMgetRegisterNumber(target), CVMJIT_CPDUMPOK, CVMJIT_CPBRANCHOK);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), target);
    };

%{

/*
 * instanceof and checkcast generate very similar code sequences:
 * though they call different helper functions, in other respects they
 * differ by only one instruction.
 */
static void
instanceTest(
    CVMJITCompilationContext*	con,
    CVMJITIRNodePtr             thisNode,
    CVMBool                     isInstanceOf)
{
    CVMRMResource *cbRes = popResource(con);
    CVMRMResource *src = popResource(con);
    int srcReg;
    void *helperFunction;
#ifdef CVM_TRACE_JIT
    char *helperFunctionName;
#endif

    if (isInstanceOf){
        CVMJITprintCodegenComment(("Do instanceof:"));
        helperFunction = (void*)CVMCCMruntimeInstanceOfGlue;
#ifdef CVM_TRACE_JIT
        helperFunctionName = "CVMCCMruntimeInstanceOfGlue";
#endif
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeInstanceOf);
    } else {
        CVMJITprintCodegenComment(("Do checkcast:"));
        helperFunction = (void*)CVMCCMruntimeCheckCastGlue;
#ifdef CVM_TRACE_JIT
        helperFunctionName = "CVMCCMruntimeCheckCastGlue";
#endif
        CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeCheckCast);
    }

    CVMJITprintCodegenComment(("Note: ARG3=cb to check against"));

    /* Make sure we flush dirty locals if we might exit without
       a "major" spill */
    CVMRMsynchronizeJavaLocals(con);

    cbRes = CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), cbRes,
				     CVMCPU_ARG3_REG);
#ifdef CVM_JIT_HAVE_CALLEE_SAVED_REGISTER
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), src,
			   CVMRM_SAFE_SET, ~CVMRM_SAFE_SET);
    srcReg = CVMRMgetRegisterNumber(src);
#else
    CVMRMpinResourceStrict(CVMRM_INT_REGS(con), src,
			   ARG1, ~ARG1);
    srcReg = CVMRMgetRegisterNumber(src);
    CVMRMunpinResource(CVMRM_INT_REGS(con), src); /* unpin to in order to have it spilled */
#endif

    /* A "major" spill is not needed here.  The helpers return
       without becoming gc-safe except when an exception is thrown,
       and in that case, we only care about the locals which are always
       flushed to memory anyway. */
    CVMRMminorSpill(con, ARG3);

    /* do shortcut tests: null can cast to anything, but
     * isn't instanceof anything
     * We can do the test and set ARG1 to 0 in one instruction.
     */
    CVMJITaddCodegenComment((con, "set cc to \"eq\" if object is null"));
#ifdef CVMCPU_HAS_ALU_SETCC
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, CVMCPU_ARG1_REG, srcReg,
			   CVMJIT_SETCC);
#else
    CVMCPUemitMoveRegister(con, CVMCPU_MOV_OPCODE, CVMCPU_ARG1_REG, srcReg,
			   CVMJIT_NOSETCC);
    /* NOTE: normally we would emit a compare instruction here after
     * having called an ALU emitter with CVMJIT_SETCC. Howver, the setcc is
     * for the benefit of the assembler glue, so platforms without SETCC
     * support will just need to do the compare in the glue code.
     */
#endif

    /* call the helper glue with the cc set to "eq" if the object is null */
    CVMJITaddCodegenComment((con, "call %s", helperFunctionName));
    CVMJITsetSymbolName((con, helperFunctionName));
    CVMCPUemitAbsoluteCall(con, helperFunction,
			   CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);

    /* reserve a word for the guessCB */
    CVMJITaddCodegenComment((con, "guessCB i.e. last successful cast CB"));
    CVMJITemitWord(con, 0); /* TODO(rr): should we align this word?? */
    CVMJITcaptureStackmap(con, 0);

    CVMJITprintCodegenComment((isInstanceOf ?
                              "instanceofDone:" : "checkcastDone:"));

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), cbRes);

    if (!isInstanceOf) {
        /* For checkcast, return the original object:*/
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), src, thisNode);
        pushResource(con, src);
    } else {
        CVMRMResource *dest;
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);
        dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),
					CVMCPU_RESULT1_REG, 1);
        CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
        pushResource(con, dest);
    }
}

%}

// Purpose: CHECKCAST(object, classBlock)
regObj: CHECKCAST regObj regAddr : 90 : SET_AVOID_C_CALL($$); :
#ifdef CVM_JIT_HAVE_CALLEE_SAVED_REGISTER
    SET_TARGET2_1($$, ARG3);
#else
    SET_TARGET2($$, ARG1, ARG3);
#endif
: : {
        instanceTest(con, $$, CVM_FALSE);
    };

// Purpose: value32 = INSTANCEOF(object, classBlock)
reg32: INSTANCEOF regObj regAddr : 90 : SET_AVOID_C_CALL($$); :
#ifdef CVM_JIT_HAVE_CALLEE_SAVED_REGISTER
    SET_TARGET2_1($$, ARG3);
#else
    SET_TARGET2($$, ARG1, ARG3);
#endif
: : {
        instanceTest(con, $$, CVM_TRUE);
    };

%{

static void
resolveConstant(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode,
		CVMRMregset target, CVMRMregset avoid)
{
    /*
     *          @ do a major spill first
     *
     * startpc:
     *          ldr     Rx, cachedConstant(PC)
     *          ldr     ARG3, =cpIndex    @ or: mov ARG3, =cpIndex.
     *          bl      =helper
     * cachedConstant:
     *          .word   -1
     * resolveReturn1:
     *          ldr     Rx, cachedConstant(PC)
     * resolveReturn2:
     *          @ capture stackmap here
     *
     * The helper will store the resolved constant at cachedConstant,
     * which it locates by the value in the link register.
     *
     * If static initialization of a class is needed, the helper will
     * do the initialization in a non-recursive way via the
     * interpreter loop, which will resume execution at resolveReturn1
     * when initialization is complete.
     *
     * If static initialization is currently being done by the current
     * thread, then the helper just returns immediately to
     * resolveReturn1.
     *
     * If static initialization is not needed, then the *second*
     * instruction of the above is patched to be the following:
     *
     *          b resolveReturn2
     *
     * NOTE: It's possible to do the above without the first ldr. However
     * there are two advantages to doing it this. The first is that by
     * doing the ldr and then the branch, you avoid a processor stall
     * if the first instruction after resolveReturn2 references the
     * cachedConstant, which it normally does. The 2nd is that we can't
     * safely patch the first instruction generated, because it might
     * be the first instruction of a basic block, and therefor may get
     * patched to handle a gc-rendezvous.
     *
     * If the platform does not support PC relative load, the
     * `ldr Rx, cachedConstant(PC)' instructions can be implemented as:
     *
     *          ldr      Rx, =physicalPC + offsetToCachecConstantPC
     *          ldr      Rx, [Rx, #0]
     */

    CVMRMResource *dest;
    int destRegNo;
    int loadCachedConstantPC;
    void *helper = NULL;
#ifdef CVM_TRACE_JIT
    char *helperName = NULL;
#endif
    CVMConstantPool* cp;
    CVMJITIRNode*    cpIndexNode;
    CVMUint16        cpIndex;
    CVMBool          needsCheckInit;

#undef setHelper

#ifdef CVM_TRACE_JIT
#define setHelper(x)				\
    helper = (void*)x##Glue;				\
    helperName = #x; 				\
    CVMJITstatsRecordInc(con, CVMJIT_STATS_##x)
#else
#define setHelper(x)				\
    helper = (void*)x##Glue;				\
    CVMJITstatsRecordInc(con, CVMJIT_STATS_##x)
#endif

#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
    /* Make JSP point just past the last argument currently on the stack */
    CVMSMadjustJSP(con);
#endif


    cp = CVMcbConstantPool(CVMmbClassBlock(con->mb));
    cpIndexNode = CVMJITirnodeGetLeftSubtree(thisNode);
    cpIndex = CVMJITirnodeGetConstant32(cpIndexNode)->j.i;
    
    needsCheckInit =
       ((CVMJITirnodeGetUnaryNodeFlag(thisNode) & CVMJITUNOP_CLASSINIT) != 0);

    if (needsCheckInit) {
        switch(CVMJITirnodeGetTag(cpIndexNode) >> CVMJIT_SHIFT_OPCODE) {
            case CVMJIT_CONST_NEW_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a class:"));
                setHelper(CVMCCMruntimeResolveNewClassBlockAndClinit);
                break;
            case CVMJIT_CONST_GETSTATIC_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static field:"));
                setHelper(CVMCCMruntimeResolveGetstaticFieldBlockAndClinit);
                break;
            case CVMJIT_CONST_PUTSTATIC_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static field:"));
                setHelper(CVMCCMruntimeResolvePutstaticFieldBlockAndClinit);
                break;
            case CVMJIT_CONST_STATIC_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a static method:"));
                setHelper(CVMCCMruntimeResolveStaticMethodBlockAndClinit);
                break;
            default:
                CVMassert(CVM_FALSE); /* Unexpected irnode tag. */
        }
    } else {
        switch(CVMJITirnodeGetTag(cpIndexNode) >> CVMJIT_SHIFT_OPCODE) {
            case CVMJIT_CONST_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a class:"));
                setHelper(CVMCCMruntimeResolveClassBlock);
                break;
            case CVMJIT_CONST_ARRAY_CB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an array class:"));
                setHelper(CVMCCMruntimeResolveArrayClassBlock);
                break;
            case CVMJIT_CONST_GETFIELD_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an instance field:"));
                setHelper(CVMCCMruntimeResolveGetfieldFieldOffset);
                break;
            case CVMJIT_CONST_PUTFIELD_FB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an instance field:"));
                setHelper(CVMCCMruntimeResolvePutfieldFieldOffset);
                break;
            case CVMJIT_CONST_SPECIAL_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a 'special' method:"));
                setHelper(CVMCCMruntimeResolveSpecialMethodBlock);
                break;
            case CVMJIT_CONST_INTERFACE_MB_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving an 'interface' method:"));
                setHelper(CVMCCMruntimeResolveMethodBlock);
                break;
            case CVMJIT_CONST_METHOD_TABLE_INDEX_UNRESOLVED:
                CVMJITprintCodegenComment(("Resolving a method table index:"));
                setHelper(CVMCCMruntimeResolveMethodTableOffset);
                break;
            default:
                CVMassert(CVM_FALSE); /* Unexpected irnode tag. */
        }
    }

    /* Note: It is important to spill and evict every register because we may
       not return from the helper via a normal route.  It's almost like an
       exception being thrown except that we may return to this method and
       expect its state to be preserved just as if we haven't done the
       resolution yet. */
    CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);
    loadCachedConstantPC = CVMJITcbufGetLogicalPC(con);

    /*
     * Load the cached constant. We don't know exactly where it will be
     * yet, so we'll need to come back and patch this instruction later.
     */
    dest = CVMRMgetResource(CVMRM_INT_REGS(con), target, avoid, 1);
    destRegNo = CVMRMgetRegisterNumber(dest);

    /*
     * Set ARG3 = cpIndex. This instruction eventually gets patched to branch
     * around the call to the helper once class initialization is done.
     */
    CVMJITaddCodegenComment((con, "ARG3 = cpIndex"));
    CVMJITsetSymbolName((con, "cpIndex"));
    CVMCPUemitLoadConstant(con, CVMCPU_ARG3_REG, (CVMInt32)cpIndex);

    /*
     * Do the helper call. Note that we can't allow the contant pool to be
     * dumped or else the helper glue won't be able to locate the instruction
     * to patch.
     */
    CVMJITaddCodegenComment((con, "call %s", helperName));
    CVMJITsetSymbolName((con, helperName));
    CVMCPUemitAbsoluteCall(con, helper, CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH, 0);

    /* emit the cachedConstant word */
    CVMJITaddCodegenComment((con, "cachedConstant"));
    CVMJITemitWord(con, -1);

    CVMJITcaptureStackmap(con, 0); /* This is the return PC in the frame. */

    /*
     * Load the cached constant, which is in the word before the
     * current instruction.
     */
    CVMJITaddCodegenComment((con, "load cachedConstant"));
    CVMCPUemitMemoryReferencePCRelative(con, CVMCPU_LDR32_OPCODE,
					destRegNo, -(int)sizeof(CVMUint32));

    /* Occupy the target register with the resolved value: */
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
    pushResource(con, dest);
}

#ifdef CVM_JIT_INLINE_CHECKINIT
static void
inlineCheckInit(CVMJITCompilationContext *con, CVMClassBlock *cb);
#endif

static void
doCheckInit(CVMJITCompilationContext *con, CVMJITIRNodePtr thisNode)
{
   /* SVMC_JIT d022609 (ML) 2004-01-23. We avoid calling the
    * initializer if initialization has already been performed.
    */
   /*
    *          @ do a major spill first
    *
    *          ldr     ARG3, =cb      @ load cb to intialize
    *          bl      =helper        @ call helper to intialize cb
    *          @ capture stackmap here
    *
    *          @ If the class is aready initialized, then the helper will
    *          @ patch the `bl' instruction with a `nop'. 
    */

   /* SVMC_JIT PJ 2004-03-01
                 fix syntax error, occurred when
                 CVM_JIT_ENABLE_ADDRESS_CONSTANTS was defined
   */
   CVMClassBlock *cb =
      CVMJITirnodeGetConstantAddr(CVMJITirnodeGetLeftSubtree(thisNode))->cb;
   
#ifndef CVMCPU_HAS_POSTINCREMENT_STORE
   /* Make JSP point just past the last argument currently on the stack */
   CVMSMadjustJSP(con);
#endif
   
   /* Note: It is important to spill and evict every register because we may
      not return from the helper via a normal route.  It's almost like an
      exception being thrown except that we may return to this method and
      expect its state to be preserved just as if we haven't done the
      checkinit yet. */
   CVMRMmajorSpill(con, CVMRM_EMPTY_SET, CVMRM_EMPTY_SET);
   
   CVMJITprintCodegenComment(("Do checkinit:"));
   
   /*
    * Load the cb into an argument register for the helper. 
    */
   CVMJITaddCodegenComment((con, "ARG3 = %C", cb));
   CVMJITsetSymbolName((con, "%C", cb));
   CVMCPUemitLoadAddrConstant(con, CVMCPU_ARG3_REG, (CVMAddr)cb);
   
   /*
    * Do the helper call. Note that we can't allow the constant
    * pool to be dumped or else the helper glue won't be able to
    * locate the instruction to patch.
    */
   CVMJITaddCodegenComment((con,
			    "call CVMCCMruntimeRunClassInitializerGlue"));
   CVMJITsetSymbolName((con, "CVMCCMruntimeRunClassInitializerGlue"));
   CVMJITstatsRecordInc(con, 
			CVMJIT_STATS_CVMCCMruntimeRunClassInitializer);
   CVMCPUemitAbsoluteCall(
      con, (void*)CVMCCMruntimeRunClassInitializerGlue,
      CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH, 0);
   /* checkinitDone. */
   CVMJITcaptureStackmap(con, 0); /* This is the return PC in the frame. */
   CVMJITprintCodegenComment(("checkinitDone:"));
}

%}


// grammar rules specific to IA-32. by choosing lower costs for these
// rules than for those in the share, risc or cisc directories, we can
// shadow the more general rules by more platform specific (more
// efficient) ones.



%{

static void doRegShift(CVMJITCompilationContext *con, int shiftOp, 
		       CVMJITIRNodePtr thisNode, CVMRMregset target, CVMRMregset avoid)
{
   CVMRMResource *dest;
   CVMInt32 destRegID;
   CVMRMResource *rhs = popResource(con);
   CVMRMResource *lhs = popResource(con);
   CVMRMpinResourceStrict(CVMRM_INT_REGS(con), rhs, 
			  (1U << CVMX86_ECX) /* IA-32 requires shift amount in ECX */, 
			  CVMRM_EMPTY_SET);
   CVMRMpinResource(CVMRM_INT_REGS(con), lhs, target, avoid);
   destRegID = CVMRMgetRegisterNumber(lhs);
   CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
   CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
   dest = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), (1U << destRegID), 
				 avoid, 1);
   CVMassert(destRegID == CVMRMgetRegisterNumber(dest));
   CVMCPUemitShiftByRegister(con, shiftOp, destRegID, destRegID, CVMX86_ECX);
   CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, thisNode);
   pushResource(con, dest);
}

static void doIDivOrIRem(CVMJITCompilationContext* con, int opcode,
			 CVMJITIRNodePtr thisNode)
{
  CVMRMResource* dividend;
  CVMRMResource* divisor;
  CVMRMResource* quotient;
  CVMRMResource* remainder;
    //         normal case                           special case
    //
    // input : eax: dividend                         min_int
    //         reg: divisor                          -1
    //
    // output: eax: quotient  (= eax idiv reg)       min_int
    //         edx: remainder (= eax irem reg)       0
    //
  divisor = popResource(con);
  dividend = popResource(con);
  CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), dividend, CVMX86_EAX);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), dividend);
  CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), divisor, CVMX86_ECX);
  CVMRMrelinquishResource(CVMRM_INT_REGS(con), divisor);
  quotient = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMX86_EAX, 1);
  CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), quotient, CVMX86_EAX);
  remainder = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMX86_EDX, 1);
  CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), remainder, CVMX86_EDX);
  /* treated as 64 bit operation by the emitter, because the result
   * occupies EAX:EDX registers on IA-32. */
  CVMCPUemitBinaryALU64(con, opcode, CVMX86_EAX, CVMX86_EAX, CVMX86_ECX);
  if (opcode == CVMCPU_DIV_OPCODE) {
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), remainder);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), quotient, thisNode);
    pushResource(con, quotient);
  }
  else {
    CVMassert(opcode == CVMCPU_REM_OPCODE);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), quotient);
    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), remainder, thisNode);
    pushResource(con, remainder);
  }
}

#ifdef CVM_JIT_INLINE_NEWARRAY
static void
inlineNewArray(CVMJITCompilationContext *con)
   {
      int branch_ARR_BADINDEX;
      int branch_ARR_PASSASS;
      int branch_ARR_LOOPTEST;
      int branch_ARR_INITLOOP;
      int branch_ARR_PASSUNLOCKASS;
      int branch_ARR_ENDINIT;
      int branch_ARR_NEWFIN;
      CVMCPUAddress addr;
      
#define LEN     CVMCPU_ARG2_REG
#define ARRCB   CVMCPU_ARG3_REG
#define OBJSIZE CVMCPU_ARG4_REG
      
      CVMJITaddCodegenComment((con, "begin inlining CVMCCMruntimeNewArray"));
      
      /*
       * This is different from the explicit assembly code since no return
       * address is stored on the stack (no call to glue code)!
       *
       *     | 4. word (free)  <- 12 + OFFSET_CVMCCExecEnv_ccmStorage(%esp)
       *     +---------------
       *     | 3. word (free)  <-  8 + OFFSET_CVMCCExecEnv_ccmStorage(%esp)
       *     +---------------
       *     | 2. LEN          <-  4 + OFFSET_CVMCCExecEnv_ccmStorage(%esp)
       *     +---------------
       *     | 1. ARRCB        <-  0 + OFFSET_CVMCCExecEnv_ccmStorage(%esp)
       *     +---------------
       *     :	
       *     | XXXXXXXXXXXX    <-      0(%esp)
       *     +---------------
       */
      
      
      // Check if length is negative or too big. If it is, bail out
      CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_AL,
				CVMCPU_ARG2_REG, 0x10000000);
      emitConditionalInstructions(con, CVMCPU_COND_HI, {
	 CVMCPUemitAbsoluteCall(con, 
				(void*)CVMCCMruntimeNewArrayBadindexGlue,
				CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	 CVMJITcaptureStackmap(con, 0);
	 branch_ARR_NEWFIN = CVMJITcbufGetLogicalPC(con); 
	 CVMCPUemitBranch(con, branch_ARR_NEWFIN, CVMCPU_COND_AL);
      });
      
      /*
	Now compute instance size of the array
	OBJSIZE = roundup(elemsize * length + 12)
	which is equal to
	(elemsize * length + 15) & ~3
      */
      /* In assembly glue code, instead of 4+ offsetof 7+offsetof is used: */
      CVMCPUinit_Address_base_disp(&addr, CVMX86_ESP, 4 + offsetof(CVMCCExecEnv, ccmStorage),
				   CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMJITaddCodegenComment((con, "EDX = *(ESP + 4 + OFFSET_CVMCCExecEnv_ccmStorage)"));
      CVMX86movl_mem_reg(con, addr, LEN); /* save length */
      CVMX86mull_reg(con, LEN);           /* elemsize * length, overwrites EAX, ie ARG1_REG. */
      CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE, OBJSIZE, CVMCPU_ARG1_REG, 
				  15, CVMJIT_SETCC);
      CVMCPUemitBinaryALUConstant(con, CVMCPU_BIC_OPCODE, OBJSIZE, OBJSIZE, 
				  0x3, CVMJIT_NOSETCC);
#undef LEN
#undef OBJSIZE
      
#define SCRATCH CVMCPU_ARG2_REG
      /* lock the heap */
      CVMCPUemitLoadConstant(con, SCRATCH, 1);
      CVMCPUinit_Address_disp(&addr, (CVMUint32)&CVMglobals.fastHeapLock, CVMCPU_MEMSPEC_ABSOLUTE);
      CVMX86xchg_reg_mem(con, SCRATCH, addr);
      CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_AL, SCRATCH, 0);
      emitConditionalInstructions(con, CVMCPU_COND_NE, {
	 CVMCPUemitAbsoluteCall(con, 
				(void*)CVMCCMruntimeNewArrayGoslowGlue,
				CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	 CVMJITcaptureStackmap(con, 0);
	 CVMCPUemitBranch(con, branch_ARR_NEWFIN, CVMCPU_COND_AL);
      });
#undef SCRATCH
      
#define OBJ       CVMCPU_ARG1_REG
#define TOPPTR    CVMCPU_ARG2_REG
#define ALLOCNEXT CVMCPU_ARG4_REG
      
      CVMCPUinit_Address_disp(&addr, (CVMUint32)&CVMglobals.allocPtrPtr, CVMCPU_MEMSPEC_ABSOLUTE);
      CVMX86movl_reg_mem(con, OBJ, addr);
      CVMCPUinit_Address_disp(&addr, (CVMUint32)&CVMglobals.allocTopPtr, CVMCPU_MEMSPEC_ABSOLUTE);
      CVMX86movl_reg_mem(con, TOPPTR, addr);
      CVMCPUinit_Address_base_disp(&addr, OBJ, 0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_reg_mem(con, OBJ, addr); /* OBJ <- allocPtr == function result */
      CVMCPUinit_Address_base_disp(&addr, TOPPTR, 0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_reg_mem(con, TOPPTR, addr); 
      /* CVMCPU_ARG1_REG holds OJBSIZE*/
      CVMCPUemitBinaryALURegister(con, CVMCPU_ADD_OPCODE, ALLOCNEXT, ALLOCNEXT,
				  OBJ, CVMJIT_SETCC); /* allocNext (allocPtr + size) */
      /* Check for overflow */
      emitConditionalInstructions(con, CVMCPU_COND_OV, {
	 CVMCPUemitAbsoluteCall(con, 
				(void*)CVMCCMruntimeNewArrayGounlockandslowGlue,
				CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	 CVMJITcaptureStackmap(con, 0);
	 CVMCPUemitBranch(con, branch_ARR_NEWFIN, CVMCPU_COND_AL);
      });
      CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_AL, ALLOCNEXT, TOPPTR);
      emitConditionalInstructions(con, CVMCPU_COND_HI, {
	 CVMCPUemitAbsoluteCall(con, (void*)CVMCCMruntimeNewArrayGounlockandslowGlue,
				CVMJIT_CPDUMPOK, CVMJIT_NOCPBRANCH, 0);
	 CVMJITcaptureStackmap(con, 0);
	 CVMCPUemitBranch(con, branch_ARR_NEWFIN, CVMCPU_COND_AL);
      });
#define ALLOCPTRPTR CVMCPU_ARG2_REG
      CVMCPUinit_Address_disp(&addr, (CVMUint32)&CVMglobals.allocPtrPtr, CVMCPU_MEMSPEC_ABSOLUTE);
      CVMX86movl_reg_mem(con, ALLOCPTRPTR, addr);
      CVMCPUinit_Address_base_disp(&addr, ALLOCPTRPTR, 0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_mem_reg(con, addr, ALLOCNEXT); 
#undef ALLOCPTRPTR
      
#define LEN CVMCPU_ARG2_REG
      /* Initialize the object header. */
      CVMCPUinit_Address_base_disp(&addr, OBJ, 0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_mem_reg(con, addr, ARRCB);  /* cb is first field of object */
      CVMCPUinit_Address_base_disp(&addr, OBJ, 4, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_mem_imm32(con, addr, 2); /* CVM_LOCKSTATE_UNLOCKED: initialize variousWord */
	    /* In explicit glue code instead of 4+offsetof 8+offsetof is used: */
      CVMCPUinit_Address_base_disp(&addr, CVMX86_ESP, 4 + offsetof(CVMCCExecEnv, ccmStorage),
				   CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_reg_mem(con, LEN, addr); 
      CVMCPUinit_Address_base_disp(&addr, OBJ, 8, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_mem_reg(con, addr, LEN);  
#undef LEN
#define FIELD CVMCPU_ARG2_REG
      CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE, FIELD, OBJ,
				  12, CVMJIT_SETCC);
      branch_ARR_LOOPTEST = CVMJITcbufGetLogicalPC(con); 
      CVMCPUemitBranch(con, branch_ARR_LOOPTEST, CVMCPU_COND_AL);
      branch_ARR_INITLOOP = CVMJITcbufGetLogicalPC(con); 
      CVMCPUinit_Address_base_disp(&addr, FIELD, 0, CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
      CVMX86movl_mem_imm32(con, addr, 0); 
      CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE, FIELD, FIELD,
				  4, CVMJIT_SETCC); /* Next object field */
      CVMJITfixupAddress(con, branch_ARR_LOOPTEST, CVMJITcbufGetLogicalPC(con),
			 CVMJIT_COND_BRANCH_ADDRESS_MODE);
      CVMCPUemitCompareRegister(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_AL, FIELD, ALLOCNEXT);
      CVMCPUemitBranch(con, branch_ARR_INITLOOP, CVMCPU_COND_NE);
#undef FIELD
      
      branch_ARR_ENDINIT = CVMJITcbufGetLogicalPC(con); 
      /* unlock the heap */
      CVMX86lock(con);
      CVMCPUinit_Address_disp(&addr, (CVMUint32)&CVMglobals.fastHeapLock, CVMCPU_MEMSPEC_ABSOLUTE);
      CVMX86decl_mem(con, addr);
      
      CVMJITfixupAddress(con, branch_ARR_NEWFIN, CVMJITcbufGetLogicalPC(con),
			 CVMJIT_COND_BRANCH_ADDRESS_MODE);
      
#undef ALLOCNEXT
#undef OBJ
#undef ARRCB
#undef TOPPTR
      CVMJITprintCodegenComment(("end inlining CVMCCMruntimeNewArrayGlue"));
   }
#endif /* CVM_JIT_INLINE_NEWARRAY */

static void
extendSmallTypeTo32Bits(CVMJITCompilationContext *con, int reg, char sig)
{
   if( sig == CVM_SIGNATURE_BYTE)
      CVMX86movsxb_reg_reg(con,reg,reg);
   else if(sig == CVM_SIGNATURE_BOOLEAN)
      CVMX86movzxb_reg_reg(con,reg,reg);
   else if( sig == CVM_SIGNATURE_SHORT)
      CVMX86movsxw_reg_reg(con,reg,reg);
   else if( sig == CVM_SIGNATURE_CHAR)
      CVMX86movzxw_reg_reg(con,reg,reg);
}

#ifdef CVM_JIT_INLINE_CHECKINIT
static void
inlineCheckInit(CVMJITCompilationContext *con, CVMClassBlock *cb)
{
   int branch_L1F1;
   CVMCPUAddress ee;
   CVMCPUAddress addr;
   
   CVMCPUinit_Address_base_disp(
      &ee, CVMX86_ESP, 8 + offsetof(CVMCCExecEnv, eeX) /* ee */, 
      CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
   
   CVMCPUinit_Address_disp(
      &addr, (int)cb + offsetof(CVMClassBlock, clinitEEX.ee),
      CVMCPU_MEMSPEC_ABSOLUTE);
   CVMJITaddCodegenComment((con, "EAX = (cb->clinitEEX.ee)"));
   CVMX86movl_reg_mem(con, CVMX86_EAX, addr);
      
   CVMJITaddCodegenComment((con, "EAX &= CVM_CLINITEE_CLASS_INIT_FLAG"));
   CVMX86andl_reg_imm32(con, CVMX86_EAX, CVM_CLINITEE_CLASS_INIT_FLAG);
   branch_L1F1 = CVMJITcbufGetLogicalPC(con);
   CVMJITaddCodegenComment((con, "if EAX != 0, checkInitDone."));
   CVMCPUemitBranch(con, 0, CVMCPU_COND_NE)/* init not needed. has been done. */;
   /* otherwise, either init is being done by the current thread
    * (uncommon case) or else must be performed from scratch
    * (common case). we do not bother to check for the uncommon
    * case. in this case the C call will return immediately. */
   CVMJITaddCodegenComment((con,
			    "call CVMCCMruntimeRunClassInitializerGlue"));
   CVMJITsetSymbolName((con, "CVMCCMruntimeRunClassInitializerGlue"));
   CVMJITstatsRecordInc(con, 
			CVMJIT_STATS_CVMCCMruntimeRunClassInitializer);
   CVMCPUemitAbsoluteCall(
      con, (void*)CVMCCMruntimeRunClassInitializerGlue,
      CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH, 0);
   /* checkinitDone. */
   CVMJITcaptureStackmap(con, 0); /* This is the return PC in the frame. */
   CVMJITprintCodegenComment(("checkinitDone:"));
   CVMJITfixupAddress(con, branch_L1F1, CVMJITcbufGetLogicalPC(con),
		      CVMJIT_COND_BRANCH_ADDRESS_MODE);
}
#endif /* CVM_JIT_INLINE_CHECKINIT */
%}

// Assumptions: 
//  o synthesis: avoid rule describes registers in which the target must not be
//               evaluated since it is destroyed by side effects??
//  o inheritance: This seems to propagate register information from the root to
//                 the children nodes. I.e. if a node expects its arguments in certain
//                 registers, this information is inherited to the children from the
//                 root node. In this case: arguments are expected in EAX and ECX.
// No specific version for ICONST_32 provided because of bytewise emitting.
reg32: IDIV32  reg32 reg32 : 40 : : 
  SET_TARGET2($$, (1U << CVMX86_EAX), (1U << CVMX86_ECX)); : : {
  doIDivOrIRem(con, CVMCPU_DIV_OPCODE, $$);
};

reg32: IREM32  reg32 reg32 : 40 : : 
  SET_TARGET2($$, (1U << CVMX86_EAX), (1U << CVMX86_ECX)); : : {
  doIDivOrIRem(con, CVMCPU_REM_OPCODE, $$);
};

// Purpose: valueLong = (long) valueInt.
reg64: I2L reg32 : 20 : :     
SET_TARGET1($$, (1<<CVMX86_EAX) ); : : {
	
	CVMRMResource* src = popResource(con);
	CVMRMResource* dest;

	CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), src, CVMX86_EAX);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), src);

    dest = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con),CVMX86_EAX, 2);

    CVMX86cdql(con);

	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, $$);
	pushResource(con, dest);
    };

reg64: IMUL64 reg64 reg64 : 30 : :   
SET_TARGET2($$, (1U << CVMX86_EAX), (1U << CVMX86_EBX)); : : {

    CVMRMResource* rhs = popResource(con);
    CVMRMResource* lhs = popResource(con);
    CVMRMResource* dst;
    CVMRMResource* tmp;
    CVMCPUAddress lhs_lo, lhs_hi, rhs_lo, rhs_hi;
    CVMInt32 dstRegID = -1; 
    CVMInt32 tmpRegID = -1; 
    
    CVMJITaddCodegenComment((con, "begin IMUL64."));
    {
       int lhsFrameReg, lhsFrameOffset;
       /* ensure `lhs' is flushed */
       CVMRMflushResource(CVMRM_INT_REGS(con), lhs);
       /* get frame location of `lhs' */
       if (CVMRMisLocal(lhs)) {
       CVMCPUgetFrameReference(con, CVMCPU_FRAME_LOCAL, lhs->localNo, 
			       &lhsFrameReg, &lhsFrameOffset);
       } else {
       CVMCPUgetFrameReference(con, CVMCPU_FRAME_TEMP, lhs->spillLoc, 
			       &lhsFrameReg, &lhsFrameOffset);
       }
       CVMCPUinit_Address_base_disp(&lhs_lo, lhsFrameReg, 0 + lhsFrameOffset, 
				    CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
       CVMCPUinit_Address_base_disp(&lhs_hi, lhsFrameReg, 4 + lhsFrameOffset, 
				    CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
       /* free the register in which `lhs' might still reside */
       CVMRMunpinResource(CVMRM_INT_REGS(con), lhs);
    }
    
    {
       int rhsFrameReg, rhsFrameOffset;
       /* ensure `rhs' is flushed */
       CVMRMflushResource(CVMRM_INT_REGS(con), rhs);
       /* get frame location of `rhs' */
       if (CVMRMisLocal(rhs)) {
       CVMCPUgetFrameReference(con, CVMCPU_FRAME_LOCAL, rhs->localNo, 
			       &rhsFrameReg, &rhsFrameOffset);
       } else {
       CVMCPUgetFrameReference(con, CVMCPU_FRAME_TEMP, rhs->spillLoc, 
			       &rhsFrameReg, &rhsFrameOffset);
       }
       CVMCPUinit_Address_base_disp(&rhs_lo, rhsFrameReg, 0 + rhsFrameOffset, 
				    CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
       CVMCPUinit_Address_base_disp(&rhs_hi, rhsFrameReg, 4 + rhsFrameOffset, 
				    CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
       /* free the register in which `rhs' might still reside */
       CVMRMunpinResource(CVMRM_INT_REGS(con), rhs);
    }

    dst = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMX86_EAX, 2);
    dstRegID = CVMRMgetRegisterNumber(dst);
    tmp = CVMRMgetResource(CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
    tmpRegID = CVMRMgetRegisterNumber(tmp);
    CVMCPUemitIMUL64(con, dstRegID, rhs_lo, rhs_hi, lhs_lo, lhs_hi, tmpRegID);
    
    CVMJITprintCodegenComment(("end IMUL64."));

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), tmp); 

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dst, $$);
    pushResource(con, dst);
};

// Purpose: LOCAL64 = value64.
//
// SVMC_JIT d022609 (ML) 2004-04-20. TODO: experience shows that
// this rule must have little cost (10), otherwise the matcher tries
// to use FP rules. on the other hand, the corresponding FP rule
// (ASSIGN LOCAL64 freg64) has cost 11. this is undesired, because the
// integer rule has two memory accesses and should be more expensive
// than the FP rule. Fix!
root:	ASSIGN LOCAL64 reg64 : 10 : : : : {
	CVMRMResource * rhs = popResource(con);
	CVMJITLocal   * lhs = CVMJITirnodeGetLocal(
	    CVMJITirnodeGetLeftSubtree($$));
	CVMRMpinResource(CVMRM_INT_REGS(con), rhs,
			 CVMRM_ANY_SET, CVMRM_EMPTY_SET);
	CVMRMstoreJavaLocal(CVMRM_INT_REGS(con), rhs, 2, CVM_FALSE,
			    lhs->localNo);
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), rhs);
    };

%{
#ifdef CVMJIT_SIMPLE_SYNC_METHODS
#if CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS

/*
 * Intrinsic emitter for fastlock CAS version of CVM.simpleLockGrab().
 *
 * Attempts to lock the object using the reserved 
 * ee->simpleSyncReservedOwnedMonitor. If the object is already locked,
 * returns FALSE. Otherwise returns TRUE.
 */

static void
simpleLockGrabEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* lockRec;
    CVMRMResource* dest;
    int objRegID, objHdrRegID, lockRecRegID, destRegID, eeRegID;
    int fixupPC1; /* To patch the conditional branches */
#ifndef CVMCPU_EE_REG
    CVMRMResource *eeRes;
#endif

    struct CVMJITCompileExpression_rule_computation_state *goal_top =
	(struct CVMJITCompileExpression_rule_computation_state *)
        (con->goal_top);

    if (CVMRMisConstant(obj) && CVMRMgetConstant(obj) == 0) {
	/* We know the object is a null, so an NPE would be thrown before
	 * code generated here is executed. Therefore we don't need to
	 * generate anything. We'll stuff the obj resource into the
	 * intrinisic node just to keep JCS happy.
	 */
	CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), obj, intrinsicNode);
	pushResource(con, obj);
	return;
    }

#ifdef CVM_DEBUG
    CVMJITprintCodegenComment(("DEBUG-ONLY CODE"));
    {
	/* Store the mb of the currently executing Simple Sync method into
	 * ee->currentSimpleSyncMB. */
	CVMRMResource* simpleSyncMBRes;
	CVMJITMethodContext* mc = con->inliningStack[con->inliningDepth-1].mc;
	/* load the Simple Sync mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M", mc->cb, mc->mb));
	simpleSyncMBRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)mc->mb);
	/* Get the ee: */
#ifndef CVMCPU_EE_REG
	eeRes = CVMRMgetResource(
	    CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
	eeRegID = CVMRMgetRegisterNumber(eeRes);
	CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
	CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
					 CVMoffsetof(CVMCCExecEnv, eeX));
#else
	eeRegID = CVMCPU_EE_REG;
#endif
	/* Store the Simple Sync mb into ee->currentSimpleSyncMB. */
	CVMJITaddCodegenComment((con,
				 "ee->currentSimpleSyncMB = %C.%M",
				 mc->cb, mc->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(simpleSyncMBRes), eeRegID,
	     CVMoffsetof(CVMExecEnv,currentSimpleSyncMB));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), simpleSyncMBRes);
    }
    {
	/* Store the mb of the currently executing method into
	 * ee->currentMB. */
	CVMRMResource* mbRes;
	/* load the current mb address into a register */
        CVMJITsetSymbolName((con, "mb %C.%M",
			     CVMmbClassBlock(con->mb), con->mb));
	mbRes = CVMRMgetResourceForConstant32(
            CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_SAFE_SET,
	    (CVMUint32)con->mb);
	/* Store the Simple Sync mb into ee->currentMB. */
	CVMJITaddCodegenComment((con,
				 "ee->currentMB = %C.%M",
				 CVMmbClassBlock(con->mb), con->mb));
	CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
	    CVMRMgetRegisterNumber(mbRes), 
            eeRegID, CVMoffsetof(CVMExecEnv,currentMB));
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), mbRes);
    }
#ifndef CVMCPU_EE_REG
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif
    CVMJITprintCodegenComment(("END OF DEBUG-ONLY CODE"));
#endif /* CVM_DEBUG */

    /* get all our resources ready */
    objHdr = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMX86_EAX, 1);
    CVMRMpinResource(CVMRM_INT_REGS(con), obj, CVMRM_ANY_SET, CVMRM_SAFE_SET);
    lockRec = CVMRMgetResource(CVMRM_INT_REGS(con),
			      CVMRM_ANY_SET, CVMRM_SAFE_SET, 1);
#ifndef CVMCPU_EE_REG
    eeRes = CVMRMgetResource(
        CVMRM_INT_REGS(con), CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
#endif

    /* ... and get all the register numbers */
    objRegID    = CVMRMgetRegisterNumber(obj);
    objHdrRegID = CVMRMgetRegisterNumber(objHdr);
    lockRecRegID = CVMRMgetRegisterNumber(lockRec);

    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* get sync bits from object header, borrow lockRecRegID for now */
    CVMJITaddCodegenComment((con, "get obj sync bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
				lockRecRegID, objHdrRegID,
				CVM_SYNC_MASK, CVMJIT_NOSETCC);
    /* check if object is unlocked */
    CVMJITaddCodegenComment((con, "check if obj unlocked"));
    CVMCPUemitCompareConstant(con, CVMCPU_CMP_OPCODE, CVMCPU_COND_EQ,
			      lockRecRegID, CVM_LOCKSTATE_UNLOCKED);
    /* branch if object is locked */
    CVMJITaddCodegenComment((con,"br simpleLockGrabDone if object is locked"));
    fixupPC1 = CVMCPUemitBranch(con, 0, CVMCPU_COND_NE);

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* Get the ee: */
#ifndef CVMCPU_EE_REG
    eeRegID = CVMRMgetRegisterNumber(eeRes);
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
				     CVMoffsetof(CVMCCExecEnv, eeX));
#else
    eeRegID = CVMCPU_EE_REG;
#endif
    /* Store the object pointer into the lock record */
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.object = obj"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
        objRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.object));
    /* Store the object header word into the lock record */
    CVMJITaddCodegenComment((con,
       "ee->simpleSyncReservedOwnedMonitor.u.fast.bits = obj->hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_STR32_OPCODE,
        objHdrRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.u.fast.bits));
    /* compute address of lock record */
    CVMJITaddCodegenComment((con, "compute address of lock record fast.bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
	lockRecRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor),
	CVMJIT_NOSETCC);
    /* CAS the lock record pointer into the object header. */
    CVMJITprintCodegenComment((
	"CAS the lock record pointer into the object header"));
    CVMJITsetSymbolName((con, "simpleLockGrabDone"));/* name of branch label */

#ifndef CVMCPU_EE_REG
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
#endif

    CVMassert(objHdrRegID == CVMX86_EAX);
    {
	CVMX86Address adr;
	CVMCPUinit_Address_base_disp(&adr, objRegID,
	    CVMoffsetof(CVMObjectHeader, various32),
	    CVMCPU_MEMSPEC_IMMEDIATE_OFFSET);
	CVMX86cmpxchg_reg_mem(con, lockRecRegID, adr);
    }
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lockRec);

    /* "done" target for failure branches. */
    CVMtraceJITCodegen(("\t\tsimpleLockGrabDone:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

    {
	int target = (CVMX86_EAX|CVMX86_EBX|CVMX86_ECX|CVMX86_EDX);
	int avoid = GET_REGISTER_AVOID_GOALS;
	avoid |= ~GET_REGISTER_TARGET_GOALS;
	dest = CVMRMgetResourceStrict(CVMRM_INT_REGS(con), target, avoid, 1);
	destRegID = CVMRMgetRegisterNumber(dest);
	/* Use AL for EAX, BL, for EBX, etc. */
	CVMX86setb_reg(con, CVMX86equal, destRegID);
	CVMCPUemitBinaryALUConstant(con, CVMCPU_AND_OPCODE,
				    destRegID, destRegID,
				    1, CVMJIT_NOSETCC);
    }

    CVMRMoccupyAndUnpinResource(CVMRM_INT_REGS(con), dest, intrinsicNode);
    pushResource(con, dest);
}

/*
 * Intrinsic emitter for fastlock CAS version of  CVM.simpleLockRelease().
 *
 * Attempts to unlock the object by swapping in the old bits, expecting
 * the reserved ee->simpleSyncReservedOwnedMonitor pointer to be swapped
 * out. If the atomic CAS of these values failed, then in if defers
 * to the CVMCCMruntimeSimpleSyncUnlock() helper.
 */
static void
simpleLockReleaseEmitter(
    CVMJITCompilationContext * con,
    CVMJITIRNode *intrinsicNode)
{
    CVMRMResource* obj = popResource(con);
    CVMRMResource* objHdr;
    CVMRMResource* lockRec;
    CVMRMResource* eeRes;
    int objRegID, objHdrRegID, lockRecRegID, eeRegID;
    int fixupPC1, fixupPC2; /* To patch the conditional branches */

    if (CVMRMisConstant(obj) && CVMRMgetConstant(obj) == 0) {
	/* We know the object is a null, so an NPE would be thrown before
	 * code generated here is executed. Therefore we don't need to
	 * generate anything.
	 */
	CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
	return;
    }

    CVMassert(CVMX86_EAX == CVMCPU_ARG1_REG);
    lockRec = CVMRMgetResourceSpecific(CVMRM_INT_REGS(con), CVMX86_EAX, 1);
    CVMRMpinResourceSpecific(CVMRM_INT_REGS(con), obj, CVMCPU_ARG2_REG);
    objRegID    = CVMRMgetRegisterNumber(obj);
    lockRecRegID = CVMRMgetRegisterNumber(lockRec);

    CVMRMmajorSpill(con, ARG1|ARG2, CVMRM_SAFE_SET);

    eeRes = CVMRMgetResource(CVMRM_INT_REGS(con),
	CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);
    objHdr = CVMRMgetResource(CVMRM_INT_REGS(con),
	CVMRM_ANY_SET, CVMRM_EMPTY_SET, 1);

    objHdrRegID = CVMRMgetRegisterNumber(objHdr);

#ifndef CVMCPU_EE_REG
    /* Get the ee: */
    eeRegID = CVMRMgetRegisterNumber(eeRes);
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
				     CVMoffsetof(CVMCCExecEnv, eeX));
#else
    eeRegID = CVMCPU_EE_REG;
#endif

    /* load the object header */
    CVMJITaddCodegenComment((con, "get obj.hdr.various32"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
				       objHdrRegID, objRegID,
				       CVMoffsetof(CVMObjectHeader,various32));
    /* compute address of lock record */
    CVMJITaddCodegenComment((con, "compute address of lock record fast.bits"));
    CVMCPUemitBinaryALUConstant(con, CVMCPU_ADD_OPCODE,
	lockRecRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor),
	CVMJIT_NOSETCC);
    /* Note the lock state bits for "locked" must be 0 in order for the
     * CAS below to work without first checking the lock state bits. */
    CVMassert(CVM_LOCKSTATE_LOCKED == 0);
    /* get old object header word from lock record. Note, it will have been
     * overwritten if inflated. */
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.u.fast.bits"));
    CVMCPUemitMemoryReferenceImmediate(con, CVMCPU_LDR32_OPCODE,
        objHdrRegID, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.u.fast.bits));
    /* CAS the old header word back into the object header, requiring
       that the swapped out word be the unmodified lock record pointer */
    CVMJITprintCodegenComment((
	"CAS the old header word back into the object header, requiring"));
    CVMJITprintCodegenComment((
	"that the swapped out word be the unmodified lock record pointer"));
    CVMJITsetSymbolName((con, "simpleLockReleaseFailed")); /* branch label */
    CVMassert(lockRecRegID == CVMX86_EAX);
    fixupPC2 = CVMCPUemitAtomicCompareAndSwap(con,
	objRegID, CVMoffsetof(CVMObjectHeader,various32),
	lockRecRegID, objHdrRegID);
    /* If we get here, then success. br simpleLockReleaseDone. */
    CVMJITaddCodegenComment((con, "br simpleLockReleaseDone"));
    fixupPC1 = CVMCPUemitBranch(con, 0, CVMCPU_COND_AL);

#ifdef IAI_CODE_SCHEDULER_SCORE_BOARD
    fixupPC1 = CVMJITcbufGetLogicalInstructionPC(con);
#endif
    /* failed: target for failed CAS */
    CVMtraceJITCodegen(("\t\tsimpleLockReleaseFailed:\n"));
    CVMJITfixupAddress(con, fixupPC2, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);

    /* Setup arguments for C helper that will do unlock */

    /* Call C helper to do unlock */
    CVMJITaddCodegenComment((con, "call CVMCCMruntimeSimpleSyncUnlockGlue"));
    CVMJITsetSymbolName((con, "CVMCCMruntimeSimpleSyncUnlockGlue"));
    CVMJITstatsRecordInc(con, CVMJIT_STATS_CVMCCMruntimeSimpleSyncUnlock);

    CVMCPUemitAbsoluteCall(con, (void *)CVMCCMruntimeSimpleSyncUnlockGlue,
	CVMJIT_NOCPDUMP, CVMJIT_NOCPBRANCH, 0);

    CVMJITcsBeginBlock(con);
    CVMJITcaptureStackmap(con, 0);

    /* "simpleLockReleaseDone" target for branches. */
    CVMtraceJITCodegen(("\t\tsimpleLockReleaseDone:\n"));
    CVMJITfixupAddress(con, fixupPC1, CVMJITcbufGetLogicalPC(con),
		       CVMJIT_COND_BRANCH_ADDRESS_MODE);
#ifdef CVM_DEBUG
    /* NULL out object pointer in the lock record */
    CVMJITprintCodegenComment(("DEBUG-ONLY CODE"));
#ifndef CVMCPU_EE_REG
    /* Get the ee: */
    CVMRMpinResource(CVMRM_INT_REGS(con), eeRes,
	CVMRM_ANY_SET, CVMRM_EMPTY_SET);
    eeRegID = CVMRMgetRegisterNumber(eeRes);
    CVMJITaddCodegenComment((con, "eeReg = ccee->ee"));
    CVMCPUemitCCEEReferenceImmediate(con, CVMCPU_LDR32_OPCODE, eeRegID,
                                     CVMoffsetof(CVMCCExecEnv, eeX));
#endif
    CVMJITaddCodegenComment((con,
	"ee->simpleSyncReservedOwnedMonitor.object = NULL"));
    CVMCPUemitMemoryReferenceImmediateConst(con, CVMCPU_STR32_OPCODE,
        0, eeRegID,
	offsetof(CVMExecEnv, simpleSyncReservedOwnedMonitor.object));
    CVMJITprintCodegenComment(("END OF DEBUG-ONLY CODE"));
#endif

    CVMRMrelinquishResource(CVMRM_INT_REGS(con), obj);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), lockRec); 
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), objHdr);
    CVMRMrelinquishResource(CVMRM_INT_REGS(con), eeRes);
}

#endif /* CVM_FASTLOCK_TYPE == CVM_FASTLOCK_ATOMICOPS */
#endif /* CVMJIT_SIMPLE_SYNC_METHODS */
%}
